{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w79TQtD755-G"
      },
      "source": [
        "# Initial Setup\n",
        "\n",
        "The project repository is mounted from Google Drive and added to the Python path to allow clean imports from the src module. The dataset is copied to the local Colab filesystem to improve I/O performance during training. All global settings (random seed, device selection, paths, batch sizes) are defined once and reused across the notebook to ensure consistency and reproducibility.\n",
        "\n",
        "Weights & Biases is initialized for experiment tracking, and all training stages use the same precomputed dataset statistics and DataLoaders for fair comparison across models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\"\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%%capture\n",
        "%pip install --no-cache-dir -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtirMD1x7EU8"
      },
      "outputs": [],
      "source": [
        "# %pip install wandb fiftyone==1.10.0 sympy==1.12 torch==2.9.0 torchvision==0.20.0 numpy open-clip-torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukkECgFX5HQT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from torch.optim import Adam\n",
        "\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qQeea1oLVvJ"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/data\n",
        "!cp -r \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data_transformed\" /content/data\n",
        "# !cp -r \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data/assessment\" /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWFUmIBoTQb4"
      },
      "outputs": [],
      "source": [
        "from src.config import (SEED, NUM_WORKERS, BATCH_SIZE, IMG_SIZE, N, CLASSES,\n",
        "                        NUM_CLASSES, LABEL_MAP, RAW_DATA, CHECKPOINTS, DEVICE, VALID_BATCHES)\n",
        "from src.utility import set_seeds, init_wandb\n",
        "from src.datasets import compute_dataset_mean_std, get_dataloaders\n",
        "from src.training import compute_class_weights, get_early_inputs, get_inputs, train_model\n",
        "from src.visualization import build_pairwise_downsampling_tables, plot_losses\n",
        "from src.models import EmbedderMaxPool, EmbedderStrided, EarlyFusionModel, ConcatIntermediateNet, AddIntermediateNet, MatmulIntermediateNet, HadamardIntermediateNet, LateNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTMc18ZD5_lr"
      },
      "outputs": [],
      "source": [
        "# Fusion specific constants\n",
        "EPOCHS = 15\n",
        "LR = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXe1O-jQTQb-"
      },
      "outputs": [],
      "source": [
        "# Usage: Call this function at the beginning and before each training phase\n",
        "set_seeds(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQPywS8q7UrH",
        "outputId": "da2ff919-f787-48cb-b42b-31b8ccda0d4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichele-marschner\u001b[0m (\u001b[33mmichele-marschner-university-of-potsdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load W&B API key from Colab Secrets and make it available as env variable\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf2LJnXb795e"
      },
      "source": [
        "# Loading and preparation of Data\n",
        "\n",
        "This section computes normalization statistics, defines the RGB+LiDAR transforms, and prepares the training, validation, and test DataLoaders. Both downsampling strategies (MaxPool and Stride) receive identical input preprocessing to guarantee a fair ablation comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9jcaSSpS3kd",
        "outputId": "36ee50bd-afce-4e19-974f-153931cc2f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning dataset in /content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data...\n",
            "cubes: 2501 RGB files found. Matching XYZA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spheres: 9999 RGB files found. Matching XYZA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preloading LiDAR XYZA tensors into RAM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading XYZA:   0%|          | 38/12500 [00:28<2:35:57,  1.33it/s]"
          ]
        }
      ],
      "source": [
        "# Calculates mean and standard deviation of the rgb train data\n",
        "# for different dataset (or change in train data) recalculate mean and standard deviation\n",
        "# mean, std = compute_dataset_mean_std(root_dir=RAW_DATA, img_size=IMG_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OErXfds6EC8O"
      },
      "outputs": [],
      "source": [
        "img_transforms = transforms.Compose([\n",
        "    transforms.ToImage(),   # Scales data into [0,1]\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize(([0.0051, 0.0052, 0.0051, 1.0000]), ([5.8023e-02, 5.8933e-02, 5.8108e-02, 2.4509e-07])) \n",
        "    # transforms.Normalize(mean.tolist(), std.tolist())     # uncomment for different dataset (or change in train data)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiXY_6-CUYOs"
      },
      "outputs": [],
      "source": [
        "set_seeds(SEED)\n",
        "\n",
        "train_data, train_dataloader, valid_data, val_dataloader, test_data, test_dataloader = get_dataloaders(\n",
        "    str(RAW_DATA),\n",
        "    VALID_BATCHES,\n",
        "    test_frac=0.15,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    img_transforms=img_transforms,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "for i, sample in enumerate(train_data):\n",
        "    print(i, *(x.shape for x in sample))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJLmVUfdGeXB"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "In this section we train multiple fusion models using either MaxPool-based embedders or StridedConv-based embedders. For each architecture, we initialize the optimizer, create a W&B run, train the model, and save checkpoints. The goal is to isolate the impact of replacing MaxPool with stride-based downsampling while keeping all other components constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tt2AIIOA6Jj"
      },
      "outputs": [],
      "source": [
        "FEATURE_DIM = 128\n",
        "\n",
        "set_seeds(SEED)\n",
        "\n",
        "class_weights = compute_class_weights(train_data, NUM_CLASSES).to(DEVICE)\n",
        "loss_func = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))\n",
        "\n",
        "metrics = {}   # store losses for each model\n",
        "\n",
        "# Defines fusion models to train and compare\n",
        "models_to_train = {\n",
        "    \"early_fusion_pool\": EarlyFusionModel(in_ch=8, output_dim=2, embedder_cls=EmbedderMaxPool).to(DEVICE),\n",
        "    \"early_fusion_stride\": EarlyFusionModel(in_ch=8, output_dim=2, embedder_cls=EmbedderStrided).to(DEVICE),\n",
        "    \"intermediate_fusion_concat_pool\": ConcatIntermediateNet(4, 4, output_dim=NUM_CLASSES, feature_dim=FEATURE_DIM, embedder_cls=EmbedderMaxPool).to(DEVICE),\n",
        "    \"intermediate_fusion_concat_stride\": ConcatIntermediateNet(4, 4, output_dim=NUM_CLASSES, feature_dim=FEATURE_DIM, embedder_cls=EmbedderStrided).to(DEVICE),\n",
        "    \"late_fusion_pool\": LateNet(4, 4, output_dim=NUM_CLASSES, embedder_cls=EmbedderMaxPool).to(DEVICE),\n",
        "    \"late_fusion_stride\": LateNet(4, 4, output_dim=NUM_CLASSES, embedder_cls=EmbedderStrided).to(DEVICE)\n",
        "}\n",
        "\n",
        "# === Main experiment loop over all fusion strategies ===\n",
        "for name, model in models_to_train.items():\n",
        "  model_save_path = CHECKPOINTS\n",
        "\n",
        "  # Number of trainable parameters (for the comparison table)\n",
        "  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "  opt = Adam(model.parameters(), lr=LR)\n",
        "\n",
        "  # Initialize a new Weights & Biases run for this model.\n",
        "  init_wandb(\n",
        "      model=model,\n",
        "      name=name,\n",
        "      fusion_name=name,\n",
        "      num_params=num_params,\n",
        "      opt_name = opt.__class__.__name__)\n",
        "\n",
        "  # Choose the proper input function depending on the fusion strategy:\n",
        "  if name.startswith(\"early_fusion\"):\n",
        "    input_fn = get_early_inputs\n",
        "  else:\n",
        "    input_fn = get_inputs\n",
        "\n",
        "  results = train_model(\n",
        "    model=model,\n",
        "    optimizer=opt,\n",
        "    input_fn=input_fn,\n",
        "    epochs=EPOCHS,\n",
        "    loss_fn=loss_func,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    model_save_path=model_save_path,\n",
        "    target_idx=-1,   # last element in batch is target\n",
        "    log_to_wandb=True,\n",
        "    device=DEVICE\n",
        "  )\n",
        "\n",
        "  metrics[name] = results\n",
        "\n",
        "  # End wandb run before starting the next model\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xux2ArSO_Nnm"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "This section plots validation losses for all trained models and generates downsampling comparison tables. These tables summarize performance differences between MaxPool and StridedConv variants across early, intermediate, and late fusion methods. All results are logged to W&B for easy inspection and reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYOG1tPOTQcC"
      },
      "outputs": [],
      "source": [
        "name_map = {\n",
        "    \"early_fusion\": \"Early Fusion\",\n",
        "    \"intermediate_fusion_concat\": \"Intermediate (Concat)\",\n",
        "    \"late_fusion\": \"Late Fusion\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSfWbBea6u4x"
      },
      "outputs": [],
      "source": [
        "loss_dict = {name: m[\"valid_losses\"] for name, m in metrics.items()}\n",
        "fig = plot_losses(loss_dict, title=\"Validation Loss per Model\")\n",
        "plt.show()\n",
        "\n",
        "tables = build_pairwise_downsampling_tables(metrics, name_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECwt6Ay0TQcC"
      },
      "outputs": [],
      "source": [
        "# logs comparison tables to wandb\n",
        "for base, df in tables.items():\n",
        "    wandb.init(\n",
        "        project=\"cilp-extended-assessment\",\n",
        "        name=f\"downsampling_comparison_{base}\",\n",
        "        job_type=\"analysis\",\n",
        "    )\n",
        "    wandb.log({f\"task4_downsampling_{base}\": wandb.Table(dataframe=df)})\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad-R30zkczAj"
      },
      "source": [
        "## When to Use\n",
        "\n",
        "This final section provides guidance on when each downsampling method—MaxPool or Strided Conv—is preferable based on the experiment results. It summarizes strengths, limitations, and practical scenarios where each approach performs best, helping translate the ablation findings into actionable design choices.\n",
        "\n",
        "**Early Fusion:**\n",
        "* Aligned, closely related low-level modalities and comparable features\n",
        "* Simple setup; avoid if sensors are noisy\n",
        "\n",
        "**Intermediate Fusion:**\n",
        "* Modalities with different structure that benefit from separate early processing in order to learn modality-specific features   \n",
        "* best overall balance of performance and flexibility\n",
        "\n",
        "**Late Fusion:**\n",
        "* Strong, independent unimodal predictors, to combine their strengths\n",
        "* ideal for heterogeneous or missing modalities\n",
        "* robust fallback when one modality fails"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
