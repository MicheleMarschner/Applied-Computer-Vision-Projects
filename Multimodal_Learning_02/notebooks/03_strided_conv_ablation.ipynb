{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overview: Strided Convolution Ablation\n",
        "\n",
        "This notebook studies the impact of replacing MaxPool-based downsampling with strided convolutions across different fusion architectures. Using identical training settings, we compare early, intermediate, and late fusion models to analyze how the choice of downsampling affects validation performance, training time, parameter efficiency, and feature stability. Results are logged with Weights & Biases and summarized in comparative tables and loss curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w79TQtD755-G"
      },
      "source": [
        "# Initial Setup\n",
        "\n",
        "The project repository is mounted from Google Drive and added to the Python path to allow clean imports from the src module. The dataset is copied to the local Colab filesystem to improve I/O performance during training. All global settings (random seed, device selection, paths, batch sizes) are defined once and reused across the notebook to ensure consistency and reproducibility.\n",
        "\n",
        "Weights & Biases is initialized for experiment tracking, and all training stages use the same precomputed dataset statistics and DataLoaders for fair comparison across models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\"\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%%capture\n",
        "%pip install --no-cache-dir -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukkECgFX5HQT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from torch.optim import Adam\n",
        "\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qQeea1oLVvJ"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/data\n",
        "!cp -r \"$DRIVE_ROOT/data/assessment\" /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWFUmIBoTQb4"
      },
      "outputs": [],
      "source": [
        "from src.config import (SEED, NUM_WORKERS, BATCH_SIZE, IMG_SIZE, NUM_CLASSES, DRIVE_ROOT, \n",
        "                        RAW_DATA, CHECKPOINTS, DEVICE, VALID_BATCHES)\n",
        "from src.utility import set_seeds, init_wandb, compute_embedding_size\n",
        "from src.datasets import get_dataloaders, get_train_stats\n",
        "from src.training import get_early_inputs, get_inputs, train_model\n",
        "from src.visualization import build_pairwise_downsampling_tables, plot_val_losses\n",
        "from src.models import EmbedderMaxPool, EmbedderStrided, EarlyFusionModel, ConcatIntermediateNet, LateNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTMc18ZD5_lr"
      },
      "outputs": [],
      "source": [
        "# Fusion specific constants\n",
        "EPOCHS = 15\n",
        "LR = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXe1O-jQTQb-"
      },
      "outputs": [],
      "source": [
        "# Usage: Call this function at the beginning and before each training phase\n",
        "set_seeds(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQPywS8q7UrH",
        "outputId": "da2ff919-f787-48cb-b42b-31b8ccda0d4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichele-marschner\u001b[0m (\u001b[33mmichele-marschner-university-of-potsdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load W&B API key from Colab Secrets and make it available as env variable\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf2LJnXb795e"
      },
      "source": [
        "# Loading and preparation of Data\n",
        "\n",
        "This section computes normalization statistics, defines the RGB+LiDAR transforms, and prepares the training, validation, and test DataLoaders. Both downsampling strategies (MaxPool and Stride) receive identical input preprocessing to guarantee a fair ablation comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9jcaSSpS3kd",
        "outputId": "36ee50bd-afce-4e19-974f-153931cc2f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning dataset in /content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data...\n",
            "cubes: 2501 RGB files found. Matching XYZA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spheres: 9999 RGB files found. Matching XYZA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preloading LiDAR XYZA tensors into RAM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading XYZA:   0%|          | 38/12500 [00:28<2:35:57,  1.33it/s]"
          ]
        }
      ],
      "source": [
        "# gets calculated mean, std from file or calculates it from the rgb train data\n",
        "# for different dataset (or change in train data) recalculate mean and standard deviation\n",
        "mean, std = get_train_stats(dir=DRIVE_ROOT, img_size=IMG_SIZE, data_dir=RAW_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OErXfds6EC8O"
      },
      "outputs": [],
      "source": [
        "img_transforms = transforms.Compose([\n",
        "    transforms.ToImage(),   # Scales data into [0,1]\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize(mean, std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiXY_6-CUYOs"
      },
      "outputs": [],
      "source": [
        "set_seeds(SEED)\n",
        "\n",
        "train_data, train_dataloader, valid_data, val_dataloader, test_data, test_dataloader = get_dataloaders(\n",
        "    str(RAW_DATA),\n",
        "    VALID_BATCHES,\n",
        "    test_frac=0.15,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    img_transforms=img_transforms,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "for i, sample in enumerate(train_data):\n",
        "    print(i, *(x.shape for x in sample))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJLmVUfdGeXB"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "In this section we train multiple fusion models using either MaxPool-based embedders or StridedConv-based embedders. For each architecture, we initialize the optimizer, create a W&B run, train the model, and save checkpoints. The goal is to isolate the impact of replacing MaxPool with stride-based downsampling while keeping all other components constant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tt2AIIOA6Jj"
      },
      "outputs": [],
      "source": [
        "FEATURE_DIM = 128\n",
        "\n",
        "set_seeds(SEED)\n",
        "\n",
        "#class_weights = compute_class_weights(train_data, NUM_CLASSES).to(DEVICE)\n",
        "#loss_func = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "metrics = {}   # store losses for each model\n",
        "\n",
        "# Defines fusion models to train and compare\n",
        "models_to_train = {\n",
        "    \"early_fusion_pool\": EarlyFusionModel(in_ch=8, output_dim=2, embedder_cls=EmbedderMaxPool).to(DEVICE),\n",
        "    \"early_fusion_stride\": EarlyFusionModel(in_ch=8, output_dim=2, embedder_cls=EmbedderStrided).to(DEVICE),\n",
        "    \"intermediate_fusion_concat_pool\": ConcatIntermediateNet(4, 4, output_dim=NUM_CLASSES, feature_dim=FEATURE_DIM, embedder_cls=EmbedderMaxPool).to(DEVICE),\n",
        "    \"intermediate_fusion_concat_stride\": ConcatIntermediateNet(4, 4, output_dim=NUM_CLASSES, feature_dim=FEATURE_DIM, embedder_cls=EmbedderStrided).to(DEVICE),\n",
        "    \"late_fusion_pool\": LateNet(4, 4, output_dim=NUM_CLASSES, embedder_cls=EmbedderMaxPool).to(DEVICE),\n",
        "    \"late_fusion_stride\": LateNet(4, 4, output_dim=NUM_CLASSES, embedder_cls=EmbedderStrided).to(DEVICE)\n",
        "}\n",
        "\n",
        "# === Main experiment loop over all fusion strategies ===\n",
        "for name, model in models_to_train.items():\n",
        "  model_save_path = CHECKPOINTS / f\"{name}.pt\"\n",
        "\n",
        "  # Number of trainable parameters (for the comparison table)\n",
        "  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "  opt = Adam(model.parameters(), lr=LR)\n",
        "\n",
        "  embedding_size = compute_embedding_size(name, FEATURE_DIM, spatial=(8, 8))\n",
        "\n",
        "  # Initialize a new Weights & Biases run for this model.\n",
        "  init_wandb(\n",
        "      model=model,\n",
        "      name=name,\n",
        "      embedding_size=embedding_size,\n",
        "      fusion_name=name,\n",
        "      num_params=num_params,\n",
        "      opt_name = opt.__class__.__name__)\n",
        "\n",
        "  # Choose the proper input function depending on the fusion strategy:\n",
        "  if name.startswith(\"early_fusion\"):\n",
        "    input_fn = get_early_inputs\n",
        "  else:\n",
        "    input_fn = get_inputs\n",
        "\n",
        "  results = train_model(\n",
        "    model=model,\n",
        "    optimizer=opt,\n",
        "    input_fn=input_fn,\n",
        "    epochs=EPOCHS,\n",
        "    loss_fn=loss_func,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    model_save_path=model_save_path,\n",
        "    target_idx=-1,   # last element in batch is target\n",
        "    log_to_wandb=True,\n",
        "    device=DEVICE\n",
        "  )\n",
        "\n",
        "  metrics[name] = results\n",
        "\n",
        "  # End wandb run before starting the next model\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xux2ArSO_Nnm"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "This section plots validation losses for all trained models and generates downsampling comparison tables. These tables summarize performance differences between MaxPool and StridedConv variants across early, intermediate, and late fusion methods. All results are logged to W&B for easy inspection and reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KYOG1tPOTQcC"
      },
      "outputs": [],
      "source": [
        "name_map = {\n",
        "    \"early_fusion\": \"Early Fusion\",\n",
        "    \"intermediate_fusion_concat\": \"Intermediate (Concat)\",\n",
        "    \"late_fusion\": \"Late Fusion\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSfWbBea6u4x"
      },
      "outputs": [],
      "source": [
        "loss_dict = {name: m[\"valid_losses\"] for name, m in metrics.items()}\n",
        "fig, ax = plot_val_losses(loss_dict, title=\"Validation Loss per Model\")\n",
        "plt.show()\n",
        "\n",
        "tables = build_pairwise_downsampling_tables(metrics, name_map)\n",
        "\n",
        "# logs comparison tables and loss curves to wandb\n",
        "for base, df in tables.items():\n",
        "    wandb.init(\n",
        "        project=\"cilp-extended-assessment\",\n",
        "        name=f\"downsampling_comparison_{base}\",\n",
        "        job_type=\"analysis\",\n",
        "    )\n",
        "    wandb.log({f\"task4_downsampling_{base}\": wandb.Table(dataframe=df)})\n",
        "\n",
        "wandb.log({\"max_pool_vs_stride/val_loss_curves\": wandb.Image(fig)})\n",
        "plt.close(fig)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for name, df in tables.items():\n",
        "    display(Markdown(f\"### {name} — MaxPool vs Strided Conv\"))\n",
        "    display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation of Ablation Study - Max Pooling vs. Strided:\n",
        "\n",
        "**Early Fusion:**\n",
        "|index|Metric|MaxPool2d|Strided Conv|Difference \\(Strided - MaxPool\\)|\n",
        "|---|---|---|---|---|\n",
        "|0|Validation Loss \\(best\\)|5\\.9752e-07|7\\.1077e-07|1\\.1325e-07|\n",
        "|1|Parameters|8387990\\.0|8387990\\.0|0\\.0|\n",
        "|2|Training Time \\(s\\)|136\\.3252|129\\.5581|-6\\.7671|\n",
        "|3|Final Accuracy|1\\.0|1\\.0|0\\.0|\n",
        "\n",
        "**Intermediate Fusion (Concat):**\n",
        "|index|Metric|MaxPool2d|Strided Conv|Difference \\(Strided - MaxPool\\)|\n",
        "|---|---|---|---|---|\n",
        "|0|Validation Loss \\(best\\)|1\\.3411e-08|4\\.7867e-06|4\\.7733e-06|\n",
        "|1|Parameters|16672374\\.0|16672374\\.0|0\\.0|\n",
        "|2|Training Time \\(s\\)|218\\.2184|168\\.5268|-49\\.6916|\n",
        "|3|Final Accuracy|1\\.0|1\\.0|0\\.0|\n",
        "\n",
        "**Late Fusion:**\n",
        "|index|Metric|MaxPool2d|Strided Conv|Difference \\(Strided - MaxPool\\)|\n",
        "|---|---|---|---|---|\n",
        "|0|Validation Loss \\(best\\)|1\\.2889e-07|5\\.1322e-06|5\\.0033e-06|\n",
        "|1|Parameters|16672374\\.0|16672374\\.0|0\\.0|\n",
        "|2|Training Time \\(s\\)|217\\.3971|169\\.4651|-47\\.9319|\n",
        "|3|Final Accuracy|1\\.0|1\\.0|0\\.0|\n",
        "\n",
        "Across all fusion strategies, MaxPool2d consistently achieves lower best validation loss than strided convolutions, while both approaches reach identical final accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad-R30zkczAj"
      },
      "source": [
        "# Downsampling Ablation: Quantitative and Theoretical Analysis\n",
        "\n",
        "Across all fusion strategies, MaxPool consistently achieves lower best validation loss than strided convolutions, while both approaches reach identical final accuracy. This indicates more stable and precise feature alignment for MaxPool, particularly for intermediate and late fusion, where strided convolutions lead to substantially higher validation loss despite reducing training time by approximately 45–50 seconds per run.\n",
        "\n",
        "Both MaxPool and strided convolutions downsample feature maps, but they differ fundamentally in how information is preserved, how features are learned, and how gradients flow through the network. The appropriate choice depends on the architecture, dataset, and task.\n",
        "\n",
        "*MaxPool* is a non-parametric operation that selects the maximum value within each pooling window, preserving only the most prominent activations and introducing translation invariance. This is beneficial for classification tasks where coarse, dominant patterns are sufficient. However, because most activations are discarded, gradient updates are sparse and less informative. Max pooling also removes subtle spatial variations and geometric cues and, lacking learnable parameters, cannot adapt its downsampling behavior to the task.\n",
        "\n",
        "*Strided convolutions*, in contrast, perform downsampling using learnable filters. They aggregate information from overlapping regions, producing denser and smoother gradients while preserving more contextual and structural detail. This allows the network to adaptively combine features before resolution reduction, making strided convolutions better suited for tasks requiring spatial precision or fine-grained feature learning.\n",
        "\n",
        "**Recommendation:**\n",
        "MaxPool is well suited for simple classification tasks with stable patterns. Strided convolutions are preferable when spatial detail is critical, such as in regression, localization, or anomaly detection. In our setting, MaxPool is clearly the better choice, offering superior validation performance and stability without unnecessary model complexity."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
