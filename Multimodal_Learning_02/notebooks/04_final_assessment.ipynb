{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwNXAWXcyOWQ"
      },
      "source": [
        "# Model Architecture:\n",
        "\n",
        "**Stage 1:** Contrastive Pretraining: CILP_model\n",
        "\n",
        "**Goal:** align RGB and LiDAR in a shared 200-D space --> encodes both rgb and lidar in the same dimensionality space\n",
        "```\n",
        "RGB ----> Img Encoder ----\\\n",
        "                            ----> CLIP-style similarity\n",
        "LiDAR -> Lidar Encoder ----/\n",
        "```\n",
        "**Outcome:** Shared embedding space where matching RGB/LiDAR pairs have high similarity and non-matches low similarity.\n",
        "\n",
        "----------------------------\n",
        "\n",
        "**Stage 2:** Projector Training: projector\n",
        "\n",
        "**Goal:** learn a mapping from RGB CILP embeddings to LiDAR embeddings used by lidar_cnn:\n",
        "ℝ²⁰⁰ (CILP RGB embedding) → ℝ³²⁰⁰ (LiDAR-CNN embedding)\n",
        "\n",
        "projector knows how to “pretend” RGBs are LiDAR internally: projected_RGB_embedding ≈ “real” LiDAR embedding for each paired RGB/LiDAR sample.\n",
        "```\n",
        "RGB ----> Img Encoder ----> Projector ----> LiDAR embedding\n",
        "                                     |\n",
        "                                     v\n",
        "                             MSE-loss to true LiDAR embedding\n",
        "\n",
        "```\n",
        "----------------------------\n",
        "\n",
        "**Stage 3:** Final Classifier: RGB2LiDARClassifier\n",
        "\n",
        "**Goal:** chaining all models together to classify spheres and cubes from images\n",
        "\n",
        "pretends the RGBs look like LiDAR in the internal feature space and then uses LiDAR classifier.\n",
        "```\n",
        "RGB (img) ----> (CILP Img Encoder) ----> 200-D CILP embedding ----> (Projector) ---> 3200-D LiDAR embedding\n",
        "---> (LiDAR Classifier) ---> cube/sphere\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYZmjLlguKJw"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install wandb weave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgqWE6wM_Y_N"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install fiftyone==1.10.0 sympy==1.12 torch==2.9.0 torchvision==0.20.0 numpy open-clip-torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgrrXotQ0t0k"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUod4cKbh8ke"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#import torchvision.transforms as transforms\n",
        "import torchvision.transforms.v2 as transforms\n",
        "\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h2BgKorid2q",
        "outputId": "f546a222-33b2-4fd2-e84f-78b637f78bda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "STORAGE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\")\n",
        "DATA_PATH = STORAGE_PATH / \"multimodal_training_workshop/data/assessment\"\n",
        "TMP_DATA_PATH = Path(\"/content/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39rSQEHcxASY"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/multimodal_training_workshop/data/assessment\" /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U43hLaLjMRxt"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9KwN-8AMVV_"
      },
      "outputs": [],
      "source": [
        "from src.utility import set_seeds\n",
        "from src.datasets import compute_dataset_mean_std_neu, get_cilp_dataloaders\n",
        "from src.training import compute_class_weights, get_rgb_inputs, train_model, init_wandb, train_with_batch_loss\n",
        "#from src.visualization import build_fusion_comparison_df, plot_losses\n",
        "from src.models import CILPBackbone, ContrastivePretraining, Classifier, Projector, EmbedderMaxPool, RGB2LiDARClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l3hqzHd0rux"
      },
      "source": [
        "## Constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETqCFts35qPy"
      },
      "outputs": [],
      "source": [
        "SEED = 51\n",
        "NUM_WORKERS = os.cpu_count()  # Number of CPU cores\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = 64\n",
        "\n",
        "CLASSES = [\"cubes\", \"spheres\"]\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "LABEL_MAP = {\"cubes\": 0, \"spheres\": 1}\n",
        "\n",
        "VALID_BATCHES = 10\n",
        "N = 12500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmi7z0qDPAqc",
        "outputId": "bdf8e038-4895-4f6d-8641-525672045156"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdQmqB-kMQ1o"
      },
      "outputs": [],
      "source": [
        "# Usage: Call this function at the beginning and before each training phase\n",
        "set_seeds(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS0HyPjdNgFC"
      },
      "source": [
        "# Integration of Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-blsH0WNfaV"
      },
      "outputs": [],
      "source": [
        "# Load W&B API key from Colab Secrets and make it available as env variable\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYWacedKNnVT"
      },
      "source": [
        "# Loading and preparation of Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQmxRnRZNu6Y"
      },
      "outputs": [],
      "source": [
        "## Final: dynamisch\n",
        "# mean, std = compute_dataset_mean_std(root_dir=root, img_size=IMG_SIZE)\n",
        "mean, std = compute_dataset_mean_std_neu(root_dir=TMP_DATA_PATH, img_size=IMG_SIZE, seed=SEED)\n",
        "print(mean, std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRuSY96XPeTP"
      },
      "outputs": [],
      "source": [
        "## Final: dynamisch\n",
        "img_transforms = transforms.Compose([\n",
        "    transforms.ToImage(),   # Scales data into [0,1]\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize(([0.0051, 0.0052, 0.0051, 1.0000]), ([5.8023e-02, 5.8933e-02, 5.8108e-02, 2.4509e-07]))     ## assessment dataset\n",
        "    # transforms.Normalize(mean.tolist(), std.tolist())     ## assessment dataset\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hofSI-z_N9d8"
      },
      "outputs": [],
      "source": [
        "train_data, train_dataloader, valid_data, val_dataloader, test_data, test_dataloader  = get_cilp_dataloaders(\n",
        "    str(TMP_DATA_PATH),\n",
        "    VALID_BATCHES,\n",
        "    test_frac=0.10,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    img_transforms=img_transforms,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "for i, sample in enumerate(train_data):\n",
        "    print(i, *(x.shape for x in sample))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP4MXN68uNRq"
      },
      "source": [
        "# The Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IMEYq_hVDQd"
      },
      "source": [
        "I use a 2-layer projection head with a 512-dimensional hidden layer between the backbone’s CNN features and the final CILP embedding.\n",
        "The 512 dimension is standard in contrastive learning literature (e.g., CLIP, SimCLR), as it provides a good balance between model expressiveness and computational efficiency.\n",
        "It allows a nonlinear transformation from the high-dimensional CNN output into the shared embedding space while avoiding overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uclun1v23rwP"
      },
      "source": [
        "## Stage 1: CILP contrastive pretraining"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NN8eON2P3qRP"
      },
      "source": [
        "## Stage 2: Projector training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fl8rikTIvGxH"
      },
      "source": [
        "# The Dataset\n",
        "\n",
        "**TODO:** I would use the code from the task 3\n",
        "\n",
        "```\n",
        "AssessmentDataset (Pattern A or your MyDataset variant)\n",
        "\n",
        "create_assessment_splits(...)\n",
        "\n",
        "make_dataloaders(...)\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMTis7eq20M2"
      },
      "source": [
        "# Training\n",
        "\n",
        "\n",
        "```\n",
        "train_model\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW5pD-KP3Urk"
      },
      "source": [
        "## Stage 1: CILP contrastive pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGxH6eynPFhe"
      },
      "outputs": [],
      "source": [
        "BEST_EMBEDDER = EmbedderMaxPool           \n",
        "FEATURE_DIM = 128\n",
        "CILP_EMB_SIZE = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0m1rjwB5wLV"
      },
      "outputs": [],
      "source": [
        "img_embedder = CILPBackbone(\n",
        "    in_ch=4, \n",
        "    embedder_cls=BEST_EMBEDDER, \n",
        "    feature_dim=FEATURE_DIM,\n",
        "    emb_size=CILP_EMB_SIZE\n",
        ").to(device)\n",
        "\n",
        "lidar_embedder = CILPBackbone(\n",
        "    in_ch=1, \n",
        "    embedder_cls=BEST_EMBEDDER, \n",
        "    feature_dim=FEATURE_DIM,\n",
        "    emb_size=CILP_EMB_SIZE\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTHa8rUL6BNO"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "CILP_model = ContrastivePretraining(img_embedder, lidar_embedder).to(device)\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_lidar = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cco0d02lWE1C"
      },
      "outputs": [],
      "source": [
        "def cilp_batch_loss_fn(model, batch, device):\n",
        "    \"\"\"\n",
        "    outputs: (logits_per_img, logits_per_lidar), each of shape (B, B)\n",
        "\n",
        "    We build ground-truth indices 0..B-1 so that:\n",
        "      - row i in logits_per_img should classify LiDAR i as the correct match\n",
        "      - row i in logits_per_lidar should classify RGB i as the correct match\n",
        "    \"\"\"\n",
        "    rgb, lidar, _ = batch\n",
        "    rgb = rgb.to(device)\n",
        "    lidar = lidar.to(device)\n",
        "\n",
        "    logits_per_img, logits_per_lidar = model(rgb, lidar)   # (B, B)\n",
        "    B = logits_per_img.size(0)\n",
        "    ground_truth = torch.arange(B, dtype=torch.long, device=device)\n",
        "\n",
        "    loss_i = loss_img(logits_per_img, ground_truth)\n",
        "    loss_l = loss_lidar(logits_per_lidar, ground_truth)\n",
        "\n",
        "    total_loss = (loss_i + loss_l) / 2.0\n",
        "\n",
        "    return total_loss, logits_per_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rDQfnNMKlET"
      },
      "outputs": [],
      "source": [
        "## train CILP\n",
        "EPOCHS_CILP = 5\n",
        "LR_CILP = 0.0001\n",
        "\n",
        "opt = torch.optim.Adam(CILP_model.parameters(), LR_CILP)\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "\n",
        "# Path where best model is saved\n",
        "checkpoint_dir = STORAGE_PATH / \"checkpoints\"\n",
        "checkpoint_dir.mkdir(exist_ok=True)\n",
        "model_save_path=checkpoint_dir / \"cilp_model.pth\"\n",
        "\n",
        "init_wandb(\n",
        "    model=CILP_model,\n",
        "    opt_name = opt.__class__.__name__\n",
        ")\n",
        "\n",
        "results = train_with_batch_loss(\n",
        "    model=CILP_model,\n",
        "    optimizer=opt,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    batch_loss_fn=cilp_batch_loss_fn,\n",
        "    epochs=EPOCHS_CILP,\n",
        "    model_save_path=model_save_path,\n",
        "    log_to_wandb=True,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "best_cilp_val = results[\"best_valid_loss\"]\n",
        "print(f\"[5.1] Best CILP validation loss: {best_cilp_val:.4f}\")\n",
        "\n",
        "\n",
        "wandb.run.summary[\"cilp_best_val_loss\"] = best_cilp_val\n",
        "# End wandb run before starting the next model\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rSHK5JmnCS1"
      },
      "outputs": [],
      "source": [
        "## freeze pre-trained model\n",
        "for param in CILP_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "CILP_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0PAZObVnI4v"
      },
      "source": [
        "## Stage 2: Cross-Modal Projection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWgPU4P7yXFq",
        "outputId": "a8a2bc20-aaf5-47b0-be6b-4459a8042fdf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (embedder): Sequential(\n",
              "    (0): Conv2d(1, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU()\n",
              "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (9): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): ReLU()\n",
              "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (12): Flatten(start_dim=1, end_dim=-1)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=3200, out_features=100, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load pre-trained lidar_cnn classifier\n",
        "lidar_cnn_path = STORAGE_PATH / \"checkpoints/lidar_cnn.pt\"\n",
        "\n",
        "lidar_cnn = Classifier(in_ch=1).to(device)\n",
        "lidar_cnn.load_state_dict(torch.load(lidar_cnn_path, weights_only=True))\n",
        "\n",
        "for param in lidar_cnn.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "lidar_cnn.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfGuIUEpWYF4"
      },
      "outputs": [],
      "source": [
        "def projector_batch_loss_fn(model, batch, device, CILP_model, lidar_cnn):\n",
        "    rgb_img, lidar_depth, _ = batch\n",
        "    rgb_img = rgb_img.to(device)\n",
        "    lidar_depth = lidar_depth.to(device)\n",
        "\n",
        "    # Use frozen encoders\n",
        "    CILP_model.eval()\n",
        "    lidar_cnn.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_embs = CILP_model.img_embedder(rgb_img)      # (B, CILP_EMB_SIZE)\n",
        "        lidar_embs = lidar_cnn.get_embs(lidar_depth)     # (B, lidar_dim)\n",
        "\n",
        "    pred_lidar_embs = model(img_embs)\n",
        "    \n",
        "    loss = F.mse_loss(pred_lidar_embs, lidar_embs)\n",
        "\n",
        "    # match the convention used in train_with_batch_loss\n",
        "    return loss, {\"loss\": loss.item()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkcLx0ikHvok"
      },
      "outputs": [],
      "source": [
        "img_dim = CILP_EMB_SIZE\n",
        "lidar_dim = 200 * 4 * 4\n",
        "\n",
        "projector = Projector(img_dim, lidar_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdthbedXSMxz"
      },
      "outputs": [],
      "source": [
        "EPOCHS_PROJ = 20\n",
        "LR_PROJECTOR = 1e-4\n",
        "model_save_path=checkpoint_dir / \"projector.pth\"\n",
        "\n",
        "opt = torch.optim.Adam(projector.parameters(), LR_PROJECTOR)\n",
        "\n",
        "best_val_proj = float(\"inf\")\n",
        "\n",
        "init_wandb(\n",
        "    model=projector,\n",
        "    opt_name = opt.__class__.__name__\n",
        ")\n",
        "\n",
        "results = train_with_batch_loss(\n",
        "    model=projector,\n",
        "    optimizer=opt,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    batch_loss_fn=projector_batch_loss_fn,\n",
        "    epochs=EPOCHS_PROJ,\n",
        "    model_save_path=model_save_path,\n",
        "    log_to_wandb=True,\n",
        "    device=device,\n",
        "    extra_args={\n",
        "        \"CILP_model\": CILP_model,\n",
        "        \"lidar_cnn\": lidar_cnn,\n",
        "    }\n",
        ")\n",
        "\n",
        "best_proj_val = results[\"best_valid_loss\"]\n",
        "print(f\"[5.2] Best projector validation MSE: {best_proj_val:.4f}\")\n",
        "\n",
        "wandb.run.summary[\"projector_best_val_mse\"] = best_proj_val\n",
        "# End wandb run before starting the next model\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp4pFlFynU9S"
      },
      "source": [
        "## Stage 3: RGB2LiDARClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_rgb2lidar_classifier(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs,\n",
        "    lr,\n",
        "    device,\n",
        "):\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.projector.parameters(), lr=lr)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_acc\": []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # ---------- TRAIN ----------\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for imgs, _, labels in train_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.float().view(-1, 1).to(device)   # [B,1]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(imgs)          # [B,1], no sigmoid\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "\n",
        "\n",
        "        # ---------- VALID ----------\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, _, labels in val_loader:\n",
        "                imgs = imgs.to(device)\n",
        "                labels = labels.float().view(-1, 1).to(device)\n",
        "\n",
        "                logits = model(imgs)              # [B,1]\n",
        "                loss = loss_fn(logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # accuracy\n",
        "                probs = torch.sigmoid(logits)     # [B,1], 0–1\n",
        "                preds = (probs >= 0.5).long()     # threshold\n",
        "                correct += (preds.view(-1) == labels.view(-1).long()).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = correct / total\n",
        "\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  val_acc={val_acc*100:.2f}%\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srtFQnE9yDVq"
      },
      "outputs": [],
      "source": [
        "LR_RGB2LIDAR = 1e-3\n",
        "EPOCHS_RGB2LIDAR = 5\n",
        "\n",
        "rgb2lidar_clf = RGB2LiDARClassifier(\n",
        "    CILP=CILP_model,\n",
        "    projector=projector,\n",
        "    lidar_cnn=lidar_cnn,\n",
        ").to(device)\n",
        "\n",
        "#class_weights = compute_class_weights(train_data, NUM_CLASSES).to(device)\n",
        "#loss_func = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "opt = torch.optim.Adam(rgb2lidar_clf.parameters(), lr=LR_RGB2LIDAR)\n",
        "\n",
        "results = train_rgb2lidar_classifier(\n",
        "    model=rgb2lidar_clf,\n",
        "    train_loader=train_dataloader,\n",
        "    val_loader=val_dataloader,\n",
        "    epochs=EPOCHS_RGB2LIDAR,\n",
        "    lr=LR_RGB2LIDAR,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "best_rgb2lidar_val = results[\"best_valid_loss\"]\n",
        "print(f\"[5.3] Best validation loss: {best_rgb2lidar_val:.4f}\")\n",
        "best_rgb2lidar_acc = results[\"best_valid_acc\"]\n",
        "print(f\"[5.3] Best validation accuracy: {best_rgb2lidar_acc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
