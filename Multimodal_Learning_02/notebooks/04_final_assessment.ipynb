{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwNXAWXcyOWQ"
      },
      "source": [
        "# Model Architecture:\n",
        "\n",
        "This section presents the three-stage pipeline used in the final assessment: contrastive pretraining (CILP), cross-modal projection, and RGB→LiDAR classification. The diagrams summarize how RGB and LiDAR information flow through the system and how each model component contributes to aligning, transforming, and classifying multimodal inputs. This high-level overview clarifies the purpose of each stage before moving on to implementation.\n",
        "\n",
        "**Stage 1:** Contrastive Pretraining: CILP_model\n",
        "\n",
        "**Goal:** align RGB and LiDAR in a shared 200-D space --> encodes both rgb and lidar in the same dimensionality space\n",
        "```\n",
        "RGB ----> Img Encoder ----\\\n",
        "                            ----> CLIP-style similarity\n",
        "LiDAR -> Lidar Encoder ----/\n",
        "```\n",
        "**Outcome:** Shared embedding space where matching RGB/LiDAR pairs have high similarity and non-matches low similarity.\n",
        "\n",
        "----------------------------\n",
        "\n",
        "**Stage 2:** Projector Training: projector\n",
        "\n",
        "**Goal:** learn a mapping from RGB CILP embeddings to LiDAR embeddings used by lidar_cnn:\n",
        "ℝ²⁰⁰ (CILP RGB embedding) → ℝ³²⁰⁰ (LiDAR-CNN embedding)\n",
        "\n",
        "projector knows how to “pretend” RGBs are LiDAR internally: projected_RGB_embedding ≈ “real” LiDAR embedding for each paired RGB/LiDAR sample.\n",
        "```\n",
        "RGB ----> Img Encoder ----> Projector ----> LiDAR embedding\n",
        "                                     |\n",
        "                                     v\n",
        "                             MSE-loss to true LiDAR embedding\n",
        "\n",
        "```\n",
        "----------------------------\n",
        "\n",
        "**Stage 3:** Final Classifier: RGB2LiDARClassifier\n",
        "\n",
        "**Goal:** chaining all models together to classify spheres and cubes from images\n",
        "\n",
        "pretends the RGBs look like LiDAR in the internal feature space and then uses LiDAR classifier.\n",
        "```\n",
        "RGB (img) ----> (CILP Img Encoder) ----> 200-D CILP embedding ----> (Projector) ---> 3200-D LiDAR embedding\n",
        "---> (LiDAR Classifier) ---> cube/sphere\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYZmjLlguKJw"
      },
      "source": [
        "# Setup\n",
        "\n",
        "This section installs required dependencies, mounts Google Drive, defines data paths, and sets global settings such as the random seed and device. It prepares the environment so that all subsequent loading, training, and evaluation steps run consistently and reproducibly, both in Colab and locally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UA2mNpY27JjF"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install wandb weave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QgqWE6wM_Y_N"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install fiftyone==1.10.0 sympy==1.12 torch==2.9.0 torchvision==0.20.0 numpy open-clip-torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgrrXotQ0t0k"
      },
      "source": [
        "## Imports\n",
        "\n",
        "Here we import all libraries and modules needed for the final assessment: PyTorch, torchvision transforms, W&B, dataset utilities, training utilities, and the model classes used across all three stages. Centralizing imports keeps the notebook organized and ensures that each component is available when needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUod4cKbh8ke"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#import torchvision.transforms as transforms\n",
        "import torchvision.transforms.v2 as transforms\n",
        "\n",
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h2BgKorid2q",
        "outputId": "134cc64d-ca78-48b0-fa09-17c5046070f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "STORAGE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\")\n",
        "TMP_STORAGE_PATH = \"/content\"\n",
        "\n",
        "# DATA_PATH = STORAGE_PATH / \"data/assessment\"\n",
        "DATA_PATH = TMP_STORAGE_PATH / \"data/assessment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39rSQEHcxASY"
      },
      "outputs": [],
      "source": [
        "!cp -r \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/multimodal_training_workshop/data\" /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U43hLaLjMRxt",
        "outputId": "bbbfd711-bd23-49ee-9e59-41906ee45309"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9KwN-8AMVV_"
      },
      "outputs": [],
      "source": [
        "from src.utility import set_seeds\n",
        "from src.datasets import compute_dataset_mean_std_neu, get_cilp_dataloaders\n",
        "from src.training import compute_class_weights, get_rgb_inputs, train_model, init_wandb, train_with_batch_loss\n",
        "#from src.visualization import build_fusion_comparison_df, plot_losses\n",
        "from src.models import CILPBackbone, ContrastivePretraining, Classifier, Projector, EmbedderMaxPool, RGB2LiDARClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7nPUUqg-b35"
      },
      "outputs": [],
      "source": [
        "import importlib, src.training as training\n",
        "importlib.reload(training)\n",
        "from src.training import train_with_batch_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3l3hqzHd0rux"
      },
      "source": [
        "## Constants\n",
        "\n",
        "We define key configuration values such as the seed, batch size, image size, number of workers, and label mappings. These constants ensure consistent behavior across all stages and make the hyperparameters easy to adjust or reference later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETqCFts35qPy"
      },
      "outputs": [],
      "source": [
        "SEED = 51\n",
        "NUM_WORKERS = os.cpu_count()  # Number of CPU cores\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = 64\n",
        "\n",
        "CLASSES = [\"cubes\", \"spheres\"]\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "LABEL_MAP = {\"cubes\": 0, \"spheres\": 1}\n",
        "\n",
        "VALID_BATCHES = 10\n",
        "N = 12500"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nmi7z0qDPAqc",
        "outputId": "3ae65332-0499-4a97-924c-0bcec9467228"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdQmqB-kMQ1o",
        "outputId": "040ae9f9-e2e8-4a1e-8089-6099605b360c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All random seeds set to 51 for reproducibility\n"
          ]
        }
      ],
      "source": [
        "# Usage: Call this function at the beginning and before each training phase\n",
        "set_seeds(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS0HyPjdNgFC"
      },
      "source": [
        "# Integration of Wandb\n",
        "\n",
        "This section authenticates with Weights & Biases using the API key stored in Colab Secrets. Initializing W&B enables automatic logging of losses, metrics, hyperparameters, and summary statistics for all training stages. This satisfies the experiment-tracking requirement of the assessment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-blsH0WNfaV",
        "outputId": "23bc9863-69ed-4088-b8a3-e33caebaff53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load W&B API key from Colab Secrets and make it available as env variable\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYWacedKNnVT"
      },
      "source": [
        "# Loading and preparation of Data\n",
        "\n",
        "We compute dataset statistics, define preprocessing transforms, and build train/validation/test DataLoaders for the assessment dataset. This ensures that all stages—CILP pretraining, projector training, and final classification—operate on consistently preprocessed and reproducible batches of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQmxRnRZNu6Y"
      },
      "outputs": [],
      "source": [
        "## Final: dynamisch\n",
        "# mean, std = compute_dataset_mean_std(root_dir=root, img_size=IMG_SIZE)\n",
        "mean, std = compute_dataset_mean_std_neu(root_dir=DATA_PATH, img_size=IMG_SIZE, seed=SEED)\n",
        "print(mean, std)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRuSY96XPeTP"
      },
      "outputs": [],
      "source": [
        "## Final: dynamisch\n",
        "img_transforms = transforms.Compose([\n",
        "    transforms.ToImage(),   # Scales data into [0,1]\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize(([0.0051, 0.0052, 0.0051, 1.0000]), ([5.8023e-02, 5.8933e-02, 5.8108e-02, 2.4509e-07]))     ## assessment dataset\n",
        "    # transforms.Normalize(mean.tolist(), std.tolist())     ## assessment dataset\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hofSI-z_N9d8",
        "outputId": "e50de5ec-1174-44ef-e9d5-2d9ebc0e08dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CILP] Total samples: 12500\n",
            "[CILP] Train: 10962, Val: 320, Test: 1218\n",
            "0 torch.Size([4, 64, 64]) torch.Size([1, 64, 64]) torch.Size([1])\n"
          ]
        }
      ],
      "source": [
        "set_seeds(SEED)\n",
        "\n",
        "train_data, train_dataloader, valid_data, val_dataloader, test_data, test_dataloader  = get_cilp_dataloaders(\n",
        "    str(DATA_PATH),\n",
        "    VALID_BATCHES,\n",
        "    test_frac=0.10,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    img_transforms=img_transforms,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "for i, sample in enumerate(train_data):\n",
        "    print(i, *(x.shape for x in sample))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMTis7eq20M2"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "This section introduces the architectural choices for the CILP backbone and projector, as well as the rationale behind using a two-layer projection head. It describes how embeddings are produced and how the shared embedding space is leveraged across the three training stages. This conceptual grounding precedes the implementation of Stage 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IMEYq_hVDQd"
      },
      "source": [
        "I use a 2-layer projection head with a 512-dimensional hidden layer between the backbone’s CNN features and the final CILP embedding.\n",
        "The 512 dimension is standard in contrastive learning literature (e.g., CLIP, SimCLR), as it provides a good balance between model expressiveness and computational efficiency.\n",
        "It allows a nonlinear transformation from the high-dimensional CNN output into the shared embedding space while avoiding overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW5pD-KP3Urk"
      },
      "source": [
        "## Stage 1: CILP contrastive pretraining\n",
        "\n",
        "In this stage, we train a dual-encoder contrastive model to align RGB and LiDAR representations in a shared embedding space. The section defines the batch loss function, sets up optimizers, and runs the contrastive training loop using train_with_batch_loss. The best-performing model is saved and logged to W&B. After training, the CILP encoders are frozen for later stages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tGxH6eynPFhe"
      },
      "outputs": [],
      "source": [
        "BEST_EMBEDDER = EmbedderMaxPool\n",
        "FEATURE_DIM = 128\n",
        "CILP_EMB_SIZE = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0m1rjwB5wLV"
      },
      "outputs": [],
      "source": [
        "set_seeds(SEED)\n",
        "\n",
        "img_embedder = CILPBackbone(\n",
        "    in_ch=4,\n",
        "    embedder_cls=BEST_EMBEDDER,\n",
        "    feature_dim=FEATURE_DIM,\n",
        "    emb_size=CILP_EMB_SIZE\n",
        ").to(device)\n",
        "\n",
        "lidar_embedder = CILPBackbone(\n",
        "    in_ch=1,\n",
        "    embedder_cls=BEST_EMBEDDER,\n",
        "    feature_dim=FEATURE_DIM,\n",
        "    emb_size=CILP_EMB_SIZE\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTHa8rUL6BNO"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "set_seeds(SEED)\n",
        "\n",
        "CILP_model = ContrastivePretraining(img_embedder, lidar_embedder).to(device)\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_lidar = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cco0d02lWE1C"
      },
      "outputs": [],
      "source": [
        "def cilp_batch_loss_fn(model, batch, device):\n",
        "    \"\"\"\n",
        "    outputs: (logits_per_img, logits_per_lidar), each of shape (B, B)\n",
        "\n",
        "    We build ground-truth indices 0..B-1 so that:\n",
        "      - row i in logits_per_img should classify LiDAR i as the correct match\n",
        "      - row i in logits_per_lidar should classify RGB i as the correct match\n",
        "    \"\"\"\n",
        "    rgb, lidar, _ = batch\n",
        "    rgb = rgb.to(device)\n",
        "    lidar = lidar.to(device)\n",
        "\n",
        "    logits_per_img, logits_per_lidar = model(rgb, lidar)   # (B, B)\n",
        "    B = logits_per_img.size(0)\n",
        "    ground_truth = torch.arange(B, dtype=torch.long, device=device)\n",
        "\n",
        "    loss_i = loss_img(logits_per_img, ground_truth)\n",
        "    loss_l = loss_lidar(logits_per_lidar, ground_truth)\n",
        "\n",
        "    total_loss = (loss_i + loss_l) / 2.0\n",
        "\n",
        "    return total_loss, logits_per_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OdOFq5DD-Ol5"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "def train_with_batch_loss(\n",
        "    model,\n",
        "    optimizer,\n",
        "    train_dataloader,\n",
        "    val_dataloader,\n",
        "    batch_loss_fn,\n",
        "    epochs,\n",
        "    model_save_path,\n",
        "    log_to_wandb=False,\n",
        "    device=None,\n",
        "    extra_args=None,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generic training loop where loss is computed by a batch_loss_fn.\n",
        "\n",
        "    batch_loss_fn(model, batch, device, **extra_args) must return either:\n",
        "      - loss\n",
        "      - (loss, log_dict)\n",
        "    \"\"\"\n",
        "    best_val_loss = float(\"inf\")\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    max_gpu_mem_mb = 0.0\n",
        "    if use_cuda:\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        start_time = time.time()\n",
        "        print(f\"[BatchLoss] Epoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # ---------- TRAIN ----------\n",
        "        model.train()\n",
        "        train_loss_epoch = 0.0\n",
        "\n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            out = batch_loss_fn(model, batch, device, **(extra_args or {}))\n",
        "            if isinstance(out, tuple):\n",
        "                loss, log_dict = out\n",
        "            else:\n",
        "                loss, log_dict = out, {}\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_epoch += loss.item()\n",
        "\n",
        "        train_loss_epoch /= (step + 1)\n",
        "        train_losses.append(train_loss_epoch)\n",
        "\n",
        "        # ---------- VALID ----------\n",
        "        model.eval()\n",
        "        valid_loss_epoch = 0.0\n",
        "        with torch.no_grad():\n",
        "            for step, batch in enumerate(val_dataloader):\n",
        "                out = batch_loss_fn(model, batch, device, **(extra_args or {}))\n",
        "                if isinstance(out, tuple):\n",
        "                    loss, _ = out\n",
        "                else:\n",
        "                    loss = out\n",
        "                valid_loss_epoch += loss.item()\n",
        "\n",
        "        valid_loss_epoch /= (step + 1)\n",
        "        valid_losses.append(valid_loss_epoch)\n",
        "\n",
        "        # ---------- SAVE BEST ----------\n",
        "        if valid_loss_epoch < best_val_loss:\n",
        "            best_val_loss = valid_loss_epoch\n",
        "            torch.save(model.state_dict(), model_save_path)\n",
        "            print(\"Found and saved better weights for the model\")\n",
        "\n",
        "        # ---------- LOG / PRINT ----------\n",
        "        epoch_time = time.time() - start_time\n",
        "        if use_cuda:\n",
        "            gpu_mem_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
        "            max_gpu_mem_mb = max(max_gpu_mem_mb, gpu_mem_mb)\n",
        "        else:\n",
        "            gpu_mem_mb = 0.0\n",
        "\n",
        "        print(\n",
        "            f\"epoch {epoch} train loss: {train_loss_epoch}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"epoch {epoch} valid loss: {valid_loss_epoch}\"\n",
        "        )\n",
        "\n",
        "        if log_to_wandb:\n",
        "            import wandb\n",
        "            wandb.log(\n",
        "                {\n",
        "                    \"model\": model.__class__.__name__,\n",
        "                    \"epoch\": epoch + 1,\n",
        "                    \"train_loss\": train_loss_epoch,\n",
        "                    \"valid_loss\": valid_loss_epoch,\n",
        "                    \"lr\": optimizer.param_groups[0][\"lr\"],\n",
        "                    \"epoch_time\": epoch_time,\n",
        "                    \"max_gpu_mem_mb_epoch\": gpu_mem_mb if use_cuda else 0.0,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return {\n",
        "        \"train_losses\": train_losses,\n",
        "        \"valid_losses\": valid_losses,\n",
        "        \"best_valid_loss\": float(best_val_loss),\n",
        "        \"max_gpu_mem_mb\": float(max_gpu_mem_mb),\n",
        "        \"num_params\": sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0rDQfnNMKlET",
        "outputId": "7188934a-83b0-4a71-9e67-e9784c5c829f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'finish_previous'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">unknown_run</strong> at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/77qp5kjh' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/77qp5kjh</a><br> View project at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251207_175746-77qp5kjh/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/wandb/run-20251207_180035-51n9pf07</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/51n9pf07' target=\"_blank\">unknown_run</a></strong> to <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/51n9pf07' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/51n9pf07</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BatchLoss] Epoch 1/5\n",
            "Found and saved better weights for the model\n",
            "epoch 0 train loss: 2.6652660460499993\n",
            "epoch 0 valid loss: 2.601843976974487\n",
            "[BatchLoss] Epoch 2/5\n",
            "Found and saved better weights for the model\n",
            "epoch 1 train loss: 2.592795539320561\n",
            "epoch 1 valid loss: 2.5814525842666627\n",
            "[BatchLoss] Epoch 3/5\n",
            "Found and saved better weights for the model\n",
            "epoch 2 train loss: 2.579532387660958\n",
            "epoch 2 valid loss: 2.5765286922454833\n",
            "[BatchLoss] Epoch 4/5\n",
            "Found and saved better weights for the model\n",
            "epoch 3 train loss: 2.570431894029093\n",
            "epoch 3 valid loss: 2.573051905632019\n",
            "[BatchLoss] Epoch 5/5\n",
            "Found and saved better weights for the model\n",
            "epoch 4 train loss: 2.56716895870298\n",
            "epoch 4 valid loss: 2.568479371070862\n",
            "[5.1] Best CILP validation loss: 2.5685\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▅▆█</td></tr><tr><td>epoch_time</td><td>▁▅▁█▃</td></tr><tr><td>lr</td><td>▁▁▁▁▁</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>▁████</td></tr><tr><td>train_loss</td><td>█▃▂▁▁</td></tr><tr><td>valid_loss</td><td>█▄▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>cilp_best_val_loss</td><td>2.56848</td></tr><tr><td>epoch</td><td>5</td></tr><tr><td>epoch_time</td><td>7.05844</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>390.20605</td></tr><tr><td>model</td><td>ContrastivePretraini...</td></tr><tr><td>train_loss</td><td>2.56717</td></tr><tr><td>valid_loss</td><td>2.56848</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">unknown_run</strong> at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/51n9pf07' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/51n9pf07</a><br> View project at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251207_180035-51n9pf07/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## train CILP\n",
        "EPOCHS_CILP = 5\n",
        "LR_CILP = 0.0001\n",
        "\n",
        "opt = torch.optim.Adam(CILP_model.parameters(), LR_CILP)\n",
        "\n",
        "best_val = float(\"inf\")\n",
        "\n",
        "# Path where best model is saved\n",
        "checkpoint_dir = STORAGE_PATH / \"checkpoints\"\n",
        "checkpoint_dir.mkdir(exist_ok=True)\n",
        "model_save_path=checkpoint_dir / \"cilp_model.pth\"\n",
        "\n",
        "init_wandb(\n",
        "    model=CILP_model,\n",
        "    opt_name = opt.__class__.__name__\n",
        ")\n",
        "\n",
        "results = train_with_batch_loss(\n",
        "    model=CILP_model,\n",
        "    optimizer=opt,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    batch_loss_fn=cilp_batch_loss_fn,\n",
        "    epochs=EPOCHS_CILP,\n",
        "    model_save_path=model_save_path,\n",
        "    log_to_wandb=True,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "best_cilp_val = results[\"best_valid_loss\"]\n",
        "print(f\"[5.1] Best CILP validation loss: {best_cilp_val:.4f}\")\n",
        "\n",
        "\n",
        "wandb.run.summary[\"cilp_best_val_loss\"] = best_cilp_val\n",
        "# End wandb run before starting the next model\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rSHK5JmnCS1",
        "outputId": "07ce9ad4-5bef-42fa-80de-e381bd1df349"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ContrastivePretraining(\n",
              "  (img_embedder): CILPBackbone(\n",
              "    (encoder): EmbedderMaxPool(\n",
              "      (conv1): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (dense_emb): Sequential(\n",
              "      (0): Linear(in_features=8192, out_features=512, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=512, out_features=200, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (lidar_embedder): CILPBackbone(\n",
              "    (encoder): EmbedderMaxPool(\n",
              "      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (dense_emb): Sequential(\n",
              "      (0): Linear(in_features=8192, out_features=512, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=512, out_features=200, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (cos): CosineSimilarity()\n",
              ")"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "## freeze pre-trained model\n",
        "for param in CILP_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "CILP_model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0PAZObVnI4v"
      },
      "source": [
        "## Stage 2: Cross-Modal Projection\n",
        "\n",
        "Here we train a projector that maps CILP RGB embeddings into the LiDAR-CNN feature space. Using a frozen CILP model and a pretrained LiDAR classifier, we minimize an MSE loss between predicted LiDAR embeddings and true LiDAR embeddings. This stage enables the system to “pretend” RGB images look like LiDAR feature vectors, allowing downstream LiDAR-based classification from RGB alone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWgPU4P7yXFq",
        "outputId": "c3fd499d-3007-4280-e94d-578d83c14d97"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Classifier(\n",
              "  (embedder): Sequential(\n",
              "    (0): Conv2d(1, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (3): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (4): ReLU()\n",
              "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (6): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (7): ReLU()\n",
              "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (9): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): ReLU()\n",
              "    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (12): Flatten(start_dim=1, end_dim=-1)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=3200, out_features=100, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=100, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 100,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# load pre-trained lidar_cnn classifier\n",
        "lidar_cnn_path = STORAGE_PATH / \"checkpoints/lidar_cnn.pt\"\n",
        "\n",
        "lidar_cnn = Classifier(in_ch=1).to(device)\n",
        "lidar_cnn.load_state_dict(torch.load(lidar_cnn_path, weights_only=True))\n",
        "\n",
        "for param in lidar_cnn.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "lidar_cnn.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfGuIUEpWYF4"
      },
      "outputs": [],
      "source": [
        "def projector_batch_loss_fn(model, batch, device, CILP_model, lidar_cnn):\n",
        "    rgb_img, lidar_depth, _ = batch\n",
        "    rgb_img = rgb_img.to(device)\n",
        "    lidar_depth = lidar_depth.to(device)\n",
        "\n",
        "    # Use frozen encoders\n",
        "    CILP_model.eval()\n",
        "    lidar_cnn.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_embs = CILP_model.img_embedder(rgb_img)      # (B, CILP_EMB_SIZE)\n",
        "        lidar_embs = lidar_cnn.get_embs(lidar_depth)     # (B, lidar_dim)\n",
        "\n",
        "    pred_lidar_embs = model(img_embs)\n",
        "\n",
        "    loss = F.mse_loss(pred_lidar_embs, lidar_embs)\n",
        "\n",
        "    # match the convention used in train_with_batch_loss\n",
        "    return loss, {\"loss\": loss.item()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkcLx0ikHvok"
      },
      "outputs": [],
      "source": [
        "img_dim = CILP_EMB_SIZE\n",
        "lidar_dim = 200 * 4 * 4\n",
        "\n",
        "set_seeds(SEED)\n",
        "\n",
        "projector = Projector(img_dim, lidar_dim).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YdthbedXSMxz",
        "outputId": "52adc6cf-4602-46c4-a9de-f8a1375a1d16"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/wandb/run-20251207_180628-kr4d48gi</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/kr4d48gi' target=\"_blank\">unknown_run</a></strong> to <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/kr4d48gi' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/kr4d48gi</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BatchLoss] Epoch 1/40\n",
            "Found and saved better weights for the model\n",
            "epoch 0 train loss: 1.6388600946169847\n",
            "epoch 0 valid loss: 1.2890848696231842\n",
            "[BatchLoss] Epoch 2/40\n",
            "Found and saved better weights for the model\n",
            "epoch 1 train loss: 1.6224197928319897\n",
            "epoch 1 valid loss: 1.2846409142017365\n",
            "[BatchLoss] Epoch 3/40\n",
            "Found and saved better weights for the model\n",
            "epoch 2 train loss: 1.6049904155800914\n",
            "epoch 2 valid loss: 1.2684852600097656\n",
            "[BatchLoss] Epoch 4/40\n",
            "Found and saved better weights for the model\n",
            "epoch 3 train loss: 1.5911426063169514\n",
            "epoch 3 valid loss: 1.2645985782146454\n",
            "[BatchLoss] Epoch 5/40\n",
            "Found and saved better weights for the model\n",
            "epoch 4 train loss: 1.5782182199216028\n",
            "epoch 4 valid loss: 1.2419158279895783\n",
            "[BatchLoss] Epoch 6/40\n",
            "Found and saved better weights for the model\n",
            "epoch 5 train loss: 1.563390554740415\n",
            "epoch 5 valid loss: 1.2293817102909088\n",
            "[BatchLoss] Epoch 7/40\n",
            "Found and saved better weights for the model\n",
            "epoch 6 train loss: 1.5521342682908152\n",
            "epoch 6 valid loss: 1.2224450349807738\n",
            "[BatchLoss] Epoch 8/40\n",
            "Found and saved better weights for the model\n",
            "epoch 7 train loss: 1.539729038177178\n",
            "epoch 7 valid loss: 1.2170666396617889\n",
            "[BatchLoss] Epoch 9/40\n",
            "Found and saved better weights for the model\n",
            "epoch 8 train loss: 1.525018750924116\n",
            "epoch 8 valid loss: 1.2014193058013916\n",
            "[BatchLoss] Epoch 10/40\n",
            "Found and saved better weights for the model\n",
            "epoch 9 train loss: 1.5151330518443682\n",
            "epoch 9 valid loss: 1.1985203564167022\n",
            "[BatchLoss] Epoch 11/40\n",
            "Found and saved better weights for the model\n",
            "epoch 10 train loss: 1.501638357576571\n",
            "epoch 10 valid loss: 1.182257729768753\n",
            "[BatchLoss] Epoch 12/40\n",
            "Found and saved better weights for the model\n",
            "epoch 11 train loss: 1.4897928248371994\n",
            "epoch 11 valid loss: 1.1784503281116485\n",
            "[BatchLoss] Epoch 13/40\n",
            "Found and saved better weights for the model\n",
            "epoch 12 train loss: 1.4764009213935563\n",
            "epoch 12 valid loss: 1.173989474773407\n",
            "[BatchLoss] Epoch 14/40\n",
            "Found and saved better weights for the model\n",
            "epoch 13 train loss: 1.467362789556994\n",
            "epoch 13 valid loss: 1.151483517885208\n",
            "[BatchLoss] Epoch 15/40\n",
            "Found and saved better weights for the model\n",
            "epoch 14 train loss: 1.4539783185685586\n",
            "epoch 14 valid loss: 1.146673208475113\n",
            "[BatchLoss] Epoch 16/40\n",
            "Found and saved better weights for the model\n",
            "epoch 15 train loss: 1.444453133999953\n",
            "epoch 15 valid loss: 1.1401613771915435\n",
            "[BatchLoss] Epoch 17/40\n",
            "Found and saved better weights for the model\n",
            "epoch 16 train loss: 1.4296012243680787\n",
            "epoch 16 valid loss: 1.1338951587677002\n",
            "[BatchLoss] Epoch 18/40\n",
            "Found and saved better weights for the model\n",
            "epoch 17 train loss: 1.4175923241857897\n",
            "epoch 17 valid loss: 1.1175341844558715\n",
            "[BatchLoss] Epoch 19/40\n",
            "epoch 18 train loss: 1.4066905907371587\n",
            "epoch 18 valid loss: 1.12904834151268\n",
            "[BatchLoss] Epoch 20/40\n",
            "Found and saved better weights for the model\n",
            "epoch 19 train loss: 1.3967210932781822\n",
            "epoch 19 valid loss: 1.1075504064559936\n",
            "[BatchLoss] Epoch 21/40\n",
            "epoch 20 train loss: 1.384663644938441\n",
            "epoch 20 valid loss: 1.1255026698112487\n",
            "[BatchLoss] Epoch 22/40\n",
            "Found and saved better weights for the model\n",
            "epoch 21 train loss: 1.3727073887286827\n",
            "epoch 21 valid loss: 1.090850555896759\n",
            "[BatchLoss] Epoch 23/40\n",
            "Found and saved better weights for the model\n",
            "epoch 22 train loss: 1.3622930069067325\n",
            "epoch 22 valid loss: 1.081387060880661\n",
            "[BatchLoss] Epoch 24/40\n",
            "Found and saved better weights for the model\n",
            "epoch 23 train loss: 1.3520212027935954\n",
            "epoch 23 valid loss: 1.0732344746589662\n",
            "[BatchLoss] Epoch 25/40\n",
            "Found and saved better weights for the model\n",
            "epoch 24 train loss: 1.340673549830565\n",
            "epoch 24 valid loss: 1.061092072725296\n",
            "[BatchLoss] Epoch 26/40\n",
            "Found and saved better weights for the model\n",
            "epoch 25 train loss: 1.3267701196740245\n",
            "epoch 25 valid loss: 1.0555457651615143\n",
            "[BatchLoss] Epoch 27/40\n",
            "Found and saved better weights for the model\n",
            "epoch 26 train loss: 1.31973730254243\n",
            "epoch 26 valid loss: 1.0519681870937347\n",
            "[BatchLoss] Epoch 28/40\n",
            "Found and saved better weights for the model\n",
            "epoch 27 train loss: 1.3076756420539835\n",
            "epoch 27 valid loss: 1.0502884685993195\n",
            "[BatchLoss] Epoch 29/40\n",
            "Found and saved better weights for the model\n",
            "epoch 28 train loss: 1.2984565606242733\n",
            "epoch 28 valid loss: 1.0339766442775726\n",
            "[BatchLoss] Epoch 30/40\n",
            "Found and saved better weights for the model\n",
            "epoch 29 train loss: 1.289205605785052\n",
            "epoch 29 valid loss: 1.0218417048454285\n",
            "[BatchLoss] Epoch 31/40\n",
            "epoch 30 train loss: 1.2786671824274007\n",
            "epoch 30 valid loss: 1.0230374872684478\n",
            "[BatchLoss] Epoch 32/40\n",
            "Found and saved better weights for the model\n",
            "epoch 31 train loss: 1.2667294620421894\n",
            "epoch 31 valid loss: 1.0061749517917633\n",
            "[BatchLoss] Epoch 33/40\n",
            "Found and saved better weights for the model\n",
            "epoch 32 train loss: 1.2584988492274145\n",
            "epoch 32 valid loss: 1.004060161113739\n",
            "[BatchLoss] Epoch 34/40\n",
            "Found and saved better weights for the model\n",
            "epoch 33 train loss: 1.2491759902726838\n",
            "epoch 33 valid loss: 0.9975543141365051\n",
            "[BatchLoss] Epoch 35/40\n",
            "Found and saved better weights for the model\n",
            "epoch 34 train loss: 1.238819341649089\n",
            "epoch 34 valid loss: 0.984465229511261\n",
            "[BatchLoss] Epoch 36/40\n",
            "epoch 35 train loss: 1.2307139710027573\n",
            "epoch 35 valid loss: 0.9847848176956177\n",
            "[BatchLoss] Epoch 37/40\n",
            "Found and saved better weights for the model\n",
            "epoch 36 train loss: 1.2214579725126078\n",
            "epoch 36 valid loss: 0.9762007892131805\n",
            "[BatchLoss] Epoch 38/40\n",
            "epoch 37 train loss: 1.2145596606689586\n",
            "epoch 37 valid loss: 0.9842787086963654\n",
            "[BatchLoss] Epoch 39/40\n",
            "Found and saved better weights for the model\n",
            "epoch 38 train loss: 1.2068802294326804\n",
            "epoch 38 valid loss: 0.9622524380683899\n",
            "[BatchLoss] Epoch 40/40\n",
            "epoch 39 train loss: 1.1983857139160758\n",
            "epoch 39 valid loss: 0.9646808803081512\n",
            "[5.2] Best projector validation MSE: 0.9623\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_time</td><td>▄▂▁▇▂▁▂▆▁▂▅▄▂▁▇▂▂▄▅▁▁▆▂▂▂█▄▄▇▅▁▄▅▂▁▆▂▁▁▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>valid_loss</td><td>███▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>epoch_time</td><td>4.89051</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>327.29541</td></tr><tr><td>model</td><td>Projector</td></tr><tr><td>projector_best_val_mse</td><td>0.96225</td></tr><tr><td>train_loss</td><td>1.19839</td></tr><tr><td>valid_loss</td><td>0.96468</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">unknown_run</strong> at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/kr4d48gi' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/kr4d48gi</a><br> View project at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251207_180628-kr4d48gi/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "EPOCHS_PROJ = 40\n",
        "LR_PROJECTOR = 1e-4\n",
        "model_save_path=checkpoint_dir / \"projector.pth\"\n",
        "\n",
        "opt = torch.optim.Adam(projector.parameters(), LR_PROJECTOR)\n",
        "\n",
        "best_val_proj = float(\"inf\")\n",
        "\n",
        "init_wandb(\n",
        "    model=projector,\n",
        "    opt_name = opt.__class__.__name__\n",
        ")\n",
        "\n",
        "results = train_with_batch_loss(\n",
        "    model=projector,\n",
        "    optimizer=opt,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    batch_loss_fn=projector_batch_loss_fn,\n",
        "    epochs=EPOCHS_PROJ,\n",
        "    model_save_path=model_save_path,\n",
        "    log_to_wandb=True,\n",
        "    device=device,\n",
        "    extra_args={\n",
        "        \"CILP_model\": CILP_model,\n",
        "        \"lidar_cnn\": lidar_cnn,\n",
        "    }\n",
        ")\n",
        "\n",
        "best_proj_val = results[\"best_valid_loss\"]\n",
        "print(f\"[5.2] Best projector validation MSE: {best_proj_val:.4f}\")\n",
        "\n",
        "wandb.run.summary[\"projector_best_val_mse\"] = best_proj_val\n",
        "# End wandb run before starting the next model\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp4pFlFynU9S"
      },
      "source": [
        "## Stage 3: RGB2LiDARClassifier\n",
        "\n",
        "In the final stage, we build and train a model that chains together the CILP backbone, the trained projector, and the LiDAR classifier. Only the projector is trainable; all other components remain frozen. The model learns to classify cubes and spheres directly from RGB input by leveraging the LiDAR feature space. The section logs training and validation metrics and reports the final accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vCONGPIm7JjP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_rgb2lidar_classifier(\n",
        "    model,\n",
        "    train_loader,\n",
        "    val_loader,\n",
        "    epochs,\n",
        "    lr,\n",
        "    device,\n",
        "):\n",
        "    model = model.to(device)\n",
        "\n",
        "    # IMPORTANT: Only projector is trained\n",
        "    optimizer = torch.optim.Adam(model.projector.parameters(), lr=lr)\n",
        "    loss_fn = nn.BCEWithLogitsLoss()\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_acc\": []\n",
        "    }\n",
        "\n",
        "    # sanity check: grads enabled\n",
        "    torch.set_grad_enabled(True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
        "\n",
        "        # ---------- TRAIN ----------\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for imgs, _, labels in train_loader:\n",
        "            imgs = imgs.to(device)\n",
        "            labels = labels.float().view(-1, 1).to(device)   # [B,1]\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            logits = model(imgs)          # [B,1] – MUST be computed with grad\n",
        "\n",
        "            # DEBUG: verify graph is alive\n",
        "            if epoch == 0 and running_loss == 0.0:\n",
        "                print(\"DEBUG logits.requires_grad:\", logits.requires_grad)\n",
        "\n",
        "            loss = loss_fn(logits, labels)\n",
        "\n",
        "            # DEBUG: verify loss is linked to graph\n",
        "            if epoch == 0 and running_loss == 0.0:\n",
        "                print(\"DEBUG loss.requires_grad:\", loss.requires_grad)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        train_loss = running_loss / len(train_loader)\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "\n",
        "        # ---------- VALID ----------\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for imgs, _, labels in val_loader:\n",
        "                imgs = imgs.to(device)\n",
        "                labels = labels.float().view(-1, 1).to(device)\n",
        "\n",
        "                logits = model(imgs)              # [B,1]\n",
        "                loss = loss_fn(logits, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                probs = torch.sigmoid(logits)     # [B,1], 0–1\n",
        "                preds = (probs >= 0.5).long()     # [B,1]\n",
        "                correct += (preds.view(-1) == labels.view(-1).long()).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_acc = correct / total\n",
        "\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        print(f\"train_loss={train_loss:.4f}  val_loss={val_loss:.4f}  val_acc={val_acc*100:.2f}%\")\n",
        "\n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "srtFQnE9yDVq",
        "outputId": "b7cb9bbc-43bc-4a83-cd82-283ff86a4b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANY trainable in projector? True\n",
            "\n",
            "Epoch 1/5\n",
            "DEBUG logits.requires_grad: False\n",
            "DEBUG loss.requires_grad: False\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-378877429.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m results = train_rgb2lidar_classifier(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrgb2lidar_clf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3217486627.py\u001b[0m in \u001b[0;36mtrain_rgb2lidar_classifier\u001b[0;34m(model, train_loader, val_loader, epochs, lr, device)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DEBUG loss.requires_grad:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "LR_RGB2LIDAR = 1e-3\n",
        "EPOCHS_RGB2LIDAR = 5\n",
        "\n",
        "set_seeds(SEED)\n",
        "\n",
        "rgb2lidar_clf = RGB2LiDARClassifier(\n",
        "    CILP=CILP_model,\n",
        "    projector=projector,\n",
        "    lidar_cnn=lidar_cnn,\n",
        ").to(device)\n",
        "\n",
        "#class_weights = compute_class_weights(train_data, NUM_CLASSES).to(device)\n",
        "#loss_func = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
        "#loss_func = nn.BCEWithLogitsLoss()\n",
        "\n",
        "#opt = torch.optim.Adam(rgb2lidar_clf.parameters(), lr=LR_RGB2LIDAR)\n",
        "\n",
        "# 1) Freeze img_embedder & lidar_cnn\n",
        "for p in rgb2lidar_clf.img_embedder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "for p in rgb2lidar_clf.shape_classifier.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# 2) Ensure projector is trainable\n",
        "for p in rgb2lidar_clf.projector.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "print(\"ANY trainable in projector?\",\n",
        "      any(p.requires_grad for p in rgb2lidar_clf.projector.parameters()))\n",
        "\n",
        "\n",
        "results = train_rgb2lidar_classifier(\n",
        "    model=rgb2lidar_clf,\n",
        "    train_loader=train_dataloader,\n",
        "    val_loader=val_dataloader,\n",
        "    epochs=EPOCHS_RGB2LIDAR,\n",
        "    lr=LR_RGB2LIDAR,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "best_rgb2lidar_val = results[\"best_valid_loss\"]\n",
        "print(f\"[5.3] Best validation loss: {best_rgb2lidar_val:.4f}\")\n",
        "best_rgb2lidar_acc = results[\"best_valid_acc\"]\n",
        "print(f\"[5.3] Best validation accuracy: {best_rgb2lidar_acc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
