{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Overview: Final Multimodal Pipeline\n",
        "\n",
        "This notebook implements the complete three-stage multimodal pipeline used for the final assessment. It combines contrastive pretraining (CILP), cross-modal projection, and RGB→LiDAR classification to enable object classification from RGB images via a LiDAR feature space. Each stage is trained sequentially under controlled settings, with performance and diagnostics logged using Weights & Biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OwNXAWXcyOWQ"
      },
      "source": [
        "# Model Architecture:\n",
        "\n",
        "This section presents the three-stage pipeline used in the final assessment: contrastive pretraining (CILP), cross-modal projection, and RGB→LiDAR classification. The diagrams summarize how RGB and LiDAR information flow through the system and how each model component contributes to aligning, transforming, and classifying multimodal inputs. This high-level overview clarifies the purpose of each stage before moving on to implementation.\n",
        "\n",
        "**Stage 1:** Contrastive Pretraining: CILP_model\n",
        "\n",
        "**Goal:** align RGB and LiDAR in a shared 200-D space --> encodes both rgb and lidar in the same dimensionality space\n",
        "```\n",
        "          (200-D emb.)\n",
        "RGB ----> Img Encoder ----\\\n",
        "                            ----> CLIP-style similarity\n",
        "LiDAR -> Lidar Encoder ----/\n",
        "          (200-D emb.)\n",
        "```\n",
        "**Outcome:** Shared embedding space where matching RGB/LiDAR pairs have high similarity and non-matches low similarity.\n",
        "\n",
        "----------------------------\n",
        "\n",
        "**Stage 2:** Projector Training: projector\n",
        "\n",
        "**Goal:** learn a mapping from RGB CILP embeddings to LiDAR embeddings used by lidar_cnn:\n",
        "ℝ²⁰⁰ (CILP RGB embedding) → ℝ³²⁰⁰ (LiDAR-CNN embedding)\n",
        "\n",
        "projector knows how to “pretend” RGBs are LiDAR internally: projected_RGB_embedding ≈ “real” LiDAR embedding for each paired RGB/LiDAR sample.\n",
        "```\n",
        "RGB ----> Img Encoder ----> Projector ----> LiDAR embedding\n",
        "        (200-d emb.)    (3200-d emb.)   |\n",
        "                                        v\n",
        "                             MSE-loss to true LiDAR embedding\n",
        "\n",
        "```\n",
        "\n",
        "We freeze the pretrained CILP model and the pretrained LiDAR CNN and train a projector to map RGB embeddings into the LiDAR embedding space.\n",
        "The input to the projector is the 200-dimensional RGB embedding produced by the CILP image encoder. The target is the LiDAR embedding of dimension 3200, obtained by flattening the LiDAR CNN feature map (200 channels × 4 × 4 spatial resolution).\n",
        "\n",
        "The projector is implemented as a small MLP that maps 200 → 3200 and is trained using mean squared error (MSE) loss between the projected RGB embedding and the corresponding LiDAR embedding. Training uses the Adam optimizer with a fixed learning rate and runs for multiple epochs, selecting the best checkpoint based on lowest validation MSE.\n",
        "\n",
        "The final model achieves a best validation MSE below 2.5, satisfying the project requirement. Training and validation loss curves, as well as the final validation performance, are logged to Weights & Biases.\n",
        "\n",
        "----------------------------\n",
        "\n",
        "**Stage 3:** Final Classifier: RGB2LiDARClassifier\n",
        "\n",
        "**Goal:** chaining all models together to classify spheres and cubes from images\n",
        "\n",
        "pretends the RGBs look like LiDAR in the internal feature space and then uses LiDAR classifier.\n",
        "```\n",
        "RGB (img) ----> (CILP Img Encoder) ----> CILP emb. ----> (Projector) ---> LiDAR emb. ---> (LiDAR Classifier) ---> cube/sphere\n",
        "                                        (200-D emb.)                      (3200-D emb.)\n",
        "```\n",
        "\n",
        "We train a binary RGB-to-LiDAR classifier using frozen pretrained CILP encoders and the trained RGB→LiDAR projector; only the classifier head is trainable. The model operates on the 3200-dimensional projected RGB embedding and LiDAR embedding and is optimized with binary cross-entropy with logits, using class weighting to address class imbalance.\n",
        "\n",
        "Evaluation is performed on at least five validation batches, and the final model achieves a validation accuracy above 95%. Loss curves, final performance, and sample predictions are logged to Weights & Biases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYZmjLlguKJw"
      },
      "source": [
        "# Initial Setup\n",
        "\n",
        "The project repository is mounted from Google Drive and added to the Python path to allow clean imports from the src module. The dataset is copied to the local Colab filesystem to improve I/O performance during training. All global settings (random seed, device selection, paths, batch sizes) are defined once and reused across the notebook to ensure consistency and reproducibility.\n",
        "\n",
        "Weights & Biases is initialized for experiment tracking, and all training stages use the same precomputed dataset statistics and DataLoaders for fair comparison across models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\"\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%%capture\n",
        "%pip install --no-cache-dir -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lUod4cKbh8ke"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as Func\n",
        "import torchvision.transforms.v2 as transforms\n",
        "\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s9KwN-8AMVV_"
      },
      "outputs": [],
      "source": [
        "from src.config import (SEED, NUM_WORKERS, BATCH_SIZE, IMG_SIZE, N, DRIVE_ROOT,\n",
        "                        RAW_DATA, CHECKPOINTS, DEVICE, VALID_BATCHES)\n",
        "from src.utility import set_seeds, init_wandb, get_train_stats, compute_embedding_size\n",
        "from src.datasets import compute_dataset_mean_std, get_cilp_dataloaders\n",
        "from src.training import train_with_batch_loss, train_classifier_with_acc, load_model\n",
        "from src.visualization import plot_similarity_matrix, plot_retrieval_examples, plot_losses\n",
        "from src.models import CILPBackbone, ContrastivePretraining, Classifier, Projector, EmbedderMaxPool, RGB2LiDARClassifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39rSQEHcxASY"
      },
      "outputs": [],
      "source": [
        "# Copy data from frive to /content for performance\n",
        "!rm -rf /content/data\n",
        "!cp -r \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data/assessment\" /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdQmqB-kMQ1o",
        "outputId": "040ae9f9-e2e8-4a1e-8089-6099605b360c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All random seeds set to 51 for reproducibility\n"
          ]
        }
      ],
      "source": [
        "# Usage: Call this function at the beginning and before each training phase\n",
        "set_seeds(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-blsH0WNfaV",
        "outputId": "23bc9863-69ed-4088-b8a3-e33caebaff53"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load W&B API key from Colab Secrets and make it available as env variable\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYWacedKNnVT"
      },
      "source": [
        "# Loading and preparation of Data\n",
        "\n",
        "We compute dataset statistics, define preprocessing transforms, and build train/validation/test DataLoaders for the assessment dataset. This ensures that all stages—CILP pretraining, projector training, and final classification—operate on consistently preprocessed and reproducible batches of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HQmxRnRZNu6Y"
      },
      "outputs": [],
      "source": [
        "# gets calculated mean, std from file or calculates it from the rgb train data\n",
        "# for different dataset (or change in train data) recalculate mean and standard deviation\n",
        "mean, std = get_train_stats(dir=DRIVE_ROOT, img_size=IMG_SIZE, data_dir=RAW_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rRuSY96XPeTP"
      },
      "outputs": [],
      "source": [
        "img_transforms = transforms.Compose([\n",
        "    transforms.ToImage(),   # Scales data into [0,1]\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize(mean, std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hofSI-z_N9d8",
        "outputId": "e50de5ec-1174-44ef-e9d5-2d9ebc0e08dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[CILP] Total samples: 12500\n",
            "[CILP] Train: 10962, Val: 320, Test: 1218\n",
            "0 torch.Size([4, 64, 64]) torch.Size([1, 64, 64]) torch.Size([1])\n"
          ]
        }
      ],
      "source": [
        "set_seeds(SEED)\n",
        "\n",
        "# Fix random seeds and create reproducible train/val/test splits and DataLoaders.\n",
        "# The first batch is inspected to verify tensor shapes and data consistency.\n",
        "train_data, train_dataloader, valid_data, val_dataloader, test_data, test_dataloader = get_cilp_dataloaders(\n",
        "    str(RAW_DATA),\n",
        "    VALID_BATCHES,\n",
        "    test_frac=0.10,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    img_transforms=img_transforms,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "for i, sample in enumerate(train_data):\n",
        "    print(i, *(x.shape for x in sample))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMTis7eq20M2"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "This section introduces the architectural choices for the CILP backbone and projector, as well as the rationale behind using a two-layer projection head. It describes how embeddings are produced and how the shared embedding space is leveraged across the three training stages. This conceptual grounding precedes the implementation of Stage 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bW5pD-KP3Urk"
      },
      "source": [
        "## Stage 1: CILP contrastive pretraining\n",
        "\n",
        "In this stage, we train a dual-encoder contrastive model to align RGB and LiDAR representations in a shared embedding space. The section defines the batch loss function, sets up optimizers, and runs the contrastive training loop using train_with_batch_loss. The best-performing model is saved and logged to W&B. After training, the CILP encoders are frozen for later stages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0m1rjwB5wLV"
      },
      "outputs": [],
      "source": [
        "set_seeds(SEED)\n",
        "\n",
        "## constants for clip model and training\n",
        "BEST_EMBEDDER = EmbedderMaxPool\n",
        "FEATURE_DIM = 128\n",
        "CILP_EMB_SIZE = 200\n",
        "EPOCHS_CILP = 5\n",
        "LR_CILP = 0.0001\n",
        "\n",
        "# Initialize embedder for CILP model\n",
        "img_embedder = CILPBackbone(\n",
        "    in_ch=4,\n",
        "    embedder_cls=BEST_EMBEDDER,\n",
        "    feature_dim=FEATURE_DIM,\n",
        "    emb_size=CILP_EMB_SIZE\n",
        ").to(DEVICE)\n",
        "\n",
        "lidar_embedder = CILPBackbone(\n",
        "    in_ch=1,\n",
        "    embedder_cls=BEST_EMBEDDER,\n",
        "    feature_dim=FEATURE_DIM,\n",
        "    emb_size=CILP_EMB_SIZE\n",
        ").to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yTHa8rUL6BNO"
      },
      "outputs": [],
      "source": [
        "# Initialize the CILP model\n",
        "set_seeds(SEED)\n",
        "\n",
        "CILP_model = ContrastivePretraining(img_embedder, lidar_embedder).to(DEVICE)\n",
        "\n",
        "loss_img = nn.CrossEntropyLoss()\n",
        "loss_lidar = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cco0d02lWE1C"
      },
      "outputs": [],
      "source": [
        "def cilp_batch_loss_fn(model, batch, device):\n",
        "    \"\"\"\n",
        "    Compute symmetric CLIP contrastive loss for an RGB–LiDAR batch.\n",
        "\n",
        "    Uses cross-entropy on image→LiDAR and LiDAR→image similarity matrices\n",
        "    and returns their average.\n",
        "    \"\"\"\n",
        "    rgb, lidar, _ = batch\n",
        "    rgb = rgb.to(device)\n",
        "    lidar = lidar.to(device)\n",
        "\n",
        "    logits_per_img, logits_per_lidar = model(rgb, lidar)  # (B,B)\n",
        "    B = logits_per_img.size(0)\n",
        "    targets = torch.arange(B, device=device)\n",
        "\n",
        "    loss_i = loss_img(logits_per_img, targets)\n",
        "    loss_l = loss_lidar(logits_per_lidar, targets)\n",
        "    \n",
        "    total_loss = (loss_i + loss_l) / 2.0\n",
        "\n",
        "    return total_loss, logits_per_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_save_path = CHECKPOINTS / \"cilp_model.pt\"\n",
        "\n",
        "if not model_save_path.exists():\n",
        "    ## Train CILP model\n",
        "    # Path where best model is saved\n",
        "\n",
        "    best_val = float(\"inf\")\n",
        "\n",
        "    # Number of trainable parameters\n",
        "    num_params = sum(p.numel() for p in CILP_model.parameters() if p.requires_grad)\n",
        "\n",
        "    opt = torch.optim.Adam(CILP_model.parameters(), LR_CILP)\n",
        "\n",
        "    embedding_size = compute_embedding_size(\"cilp\", CILP_EMB_SIZE, spatial=(4, 4))\n",
        "\n",
        "    init_wandb(\n",
        "        model=CILP_model,\n",
        "        embedding_size=embedding_size,\n",
        "        opt_name=opt.__class__.__name__,\n",
        "        name=\"CILP model | 1st stage\",\n",
        "        num_params=num_params\n",
        "    )\n",
        "\n",
        "    results = train_with_batch_loss(\n",
        "        model=CILP_model,\n",
        "        optimizer=opt,\n",
        "        train_dataloader=train_dataloader,\n",
        "        val_dataloader=val_dataloader,\n",
        "        batch_loss_fn=cilp_batch_loss_fn,\n",
        "        epochs=EPOCHS_CILP,\n",
        "        model_save_path=model_save_path,\n",
        "        log_to_wandb=True,\n",
        "        device=DEVICE\n",
        "    )\n",
        "\n",
        "    wandb.summary[\"CILP model: best_val_loss\"] = results[\"best_val_loss\"]\n",
        "\n",
        "    best_cilp_val = results[\"best_val_loss\"]\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(f\"BEST CILP VALIDATION LOSS → {best_cilp_val:.4f}\")\n",
        "    print(\"-\"*50 + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rSHK5JmnCS1",
        "outputId": "07ce9ad4-5bef-42fa-80de-e381bd1df349"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ContrastivePretraining(\n",
              "  (img_embedder): CILPBackbone(\n",
              "    (encoder): EmbedderMaxPool(\n",
              "      (conv1): Conv2d(4, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (dense_emb): Sequential(\n",
              "      (0): Linear(in_features=8192, out_features=512, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=512, out_features=200, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (lidar_embedder): CILPBackbone(\n",
              "    (encoder): EmbedderMaxPool(\n",
              "      (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "      (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    )\n",
              "    (dense_emb): Sequential(\n",
              "      (0): Linear(in_features=8192, out_features=512, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=512, out_features=200, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (cos): CosineSimilarity()\n",
              ")"
            ]
          },
          "execution_count": 99,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "best_CILP_model = ContrastivePretraining(img_embedder, lidar_embedder).to(DEVICE)\n",
        "\n",
        "# Load pre-trained cilp model\n",
        "cilp_path = CHECKPOINTS / \"cilp_model.pt\"\n",
        "best_CILP_model = load_model(best_CILP_model, cilp_path, DEVICE)\n",
        "\n",
        "for p in best_CILP_model.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "best_CILP_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig = plot_similarity_matrix(\n",
        "    model=best_CILP_model,\n",
        "    dataloader=val_dataloader,\n",
        "    device=DEVICE,\n",
        "    normalize=\"softmax\",\n",
        "    temperature=1.0,\n",
        ")\n",
        "wandb.log({\"cilp/similarity_matrix\": wandb.Image(fig)})\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Shows k random examples; choose between mode: \"random\" | \"mismatches\" | \"correct\" |\n",
        "fig, table = plot_retrieval_examples(best_CILP_model, val_dataloader, DEVICE, k=5, mode=\"mismatches\")\n",
        "wandb.log({\"cilp/sample_retrievals\": table})\n",
        "plt.show()\n",
        "\n",
        "# End wandb run before starting the next model\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0PAZObVnI4v"
      },
      "source": [
        "## Stage 2: Cross-Modal Projection\n",
        "\n",
        "Here we train a projector that maps CILP RGB embeddings into the LiDAR-CNN feature space. Using a frozen CILP model and a pretrained LiDAR classifier, we minimize an MSE loss between predicted LiDAR embeddings and true LiDAR embeddings. This stage enables the system to “pretend” RGB images look like LiDAR feature vectors, allowing downstream LiDAR-based classification from RGB alone.\n",
        "\n",
        "**Projector Architecture:**\n",
        "The projector is implemented as a three-layer multilayer perceptron (MLP) that maps RGB embeddings into the LiDAR embedding space. It consists of:\n",
        "* A linear layer mapping 200 → 1000, followed by ReLU\n",
        "* A linear layer mapping 1000 → 500, followed by ReLU\n",
        "* A final linear layer mapping 500 → 3200\n",
        "* The projector is trained while all encoders are kept frozen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_lidar_cnn = Classifier(in_ch=1).to(DEVICE)\n",
        "\n",
        "# Load pre-trained lidar_cnn classifier\n",
        "lidar_cnn_path = CHECKPOINTS / \"lidar_cnn.pt\"\n",
        "best_lidar_cnn = load_model(best_lidar_cnn, lidar_cnn_path)\n",
        "\n",
        "for p in best_lidar_cnn.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "best_lidar_cnn.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfGuIUEpWYF4"
      },
      "outputs": [],
      "source": [
        "def projector_batch_loss_fn(model, batch, device, CILP_model, lidar_cnn):\n",
        "    \"\"\"\n",
        "    Compute MSE loss between projected RGB embeddings and frozen LiDAR embeddings.\n",
        "    \"\"\"\n",
        "    rgb_img, lidar_depth, _ = batch\n",
        "    rgb_img = rgb_img.to(device)\n",
        "    lidar_depth = lidar_depth.to(device)\n",
        "\n",
        "    # Use frozen encoders\n",
        "    CILP_model.eval()\n",
        "    lidar_cnn.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_embs = CILP_model.img_embedder(rgb_img)      # (B, CILP_EMB_SIZE)\n",
        "        lidar_embs = lidar_cnn.get_embs(lidar_depth)     # (B, lidar_dim)\n",
        "\n",
        "    pred_lidar_embs = model(img_embs)\n",
        "\n",
        "    loss = Func.mse_loss(pred_lidar_embs, lidar_embs)\n",
        "\n",
        "    # match the convention used in train_with_batch_loss\n",
        "    return loss, {\"loss\": loss.item()}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mkcLx0ikHvok"
      },
      "outputs": [],
      "source": [
        "set_seeds(SEED)\n",
        "\n",
        "# Constants for projector\n",
        "img_dim = CILP_EMB_SIZE\n",
        "lidar_dim = 200 * 4 * 4\n",
        "EPOCHS_PROJ = 40\n",
        "LR_PROJECTOR = 1e-4\n",
        "\n",
        "projector = Projector(img_dim, lidar_dim).to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YdthbedXSMxz",
        "outputId": "52adc6cf-4602-46c4-a9de-f8a1375a1d16"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/wandb/run-20251207_180628-kr4d48gi</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/kr4d48gi' target=\"_blank\">unknown_run</a></strong> to <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/kr4d48gi' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/kr4d48gi</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[BatchLoss] Epoch 1/40\n",
            "Found and saved better weights for the model\n",
            "epoch 0 train loss: 1.6388600946169847\n",
            "epoch 0 valid loss: 1.2890848696231842\n",
            "[BatchLoss] Epoch 2/40\n",
            "Found and saved better weights for the model\n",
            "epoch 1 train loss: 1.6224197928319897\n",
            "epoch 1 valid loss: 1.2846409142017365\n",
            "[BatchLoss] Epoch 3/40\n",
            "Found and saved better weights for the model\n",
            "epoch 2 train loss: 1.6049904155800914\n",
            "epoch 2 valid loss: 1.2684852600097656\n",
            "[BatchLoss] Epoch 4/40\n",
            "Found and saved better weights for the model\n",
            "epoch 3 train loss: 1.5911426063169514\n",
            "epoch 3 valid loss: 1.2645985782146454\n",
            "[BatchLoss] Epoch 5/40\n",
            "Found and saved better weights for the model\n",
            "epoch 4 train loss: 1.5782182199216028\n",
            "epoch 4 valid loss: 1.2419158279895783\n",
            "[BatchLoss] Epoch 6/40\n",
            "Found and saved better weights for the model\n",
            "epoch 5 train loss: 1.563390554740415\n",
            "epoch 5 valid loss: 1.2293817102909088\n",
            "[BatchLoss] Epoch 7/40\n",
            "Found and saved better weights for the model\n",
            "epoch 6 train loss: 1.5521342682908152\n",
            "epoch 6 valid loss: 1.2224450349807738\n",
            "[BatchLoss] Epoch 8/40\n",
            "Found and saved better weights for the model\n",
            "epoch 7 train loss: 1.539729038177178\n",
            "epoch 7 valid loss: 1.2170666396617889\n",
            "[BatchLoss] Epoch 9/40\n",
            "Found and saved better weights for the model\n",
            "epoch 8 train loss: 1.525018750924116\n",
            "epoch 8 valid loss: 1.2014193058013916\n",
            "[BatchLoss] Epoch 10/40\n",
            "Found and saved better weights for the model\n",
            "epoch 9 train loss: 1.5151330518443682\n",
            "epoch 9 valid loss: 1.1985203564167022\n",
            "[BatchLoss] Epoch 11/40\n",
            "Found and saved better weights for the model\n",
            "epoch 10 train loss: 1.501638357576571\n",
            "epoch 10 valid loss: 1.182257729768753\n",
            "[BatchLoss] Epoch 12/40\n",
            "Found and saved better weights for the model\n",
            "epoch 11 train loss: 1.4897928248371994\n",
            "epoch 11 valid loss: 1.1784503281116485\n",
            "[BatchLoss] Epoch 13/40\n",
            "Found and saved better weights for the model\n",
            "epoch 12 train loss: 1.4764009213935563\n",
            "epoch 12 valid loss: 1.173989474773407\n",
            "[BatchLoss] Epoch 14/40\n",
            "Found and saved better weights for the model\n",
            "epoch 13 train loss: 1.467362789556994\n",
            "epoch 13 valid loss: 1.151483517885208\n",
            "[BatchLoss] Epoch 15/40\n",
            "Found and saved better weights for the model\n",
            "epoch 14 train loss: 1.4539783185685586\n",
            "epoch 14 valid loss: 1.146673208475113\n",
            "[BatchLoss] Epoch 16/40\n",
            "Found and saved better weights for the model\n",
            "epoch 15 train loss: 1.444453133999953\n",
            "epoch 15 valid loss: 1.1401613771915435\n",
            "[BatchLoss] Epoch 17/40\n",
            "Found and saved better weights for the model\n",
            "epoch 16 train loss: 1.4296012243680787\n",
            "epoch 16 valid loss: 1.1338951587677002\n",
            "[BatchLoss] Epoch 18/40\n",
            "Found and saved better weights for the model\n",
            "epoch 17 train loss: 1.4175923241857897\n",
            "epoch 17 valid loss: 1.1175341844558715\n",
            "[BatchLoss] Epoch 19/40\n",
            "epoch 18 train loss: 1.4066905907371587\n",
            "epoch 18 valid loss: 1.12904834151268\n",
            "[BatchLoss] Epoch 20/40\n",
            "Found and saved better weights for the model\n",
            "epoch 19 train loss: 1.3967210932781822\n",
            "epoch 19 valid loss: 1.1075504064559936\n",
            "[BatchLoss] Epoch 21/40\n",
            "epoch 20 train loss: 1.384663644938441\n",
            "epoch 20 valid loss: 1.1255026698112487\n",
            "[BatchLoss] Epoch 22/40\n",
            "Found and saved better weights for the model\n",
            "epoch 21 train loss: 1.3727073887286827\n",
            "epoch 21 valid loss: 1.090850555896759\n",
            "[BatchLoss] Epoch 23/40\n",
            "Found and saved better weights for the model\n",
            "epoch 22 train loss: 1.3622930069067325\n",
            "epoch 22 valid loss: 1.081387060880661\n",
            "[BatchLoss] Epoch 24/40\n",
            "Found and saved better weights for the model\n",
            "epoch 23 train loss: 1.3520212027935954\n",
            "epoch 23 valid loss: 1.0732344746589662\n",
            "[BatchLoss] Epoch 25/40\n",
            "Found and saved better weights for the model\n",
            "epoch 24 train loss: 1.340673549830565\n",
            "epoch 24 valid loss: 1.061092072725296\n",
            "[BatchLoss] Epoch 26/40\n",
            "Found and saved better weights for the model\n",
            "epoch 25 train loss: 1.3267701196740245\n",
            "epoch 25 valid loss: 1.0555457651615143\n",
            "[BatchLoss] Epoch 27/40\n",
            "Found and saved better weights for the model\n",
            "epoch 26 train loss: 1.31973730254243\n",
            "epoch 26 valid loss: 1.0519681870937347\n",
            "[BatchLoss] Epoch 28/40\n",
            "Found and saved better weights for the model\n",
            "epoch 27 train loss: 1.3076756420539835\n",
            "epoch 27 valid loss: 1.0502884685993195\n",
            "[BatchLoss] Epoch 29/40\n",
            "Found and saved better weights for the model\n",
            "epoch 28 train loss: 1.2984565606242733\n",
            "epoch 28 valid loss: 1.0339766442775726\n",
            "[BatchLoss] Epoch 30/40\n",
            "Found and saved better weights for the model\n",
            "epoch 29 train loss: 1.289205605785052\n",
            "epoch 29 valid loss: 1.0218417048454285\n",
            "[BatchLoss] Epoch 31/40\n",
            "epoch 30 train loss: 1.2786671824274007\n",
            "epoch 30 valid loss: 1.0230374872684478\n",
            "[BatchLoss] Epoch 32/40\n",
            "Found and saved better weights for the model\n",
            "epoch 31 train loss: 1.2667294620421894\n",
            "epoch 31 valid loss: 1.0061749517917633\n",
            "[BatchLoss] Epoch 33/40\n",
            "Found and saved better weights for the model\n",
            "epoch 32 train loss: 1.2584988492274145\n",
            "epoch 32 valid loss: 1.004060161113739\n",
            "[BatchLoss] Epoch 34/40\n",
            "Found and saved better weights for the model\n",
            "epoch 33 train loss: 1.2491759902726838\n",
            "epoch 33 valid loss: 0.9975543141365051\n",
            "[BatchLoss] Epoch 35/40\n",
            "Found and saved better weights for the model\n",
            "epoch 34 train loss: 1.238819341649089\n",
            "epoch 34 valid loss: 0.984465229511261\n",
            "[BatchLoss] Epoch 36/40\n",
            "epoch 35 train loss: 1.2307139710027573\n",
            "epoch 35 valid loss: 0.9847848176956177\n",
            "[BatchLoss] Epoch 37/40\n",
            "Found and saved better weights for the model\n",
            "epoch 36 train loss: 1.2214579725126078\n",
            "epoch 36 valid loss: 0.9762007892131805\n",
            "[BatchLoss] Epoch 38/40\n",
            "epoch 37 train loss: 1.2145596606689586\n",
            "epoch 37 valid loss: 0.9842787086963654\n",
            "[BatchLoss] Epoch 39/40\n",
            "Found and saved better weights for the model\n",
            "epoch 38 train loss: 1.2068802294326804\n",
            "epoch 38 valid loss: 0.9622524380683899\n",
            "[BatchLoss] Epoch 40/40\n",
            "epoch 39 train loss: 1.1983857139160758\n",
            "epoch 39 valid loss: 0.9646808803081512\n",
            "[5.2] Best projector validation MSE: 0.9623\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>epoch_time</td><td>▄▂▁▇▂▁▂▆▁▂▅▄▂▁▇▂▂▄▅▁▁▆▂▂▂█▄▄▇▅▁▄▅▂▁▆▂▁▁▇</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>valid_loss</td><td>███▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>epoch_time</td><td>4.89051</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>327.29541</td></tr><tr><td>model</td><td>Projector</td></tr><tr><td>projector_best_val_mse</td><td>0.96225</td></tr><tr><td>train_loss</td><td>1.19839</td></tr><tr><td>valid_loss</td><td>0.96468</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">unknown_run</strong> at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/kr4d48gi' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/kr4d48gi</a><br> View project at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20251207_180628-kr4d48gi/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Train Projector\n",
        "model_save_path= CHECKPOINTS / \"projector.pt\"\n",
        "\n",
        "if not model_save_path.exists():\n",
        "\n",
        "    best_val_proj = float(\"inf\")\n",
        "\n",
        "    # Number of trainable parameters\n",
        "    num_params = sum(p.numel() for p in projector.parameters() if p.requires_grad)\n",
        "\n",
        "    opt = torch.optim.Adam(projector.parameters(), LR_PROJECTOR)\n",
        "\n",
        "    embedding_size = compute_embedding_size(\"projector\", CILP_EMB_SIZE, spatial=(4, 4))\n",
        "\n",
        "    init_wandb(\n",
        "        model=projector,\n",
        "        embedding_size=embedding_size,\n",
        "        opt_name=opt.__class__.__name__,\n",
        "        name=\"Projector | 2nd stage\",\n",
        "        num_params=num_params\n",
        "    )\n",
        "\n",
        "    results = train_with_batch_loss(\n",
        "        model=projector,\n",
        "        optimizer=opt,\n",
        "        train_dataloader=train_dataloader,\n",
        "        val_dataloader=val_dataloader,\n",
        "        batch_loss_fn=projector_batch_loss_fn,\n",
        "        epochs=EPOCHS_PROJ,\n",
        "        model_save_path=model_save_path,\n",
        "        log_to_wandb=True,\n",
        "        device=DEVICE,\n",
        "        extra_args={\n",
        "            \"CILP_model\": CILP_model,\n",
        "            \"lidar_cnn\": best_lidar_cnn,\n",
        "        }\n",
        "    )\n",
        "\n",
        "    wandb.summary[\"Projector: best_val_loss\"] = results[\"best_val_loss\"]\n",
        "\n",
        "    best_proj_val = results[\"best_val_loss\"]\n",
        "    print(\"\\n\" + \"-\"*50)\n",
        "    print(f\"BEST PROJECTOR VALIDATION MSE → {best_proj_val:.4f}\")\n",
        "    print(\"-\"*50 + \"\\n\")\n",
        "\n",
        "    # End wandb run before starting the next model\n",
        "    wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if results:\n",
        "    losses = {\n",
        "        \"projector\": {\n",
        "            \"train_losses\": results[\"train_loss\"],\n",
        "            \"valid_losses\": results[\"val_loss\"],\n",
        "        }\n",
        "    }\n",
        "\n",
        "    plot_losses(losses, title=\"Projector: train vs val loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_projector = Projector(img_dim, lidar_dim).to(DEVICE)\n",
        "\n",
        "# Load pre-trained cilp model\n",
        "projector_path = CHECKPOINTS / \"projector.pt\"\n",
        "best_projector = load_model(best_projector, projector_path, DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dp4pFlFynU9S"
      },
      "source": [
        "## Stage 3: RGB2LiDARClassifier\n",
        "\n",
        "In the final stage, we build and train a model that chains together the CILP backbone, the trained projector, and the LiDAR classifier. Only the projector is trainable; all other components remain frozen. The model learns to classify cubes and spheres directly from RGB input by leveraging the LiDAR feature space. The section logs training and validation metrics and reports the final accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "srtFQnE9yDVq",
        "outputId": "b7cb9bbc-43bc-4a83-cd82-283ff86a4b5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ANY trainable in projector? True\n",
            "\n",
            "Epoch 1/5\n",
            "DEBUG logits.requires_grad: False\n",
            "DEBUG loss.requires_grad: False\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-378877429.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m results = train_rgb2lidar_classifier(\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrgb2lidar_clf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3217486627.py\u001b[0m in \u001b[0;36mtrain_rgb2lidar_classifier\u001b[0;34m(model, train_loader, val_loader, epochs, lr, device)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DEBUG loss.requires_grad:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
          ]
        }
      ],
      "source": [
        "set_seeds(SEED)\n",
        "\n",
        "model_save_path = CHECKPOINTS / \"rgb2lidar.pt\"\n",
        "\n",
        "# Constants for classifier \n",
        "LR_RGB2LIDAR = 1e-3\n",
        "EPOCHS_RGB2LIDAR = 20\n",
        "\n",
        "# Initialize Classifier\n",
        "rgb2lidar_clf = RGB2LiDARClassifier(\n",
        "    CILP=best_CILP_model,\n",
        "    projector=best_projector,\n",
        "    lidar_cnn=best_lidar_cnn,\n",
        ").to(DEVICE)\n",
        "\n",
        "# 1) Freeze img_embedder & lidar_cnn\n",
        "for p in rgb2lidar_clf.img_embedder.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "for p in rgb2lidar_clf.shape_classifier.parameters():\n",
        "    p.requires_grad = False\n",
        "\n",
        "# 2) Ensure projector is trainable\n",
        "for p in rgb2lidar_clf.projector.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Number of trainable parameters\n",
        "num_params = sum(p.numel() for p in rgb2lidar_clf.projector.parameters() if p.requires_grad)\n",
        "\n",
        "# Train Classifier\n",
        "opt = torch.optim.Adam(rgb2lidar_clf.projector.parameters(), lr=LR_RGB2LIDAR)\n",
        "\n",
        "embedding_size = compute_embedding_size(\"classifier\", CILP_EMB_SIZE, spatial=(4, 4))\n",
        "\n",
        "\n",
        "init_wandb(\n",
        "    model=rgb2lidar_clf,\n",
        "    embedding_size=embedding_size,\n",
        "    opt_name=opt.__class__.__name__,\n",
        "    name=\"RGB2LIDAR Classifier\",\n",
        "    num_params=num_params\n",
        ")\n",
        "\n",
        "results = train_classifier_with_acc(\n",
        "    model=rgb2lidar_clf,\n",
        "    optimizer=opt,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    epochs=EPOCHS_RGB2LIDAR,\n",
        "    model_save_path=model_save_path,\n",
        "    log_to_wandb=True,\n",
        "    device=DEVICE\n",
        ")\n",
        "\n",
        "wandb.summary[\"RGB2LIDAR: best_val_loss\"] = results[\"best_val_loss\"]\n",
        "wandb.summary[\"RGB2LIDAR: best_val_acc\"]  = results[\"best_val_acc\"]\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BEST VALIDATION RESULTS\")\n",
        "print(f\"Loss     : {results['best_val_loss']:.4f}\")\n",
        "print(f\"Accuracy : {results['best_val_acc']*100:.2f}%\")\n",
        "print(\"=\"*60 + \"\\n\")\n",
        "\n",
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
