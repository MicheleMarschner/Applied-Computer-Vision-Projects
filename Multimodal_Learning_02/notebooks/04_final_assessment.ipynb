{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyMO1a+murA9fuqogb1sey34"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Model Architecture:\n","\n","**Stage 1:** Contrastive Pretraining: CILP_model\n","\n","**Goal:** align RGB and LiDAR in a shared 200-D space --> encodes both rgb and lidar in the same dimensionality space\n","```\n","RGB ----> Img Encoder ----\\\n","                            ----> CLIP-style similarity\n","LiDAR -> Lidar Encoder ----/\n","```\n","**Outcome:** Shared embedding space where matching RGB/LiDAR pairs have high similarity and non-matches low similarity.\n","\n","----------------------------\n","\n","**Stage 2:** Projector Training: projector\n","\n","**Goal:** learn a mapping from RGB CILP embeddings to LiDAR embeddings used by lidar_cnn:\n","ℝ²⁰⁰ (CILP RGB embedding) → ℝ³²⁰⁰ (LiDAR-CNN embedding)\n","\n","projector knows how to “pretend” RGBs are LiDAR internally: projected_RGB_embedding ≈ “real” LiDAR embedding for each paired RGB/LiDAR sample.\n","```\n","RGB ----> Img Encoder ----> Projector ----> LiDAR embedding\n","                                     |\n","                                     v\n","                             MSE-loss to true LiDAR embedding\n","\n","```\n","----------------------------\n","\n","**Stage 3:** Final Classifier: RGB2LiDARClassifier\n","\n","**Goal:** chaining all models together to classify spheres and cubes from images\n","\n","pretends the RGBs look like LiDAR in the internal feature space and then uses LiDAR classifier.\n","```\n","RGB (img) ----> (CILP Img Encoder) ----> 200-D CILP embedding ----> (Projector) ---> 3200-D LiDAR embedding\n","---> (LiDAR Classifier) ---> cube/sphere\n","\n","```"],"metadata":{"id":"OwNXAWXcyOWQ"}},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"oYZmjLlguKJw"}},{"cell_type":"code","source":["%%capture\n","%pip install fiftyone==1.10.0 sympy==1.12 torch==2.9.0 torchvision==0.20.0 numpy open-clip-torch"],"metadata":{"id":"QgqWE6wM_Y_N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# %pip install --pre torchvision --extra-index-url https://download.pytorch.org/whl/nightly/cu118\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D3vXsW-hA_qm","executionInfo":{"status":"ok","timestamp":1764359542661,"user_tz":-60,"elapsed":4342,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"d93b10bb-c26f-4186-8333-c92949725709"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/nightly/cu118\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n","Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu126)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.14.0)\n","Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n","Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n","Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.11.1.6)\n","Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.9.0->torchvision) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n"]}]},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"VgrrXotQ0t0k"}},{"cell_type":"code","source":["import os\n","import random\n","import time\n","from pathlib import Path\n","from google.colab import userdata\n","\n","import numpy as np\n","from PIL import Image\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, Subset\n","import torchvision.transforms as transforms\n","#import torchvision.transforms.v2 as transforms   ##TODO Note:\n","\n","import wandb\n","import cv2\n","import albumentations as A"],"metadata":{"id":"lUod4cKbh8ke"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","STORAGE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4h2BgKorid2q","executionInfo":{"status":"ok","timestamp":1764360289848,"user_tz":-60,"elapsed":78106,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"f546a222-33b2-4fd2-e84f-78b637f78bda"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Constants"],"metadata":{"id":"3l3hqzHd0rux"}},{"cell_type":"code","source":["SEED = 51\n","NUM_WORKERS = os.cpu_count() if os.cpu_count() is not None else 2  # Number of CPU cores\n","\n","DATASET_PATH = STORAGE_PATH / \"multimodal_training_workshop/data/assessment\"\n","\n","BATCH_SIZE = 32\n","IMG_SIZE = 64\n","\n","CLASSES = [\"cubes\", \"spheres\"]\n","LABEL_MAP = {\"cubes\": 0, \"spheres\": 1}"],"metadata":{"id":"ETqCFts35qPy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utility Functions"],"metadata":{"id":"Qa00VhIvPA8I"}},{"cell_type":"markdown","source":["## Reproducibility"],"metadata":{"id":"gE633WJJPGxy"}},{"cell_type":"code","source":["def set_seeds(seed=SEED):\n","    \"\"\"\n","    Set seeds for complete reproducibility across all libraries and operations.\n","\n","    Args:\n","        seed (int): Random seed value\n","    \"\"\"\n","    # Set environment variables before other imports\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n","\n","    # Python random module\n","    random.seed(seed)\n","\n","    # NumPy\n","    np.random.seed(seed)\n","\n","    # PyTorch CPU\n","    torch.manual_seed(seed)\n","\n","    # PyTorch GPU (all devices)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n","\n","        # CUDA deterministic operations\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","    # OpenCV\n","    cv2.setRNGSeed(seed)\n","\n","    # Albumentations (for data augmentation)\n","    try:\n","        A.seed_everything(seed)\n","    except AttributeError:\n","        # Older versions of albumentations\n","        pass\n","\n","    # PyTorch deterministic algorithms (may impact performance)\n","    try:\n","        torch.use_deterministic_algorithms(True)\n","    except RuntimeError:\n","        # Some operations don't have deterministic implementations\n","        print(\"Warning: Some operations may not be deterministic\")\n","\n","    print(f\"All random seeds set to {seed} for reproducibility\")\n","\n","\n","\n","# Usage: Call this function at the beginning and before each training phase\n","set_seeds(SEED)\n","\n","# Additional reproducibility considerations:\n","\n","def create_deterministic_training_dataloader(dataset, batch_size, shuffle=True, **kwargs):\n","    \"\"\"\n","    Create a DataLoader with deterministic behavior.\n","\n","    Args:\n","        dataset: PyTorch Dataset instance\n","        batch_size: Batch size\n","        shuffle: Whether to shuffle data\n","        **kwargs: Additional DataLoader arguments\n","\n","    Returns:\n","        Training DataLoader with reproducible behavior\n","    \"\"\"\n","    # Use a generator with fixed seed for reproducible shuffling\n","    generator = torch.Generator()\n","    generator.manual_seed(51)\n","\n","    return torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        generator=generator if shuffle else None,\n","        **kwargs\n","    )\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"F9AGyB8DPIic","executionInfo":{"status":"ok","timestamp":1764360289905,"user_tz":-60,"elapsed":54,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"922deb52-a755-44eb-9e9e-986a98eba537"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["All random seeds set to 51 for reproducibility\n"]}]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.cuda.is_available()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nmi7z0qDPAqc","executionInfo":{"status":"ok","timestamp":1764360289926,"user_tz":-60,"elapsed":20,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"bdf8e038-4895-4f6d-8641-525672045156"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["img_transforms = transforms.Compose([\n","    #transforms.ToImage(),\n","    transforms.ToTensor(),\n","    transforms.Resize(IMG_SIZE),\n","    #transforms.ToDType(torch.float32, scale=True),\n","    ## transforms.Normalize((mean_intensity,), (std_intensity,))\n","])"],"metadata":{"id":"rRuSY96XPeTP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def input_fn(batch):\n","    \"\"\"\n","    Adapts a batch from the dataloader to the model inputs.\n","\n","    batch = (rgb, lidar, label), but CILP, projector only need (rgb, lidar).\n","    \"\"\"\n","    rgb, lidar, _ = batch\n","    return (rgb.to(device), lidar.to(device))"],"metadata":{"id":"LZ32UyK_cn4M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def rgb_only_input_fn(batch):\n","    rgb_imgs = batch[0].to(device)     # (B, 4, 64, 64)\n","    return (rgb_imgs,)                 # tuple, because model(*input_fn(batch))"],"metadata":{"id":"xWSfQtN5xymI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TODO: Integrate Wandb as in task 3\n","\n","W&B logging, metrics, plots → reuse almost as-is\n","\n","\n","\n","```\n","W&B init, wandb.log(...)\n","\n","a metrics dict holding losses, epoch times, params, GPU memory\n","\n","a plot_losses(losses) helper that plots train/valid curves\n","\n","maybe a comparison DataFrame builder\n","```\n","\n"],"metadata":{"id":"bZWiqY0O32Wy"}},{"cell_type":"code","source":["# Load W&B API key from .env file and make it available as env variable\n","# from dotenv import load_dotenv\n","# load_dotenv()  # loads .env automatically\n","\n","# os.environ[\"WANDB_API_KEY\"]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"f6ECQZliDmW2","executionInfo":{"status":"error","timestamp":1764360290089,"user_tz":-60,"elapsed":133,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"9a6873bf-ec5a-4685-9c78-b7122df041b1"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"'WANDB_API_KEY'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3036353213.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mload_dotenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# loads .env automatically\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"WANDB_API_KEY\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/lib/python3.12/os.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'WANDB_API_KEY'"]}]},{"cell_type":"code","source":["# Load W&B API key from Colab Secrets and make it available as env variable\n","wandb_key = userdata.get('WANDB_API_KEY')\n","os.environ[\"WANDB_API_KEY\"] = wandb_key"],"metadata":{"id":"imATXiFeDhOL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-DEjk6IDoVc","executionInfo":{"status":"ok","timestamp":1764360299989,"user_tz":-60,"elapsed":2312,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"3cdda376-c94d-42fe-efdd-fa648531517c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n","  | |_| | '_ \\/ _` / _` |  _/ -_)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichele-marschner\u001b[0m (\u001b[33mmichele-marschner-university-of-potsdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["def init_wandb(model, fusion_name, num_params, opt_name, batch_size=BATCH_SIZE, epochs=15):\n","  config = {\n","    # \"embedding_size\": embedding_size,      ## TODO: ändert die sich? hab ich die bei fusion?\n","    \"optimizer_type\": opt_name,\n","    \"fusion_strategy\": fusion_name,\n","    \"model_architecture\": model.__class__.__name__,\n","    \"batch_size\": batch_size,\n","    \"num_epochs\": epochs,\n","    \"num_parameters\": num_params\n","  }\n","\n","  run = wandb.init(\n","    project=\"cilp-extended-assessment\",\n","    name=f\"{fusion_name}_run\",\n","    config=config,\n","    reinit=True,                          # allows multiple runs in one script\n","  )\n","\n","  return"],"metadata":{"id":"9C7tqqRSEDFP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# The Model\n","\n","\n","\n","```\n","ConvEncoder\n","```\n","\n","\n","```\n","CILP_EMB_SIZE = 200\n","\n","img_embedder   = ConvEncoder(in_ch=4, emb_dim=CILP_EMB_SIZE).to(device)\n","lidar_embedder = ConvEncoder(in_ch=1, emb_dim=CILP_EMB_SIZE).to(device)\n","```\n","--> definiere vorab CILP_EMB_SIZE=200 (im assessment)\n","\n","\n","\n","\n","\n","\n","\n","\n","**TODO:** Does it make sense to save the best model figured out in task 3 and 4 and re-train it here? (use it as \"foundation\")\n","\n","Generell ist die Frage - da sich hier alles nur um diesen einen Datensatz dreht - klar um fusion performance - aber eben nicht um position - dürfte es ja sogar schon das Endmodel sein, dass ich in task 3 und task 4 identifiziere - auch der head sollte ja dann schon passen"],"metadata":{"id":"GP4MXN68uNRq"}},{"cell_type":"code","source":["## move to models.py\n","class ConvEncoder(nn.Module):\n","    \"\"\"\n","    Small CNN that turns a 4-channel image into a compact embedding vector.\n","\n","    Why:\n","    - We want to compare RGB+LiDAR representations using similarity.\n","    - For that, we need each modality mapped to a fixed-size vector (embedding).\n","    - This encoder is the \"feature extractor\" for one modality.\n","    \"\"\"\n","\n","    def __init__(self, in_ch: int, emb_dim: int = 128):\n","        \"\"\"\n","        Args:\n","            in_ch: number of input channels (4 for RGB, 4 for LiDAR)\n","            emb_dim: dimensionality of the output embedding (e.g. 128)\n","        \"\"\"\n","        super().__init__()\n","        k = 3      # kernel size\n","\n","        # Three conv layers with pooling\n","        self.conv1 = nn.Conv2d(in_ch, 50, k, padding=1)\n","        self.conv2 = nn.Conv2d(50, 100, k, padding=1)\n","        self.conv3 = nn.Conv2d(100, 200, k, padding=1)\n","        self.conv4 = nn.Conv2d(200, 200, k, padding=1)    ## TODO: nur bei dem CILP Model\n","        self.pool = nn.MaxPool2d(2)\n","\n","        # After 3x pooling (factor 2 each time) on 64x64 → 8x8 feature maps\n","        # channels: 200, so flattened size = 200 * 8 * 8\n","        self.fc1 = nn.Linear(200 * 8 * 8, 1000)         ## TODO: CILP macht 4*4\n","        self.fc2 = nn.Linear(1000, emb_dim)\n","\n","        self.output_dim = emb_dim\n","\n","    def forward(self, x):\n","        # Convolution + nonlinearity + downsampling\n","        x = self.pool(F.relu(self.conv1(x)))   # (B,50,32,32)\n","        x = self.pool(F.relu(self.conv2(x)))   # (B,100,16,16)\n","        x = self.pool(F.relu(self.conv3(x)))   # (B,200,8,8)\n","\n","        # Flatten spatial dimensions\n","        x = torch.flatten(x, 1)                # (B,200*8*8)\n","\n","        # Two fully connected layers to get to embedding dimension\n","        x = F.relu(self.fc1(x))                # (B,1000)\n","        emb = self.fc2(x)                        # (B,emb_dim)\n","\n","        return F.normalize(emb)                 ##  TODO: nur bei CILP? wegen Cosine similarity - schädlich bei anderen?\n"],"metadata":{"id":"LKSBizqqNk48"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["CILP_EMB_SIZE = 200\n","\n","img_embedder   = ConvEncoder(in_ch=4, emb_dim=CILP_EMB_SIZE).to(device)\n","lidar_embedder = ConvEncoder(in_ch=1, emb_dim=CILP_EMB_SIZE).to(device)"],"metadata":{"id":"l0m1rjwB5wLV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stage 1: CILP contrastive pretraining"],"metadata":{"id":"Uclun1v23rwP"}},{"cell_type":"code","source":["## move to models.py\n","class ContrastivePretraining(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.img_embedder = img_embedder\n","        self.lidar_embedder = lidar_embedder\n","        self.cos = nn.CosineSimilarity(dim=-1)\n","\n","    def forward(self, rgb_imgs, lidar_depths):\n","        img_emb = self.img_embedder(rgb_imgs)\n","        lidar_emb = self.lidar_embedder(lidar_depths)\n","\n","        repeated_img_emb = img_emb.repeat_interleave(len(img_emb), dim=0)\n","        repeated_lidar_emb = lidar_emb.repeat(len(lidar_emb), 1)\n","\n","        similarity = self.cos(repeated_img_emb, repeated_lidar_emb)\n","        similarity = torch.unflatten(similarity, 0, (len(img_emb), len(img_emb)))\n","        similarity = (similarity + 1) / 2\n","\n","        logits_per_img = similarity\n","        logits_per_lidar = similarity.T\n","        return logits_per_img, logits_per_lidar"],"metadata":{"id":"pUmjWaHl4y6V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stage 2: Projector training"],"metadata":{"id":"NN8eON2P3qRP"}},{"cell_type":"markdown","source":["# The Dataset\n","\n","**TODO:** I would use the code from the task 3\n","\n","```\n","AssessmentDataset (Pattern A or your MyDataset variant)\n","\n","create_assessment_splits(...)\n","\n","make_dataloaders(...)\n","```\n","\n"],"metadata":{"id":"Fl8rikTIvGxH"}},{"cell_type":"code","source":["## move to datasets.py\n","class AssessmentDataset(Dataset):\n","    \"\"\"\n","    Loads paired RGB/LiDAR samples from the assessment dataset.\n","\n","    Used to create train/val/test splits by constructing multiple instances\n","    with different index ranges.\n","\n","    One sample: (rgb_tensor, lidar_tensor, label_tensor)\n","    - rgb: 4-channel image (B,4,H,W) after transform\n","    - lidar: 4-channel tensor (B,4,H,W) after transform\n","    \"\"\"\n","\n","    def __init__(self, root_dir=DATASET_PATH, start_idx=0, end_idx=None,\n","                 transform_rgb=None, transform_lidar=None, shuffle=True):\n","        \"\"\"\n","        Args:\n","            start_idx: first sample index to include\n","            end_idx: one past last index to include (None = all)\n","            transform_rgb: optional transform for RGB image (PIL -> tensor)\n","            transform_lidar: optional transform for LiDAR tensor\n","            shuffle: if True, shuffle full sample list once before slicing\n","        \"\"\"\n","        self.classes = CLASSES\n","        self.root_dir = root_dir\n","        self.transform_rgb = transform_rgb\n","        self.transform_lidar = transform_lidar\n","        self.samples = []\n","\n","        # 1) Build full list of samples\n","        all_samples = []\n","        # Paths to different modalities, organized by class\n","        for class_name in self.classes:\n","            class_dir = self.root_dir / class_name\n","            RGB_DIR = class_dir / \"rgb\"\n","            LIDAR_DIR = class_dir / \"lidar\"\n","\n","            rgb_files = sorted(RGB_DIR.glob(\"*.png\"))\n","            npy_files = sorted(LIDAR_DIR.glob(\"*.npy\"))\n","\n","            # Verify matching files\n","            rgb_stems = {f.stem for f in rgb_files}\n","            npy_stems = {f.stem for f in npy_files}\n","            matching = rgb_stems & npy_stems\n","\n","            print(f\"{class_name}: {len(matching)} paired samples\")\n","\n","            label = LABEL_MAP[class_name]\n","\n","            for stem in sorted(matching):\n","                all_samples.append(\n","                    {\n","                        \"rgb\": RGB_DIR / f\"{stem}.png\",\n","                        \"lidar\": LIDAR_DIR / f\"{stem}.npy\",\n","                        \"label\": label,\n","                    }\n","                )\n","\n","        # Optional: shuffle once, so contiguous splits are roughly balanced\n","        if shuffle:\n","            rng = random.Random(SEED)\n","            rng.shuffle(all_samples)\n","\n","        # 2) Apply slice [start_idx:end_idx]\n","        if end_idx is None:\n","            end_idx = len(all_samples)\n","\n","        self.samples = all_samples[start_idx:end_idx]\n","\n","        print(\n","            f\"AssessmentDataset: \"\n","            f\"slice [{start_idx}:{end_idx}] -> {len(self.samples)} samples\"\n","        )\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        sample = self.samples[idx]\n","\n","        # RGB\n","        rgb = Image.open(sample[\"rgb\"])\n","        if self.transform_rgb is not None:\n","            rgb = self.transform_rgb(rgb)\n","\n","        # LiDAR\n","        lidar_np = np.load(sample[\"lidar\"])        # (64, 64)\n","        if lidar_np.ndim == 2:\n","            lidar_np = lidar_np[None, :, :]        # (1, 64, 64)\n","\n","        lidar = torch.tensor(lidar_np).float()\n","\n","        if self.transform_lidar is not None:\n","            lidar = self.transform_lidar(lidar)\n","\n","        label = torch.tensor(sample[\"label\"], dtype=torch.long)\n","\n","        return rgb, lidar, label\n"],"metadata":{"id":"eibC6445O5JD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## move to training.py or datasets.py?\n","def create_data_splits(\n","    root_dir,\n","    train_frac=0.7,\n","    val_frac=0.15,\n","    test_frac=0.15,\n","    transform_rgb=None,\n","    transform_lidar=None,\n","    seed=SEED,\n","):\n","    \"\"\"\n","    Build a single AssessmentDataset and return train/val/test Subsets.\n","    \"\"\"\n","    total = train_frac + val_frac + test_frac\n","    assert abs(total - 1.0) < 1e-6, \"Fractions must sum to 1.0\"\n","\n","    # One unified dataset over all samples\n","    full_ds = AssessmentDataset(\n","        root_dir=root_dir,\n","        start_idx=0,\n","        end_idx=None,\n","        transform_rgb=transform_rgb,\n","        transform_lidar=transform_lidar,\n","        shuffle=False,\n","    )\n","\n","    rgb0, lidar0, label0 = full_ds[0]\n","    print(f\"One sample debug:\")\n","    print(f\"  RGB tensor shape:   {rgb0.shape}\")\n","    print(f\"  LiDAR tensor shape: {lidar0.shape}\")\n","    print(f\"  Label:              {label0.item()}\")\n","\n","    N = len(full_ds)\n","    n_train = int(N * train_frac)\n","    n_val   = int(N * val_frac)\n","    n_test  = N - n_train - n_val\n","\n","    # deterministic shuffle of indices\n","    g = torch.Generator().manual_seed(seed)\n","    perm = torch.randperm(N, generator=g)\n","\n","    train_idx = perm[:n_train]\n","    val_idx   = perm[n_train:n_train+n_val]\n","    test_idx  = perm[n_train+n_val:]\n","\n","    train_ds = Subset(full_ds, train_idx)\n","    val_ds   = Subset(full_ds, val_idx)\n","    test_ds  = Subset(full_ds, test_idx)\n","\n","    print(f\"Total: {N}, train: {len(train_ds)}, val: {len(val_ds)}, test: {len(test_ds)}\")\n","\n","    return train_ds, val_ds, test_ds\n"],"metadata":{"id":"VXN9MExkSWq9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## move to training.py\n","def make_dataloaders(\n","    train_ds,\n","    val_ds,\n","    test_ds,\n","    batch_size,\n","    num_workers=2,\n","    drop_last=True,\n","    device=\"cpu\",\n","):\n","    \"\"\"\n","    Given train/val/test datasets, return their DataLoaders.\n","    \"\"\"\n","\n","    train_dataloader = create_deterministic_training_dataloader(\n","        train_ds,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        drop_last=drop_last,        ## TODO: überprüfen mit ANotnio: in case of contrastive learning later\n","    )\n","\n","    val_dataloader = DataLoader(\n","        val_ds,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    test_dataloader = DataLoader(\n","        test_ds,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        drop_last=False,\n","    )\n","\n","    return train_dataloader, val_dataloader, test_dataloader\n"],"metadata":{"id":"6SCiEbmVSZuG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds, val_ds, test_ds = create_data_splits(\n","    root_dir=DATASET_PATH,\n","    train_frac=0.7,\n","    val_frac=0.15,\n","    test_frac=0.15,\n","    transform_rgb=img_transforms\n","  )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tNdw6u5nTEuB","executionInfo":{"status":"ok","timestamp":1764360446109,"user_tz":-60,"elapsed":142204,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"54e6e48d-9965-4e06-e1a3-74b983e0617f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cubes: 2501 paired samples\n","spheres: 9999 paired samples\n","AssessmentDataset: slice [0:12500] -> 12500 samples\n","One sample debug:\n","  RGB tensor shape:   torch.Size([4, 64, 64])\n","  LiDAR tensor shape: torch.Size([1, 64, 64])\n","  Label:              0\n","Total: 12500, train: 8750, val: 1875, test: 1875\n"]}]},{"cell_type":"code","source":["train_dataloader, val_dataloader, test_dataloader = make_dataloaders(\n","    train_ds,\n","    val_ds,\n","    test_ds,\n","    batch_size=BATCH_SIZE\n",")"],"metadata":{"id":"Ig9ItofVWHcS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training\n","\n","\n","```\n","train_model\n","```\n","\n"],"metadata":{"id":"EMTis7eq20M2"}},{"cell_type":"code","source":["def format_positions(positions):\n","    return ['{0: .3f}'.format(x) for x in positions]"],"metadata":{"id":"bCBvyrT7NI4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def print_loss(epoch, loss, outputs, target, is_train=True, is_debug=False):\n","    loss_type = \"train loss:\" if is_train else \"valid loss:\"\n","    print(\"epoch\", str(epoch), loss_type, str(loss))\n","    if is_debug:\n","        print(\"example pred:\", format_positions(outputs[0].tolist()))\n","        print(\"example real:\", format_positions(target[0].tolist()))"],"metadata":{"id":"sRFb7w5wNGiq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## move to training.py\n","def train_model(model, optimizer, input_fn, epochs, loss_fn, train_dataloader, valid_dataloader, target_idx=-1, log_to_wandb=False, model_name=None):\n","    train_losses = []\n","    valid_losses = []\n","    epoch_times = []\n","\n","    # for GPU memory tracking\n","    max_gpu_mem_mb = 0.0\n","    use_cuda = torch.cuda.is_available()\n","\n","    if use_cuda:\n","        torch.cuda.reset_peak_memory_stats()\n","\n","    for epoch in range(epochs):\n","        start_time = time.time()                  # to track the train time per model\n","        print(f\"Epoch and start time: {epoch} und {start_time}\")\n","        model.train()\n","        train_loss = 0\n","\n","        for step, batch in enumerate(train_dataloader):\n","            optimizer.zero_grad()\n","            target = batch[target_idx].to(device)  # labels: 0/1 for cube/sphere\n","            outputs = model(*input_fn(batch))      # e.g. model(rgb, lidar)\n","\n","            loss = loss_fn(outputs, target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        train_loss = train_loss / (step + 1)\n","        train_losses.append(train_loss)\n","        print_loss(epoch, train_loss, outputs, target, is_train=True)\n","\n","\n","        # ----- validation -----\n","        model.eval()\n","        valid_loss = 0\n","        with torch.no_grad():\n","          for step, batch in enumerate(valid_dataloader):\n","              target = batch[target_idx].to(device)\n","              outputs = model(*input_fn(batch))\n","              valid_loss += loss_fn(outputs, target).item()\n","        valid_loss = valid_loss / (step + 1)\n","        valid_losses.append(valid_loss)\n","        print_loss(epoch, valid_loss, outputs, target, is_train=False)\n","\n","        # timing\n","        epoch_time = time.time() - start_time\n","        epoch_times.append(epoch_time)\n","\n","        # GPU memory\n","        if use_cuda:\n","            gpu_mem_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)\n","            max_gpu_mem_mb = max(max_gpu_mem_mb, gpu_mem_mb)\n","\n","        # console logging (reuse your print_loss if you want)\n","        print(\n","            f\"[{model_name or 'model'}] Epoch {epoch+1}/{epochs} \"\n","            f\"- train_loss: {train_loss:.4f}  valid_loss: {valid_loss:.4f}  \"\n","            f\"time: {epoch_time:.2f}s\"\n","        )\n","\n","        # wandb logging\n","        if log_to_wandb:\n","            wandb.log(\n","                {\n","                    \"model\": model_name or \"model\",\n","                    \"epoch\": epoch + 1,\n","                    \"train_loss\": train_loss,\n","                    \"valid_loss\": valid_loss,\n","                    \"epoch_time_sec\": epoch_time,\n","                    \"max_gpu_mem_mb_epoch\": gpu_mem_mb if use_cuda else 0.0,\n","                }\n","            )\n","\n","    return train_losses, valid_losses, epoch_times, max_gpu_mem_mb"],"metadata":{"id":"DI2Oc8O2O0mW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stage 1: CILP contrastive pretraining"],"metadata":{"id":"bW5pD-KP3Urk"}},{"cell_type":"code","source":["# Initialize the model\n","CILP_model = ContrastivePretraining().to(device)\n","\n","loss_img = nn.CrossEntropyLoss()\n","loss_lidar = nn.CrossEntropyLoss()"],"metadata":{"id":"yTHa8rUL6BNO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cilp_loss_fn(outputs, target_ignored):\n","    \"\"\"\n","    outputs: (logits_per_img, logits_per_lidar), each of shape (B, B)\n","\n","    We build ground-truth indices 0..B-1 so that:\n","      - row i in logits_per_img should classify LiDAR i as the correct match\n","      - row i in logits_per_lidar should classify RGB i as the correct match\n","    \"\"\"\n","    logits_per_img, logits_per_lidar = outputs   # both are (B, B)\n","    actual_batch_size = logits_per_img.size(0)\n","\n","    # ground-truth: diagonal is the correct match\n","    ground_truth = torch.arange(actual_batch_size, dtype=torch.long, device=logits_per_img.device)\n","\n","    loss_i = loss_img(logits_per_img, ground_truth)        # image → lidar\n","    loss_l = loss_lidar(logits_per_lidar, ground_truth)    # lidar → image\n","\n","    total_loss = (loss_i + loss_l) * 0.5\n","\n","    return total_loss\n"],"metadata":{"id":"2s524SyHXEs2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS = 15\n","LR_CILP = 1e-3    # 3e-4\n","\n","optimizer = torch.optim.Adam(CILP_model.parameters(), lr=LR_CILP)\n","\n","cilp_train_losses, cilp_valid_losses, cilp_epoch_times, cilp_max_gpu_mb = train_model(\n","    model=CILP_model,\n","    optimizer=optimizer,\n","    input_fn=input_fn,\n","    epochs=EPOCHS,\n","    loss_fn=cilp_loss_fn,\n","    train_dataloader=train_dataloader,\n","    valid_dataloader=val_dataloader,\n","    target_idx=-1,\n","    log_to_wandb=False,\n","    model_name=\"CILP_contrastive\",\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":373},"id":"wBRjLeIzW7Rt","executionInfo":{"status":"error","timestamp":1764364987091,"user_tz":-60,"elapsed":1737,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"a622b52e-4601-4a66-962c-05c246ecbed8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch and start time: 0 und 1764364985.301687\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"element 0 of tensors does not require grad and does not have a grad_fn","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1607598741.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCILP_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR_CILP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m cilp_train_losses, cilp_valid_losses, cilp_epoch_times, cilp_max_gpu_mb = train_model(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCILP_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3315787643.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, input_fn, epochs, loss_fn, train_dataloader, valid_dataloader, target_idx, log_to_wandb, model_name)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             )\n\u001b[0;32m--> 625\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    626\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    842\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"]}]},{"cell_type":"code","source":["for param in CILP_model.parameters(): ## TODO: again done in projector??\n","    param.requires_grad = False"],"metadata":{"id":"-rSHK5JmnCS1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stage 2: Cross-Modal Projection"],"metadata":{"id":"i0PAZObVnI4v"}},{"cell_type":"code","source":["## move to models.py\n","class Projector(nn.Module):\n","    \"\"\"\n","    Maps RGB embeddings -> LiDAR embedding space.\n","    Used after CILP encoders are frozen.\n","    \"\"\"\n","    def __init__(self, img_dim, lidar_dim):\n","        super().__init__()\n","        self.net = nn.Sequential(\n","            nn.Linear(img_dim, 1000),\n","            nn.ReLU(),\n","            nn.Linear(1000, 500),\n","            nn.ReLU(),\n","            nn.Linear(500, lidar_dim),\n","        )\n","\n","    def forward(self, x):\n","        return self.net(x)"],"metadata":{"id":"pvrhn0YMcYkR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## move to models.py\n","class ProjectorTrainingWrapper(nn.Module):\n","    \"\"\"\n","    Combines:\n","    - frozen CILP img_embedder\n","    - frozen CILP lidar_embedder\n","    - trainable projector\n","\n","    And returns what the loss_fn needs.\n","    \"\"\"\n","    def __init__(self, cilp_model, projector):\n","        super().__init__()\n","        self.cilp = cilp_model\n","        self.projector = projector\n","\n","        # freeze CILP\n","        for p in self.cilp.parameters():\n","            p.requires_grad = False\n","\n","    def forward(self, rgb, lidar):\n","        # obtain embeddings\n","        rgb_emb = self.cilp.img_embedder(rgb)          # (B, D_img)\n","        lidar_emb = self.cilp.lidar_embedder(lidar)    # (B, D_lidar)\n","\n","        # predict lidar embedding from rgb embedding\n","        pred_lidar_emb = self.projector(rgb_emb)\n","\n","        # train_model expects ANY tuple → returned to loss_fn\n","        return rgb_emb, lidar_emb, pred_lidar_emb\n"],"metadata":{"id":"J-EoJBtQdAuO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_dim = CILP_model.img_embedder.output_dim   #\n","lidar_dim = CILP_model.lidar_embedder.output_dim\n","\n","projector = Projector(img_dim, lidar_dim).to(device)\n","\n","projector_wrapper = ProjectorTrainingWrapper(CILP_model, projector).to(device)"],"metadata":{"id":"mkcLx0ikHvok"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def projector_loss_fn(outputs, _target_ignored):\n","    \"\"\"\n","    outputs: tuple (rgb_embs, lidar_embs, pred_lidar_embs)\n","\n","    The projector should map: pred_lidar_embs ≈ lidar_embs\n","    \"\"\"\n","    rgb_embs, lidar_embs, pred_lidar_embs = outputs\n","    mse = nn.MSELoss()\n","    return mse(pred_lidar_embs, lidar_embs)"],"metadata":{"id":"kIxGG2-ec4Lh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["EPOCHS_PROJECTOR = 10\n","LR_PROJECTOR = 1e-4\n","\n","optimizer_proj = torch.optim.Adam(projector_wrapper.projector.parameters(), lr=LR_PROJECTOR)\n","\n","proj_train_losses, proj_valid_losses, proj_epoch_times, proj_max_gpu_mb = train_model(\n","    model=projector_wrapper,\n","    optimizer=optimizer_proj,\n","    input_fn=input_fn,\n","    epochs=EPOCHS_PROJECTOR,\n","    loss_fn=projector_loss_fn,\n","    train_dataloader=train_dataloader,\n","    valid_dataloader=val_dataloader,\n","    target_idx=-1,\n","    log_to_wandb=False,\n","    model_name=\"Projector_RGB_to_LiDAR\",\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":790},"id":"cX1MbvkmeHI7","executionInfo":{"status":"error","timestamp":1764364570637,"user_tz":-60,"elapsed":419630,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"5c618087-4d50-4e46-ca4b-28326acc8fd7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch and start time: 0 und 1764364150.9628556\n","epoch 0 train loss: 0.0019723757358245646\n","epoch 0 valid loss: 0.00017922741584091756\n","[Projector_RGB_to_LiDAR] Epoch 1/30 - train_loss: 0.0020  valid_loss: 0.0002  time: 71.83s\n","Epoch and start time: 1 und 1764364222.793578\n","epoch 1 train loss: 9.961871289894643e-05\n","epoch 1 valid loss: 9.070638213112639e-05\n","[Projector_RGB_to_LiDAR] Epoch 2/30 - train_loss: 0.0001  valid_loss: 0.0001  time: 67.72s\n","Epoch and start time: 2 und 1764364290.512395\n","epoch 2 train loss: 6.278865497305935e-05\n","epoch 2 valid loss: 7.795102307378935e-05\n","[Projector_RGB_to_LiDAR] Epoch 3/30 - train_loss: 0.0001  valid_loss: 0.0001  time: 67.38s\n","Epoch and start time: 3 und 1764364357.888088\n","epoch 3 train loss: 5.43285697377167e-05\n","epoch 3 valid loss: 7.360645455870933e-05\n","[Projector_RGB_to_LiDAR] Epoch 4/30 - train_loss: 0.0001  valid_loss: 0.0001  time: 66.75s\n","Epoch and start time: 4 und 1764364424.6400433\n","epoch 4 train loss: 5.148220647884848e-05\n","epoch 4 valid loss: 7.122890043926115e-05\n","[Projector_RGB_to_LiDAR] Epoch 5/30 - train_loss: 0.0001  valid_loss: 0.0001  time: 67.50s\n","Epoch and start time: 5 und 1764364492.135465\n","epoch 5 train loss: 4.980162908823866e-05\n","epoch 5 valid loss: 6.845024642340528e-05\n","[Projector_RGB_to_LiDAR] Epoch 6/30 - train_loss: 0.0000  valid_loss: 0.0001  time: 68.02s\n","Epoch and start time: 6 und 1764364560.155756\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1763923605.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojector_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLR_PROJECTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m proj_train_losses, proj_valid_losses, proj_epoch_times, proj_max_gpu_mb = train_model(\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprojector_wrapper\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer_proj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3315787643.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, input_fn, epochs, loss_fn, train_dataloader, valid_dataloader, target_idx, log_to_wandb, model_name)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtarget_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# labels: 0/1 for cube/sphere\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1481\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1482\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1483\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1433\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1434\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1435\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1436\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1273\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mremaining\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mEmpty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremaining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotify\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["## Stage 3: RGB2LiDARClassifier"],"metadata":{"id":"Dp4pFlFynU9S"}},{"cell_type":"code","source":["## move to models.py\n","class Classifier(nn.Module):\n","    def __init__(self, in_ch):\n","        super().__init__()\n","        kernel_size = 3\n","        n_classes = 1\n","        self.embedder = nn.Sequential(\n","            nn.Conv2d(in_ch, 50, kernel_size, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(50, 100, kernel_size, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(100, 200, kernel_size, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Conv2d(200, 200, kernel_size, padding=1),\n","            nn.ReLU(),\n","            nn.MaxPool2d(2),\n","            nn.Flatten()\n","        )\n","        self.classifier = nn.Sequential(\n","            nn.Linear(200 * 4 * 4, 100),\n","            nn.ReLU(),\n","            nn.Linear(100, n_classes)\n","        )\n","\n","    def get_embs(self, imgs):\n","        return self.embedder(imgs)\n","\n","    def forward(self, raw_data=None, data_embs=None):\n","        assert (raw_data is not None or data_embs is not None), \"No images or embeddings given.\"\n","        if raw_data is not None:\n","            data_embs = self.get_embs(raw_data)\n","        return self.classifier(data_embs)"],"metadata":{"id":"jEkmp72P2OXb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_save_path = STORAGE_PATH / \"models/lidar_cnn.pt\"\n","\n","lidar_cnn = Classifier(1).to(device)\n","lidar_cnn.load_state_dict(torch.load(model_save_path, weights_only=True))\n","\n","\n","for param in lidar_cnn.parameters():\n","    param.requires_grad = False\n","\n","lidar_cnn.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GWgPU4P7yXFq","executionInfo":{"status":"ok","timestamp":1764364579984,"user_tz":-60,"elapsed":599,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"a8a2bc20-aaf5-47b0-be6b-4459a8042fdf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Classifier(\n","  (embedder): Sequential(\n","    (0): Conv2d(1, 50, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (1): ReLU()\n","    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (3): Conv2d(50, 100, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (4): ReLU()\n","    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (6): Conv2d(100, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (7): ReLU()\n","    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (9): Conv2d(200, 200, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","    (10): ReLU()\n","    (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","    (12): Flatten(start_dim=1, end_dim=-1)\n","  )\n","  (classifier): Sequential(\n","    (0): Linear(in_features=3200, out_features=100, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=100, out_features=1, bias=True)\n","  )\n",")"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["## move to models.py\n","class RGB2LiDARClassifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.projector = projector\n","        self.img_embedder = CILP_model.img_embedder\n","        self.shape_classifier = lidar_cnn\n","\n","    def forward(self, imgs):\n","        img_encodings = self.img_embedder(imgs)\n","        proj_lidar_embs = self.projector(img_encodings)\n","        return self.shape_classifier(data_embs=proj_lidar_embs)"],"metadata":{"id":"qgLF-BNinWaz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["classifier = RGB2LiDARClassifier()"],"metadata":{"id":"HQHpk8H6nu3r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_correct(output, y):\n","    zero_tensor = torch.tensor([0]).to(device)\n","    pred = torch.gt(output, zero_tensor)\n","    correct = pred.eq(y.view_as(pred)).sum().item()\n","    return correct"],"metadata":{"id":"-sU992FdxT__"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_valid_metrics():\n","    classifier.eval()\n","    correct = 0\n","    batch_correct = 0\n","    for step, batch in enumerate(val_dataloader):\n","        rbg_img, _, class_idx = batch\n","        output = classifier(rbg_img)\n","        loss = nn.BCEWithLogitsLoss()(output, class_idx)\n","        batch_correct = get_correct(output, class_idx)\n","        correct += batch_correct\n","    print(f\"Valid Loss: {loss.item():2.4f} | Accuracy {correct/valid_N:2.4f}\")\n","\n","get_valid_metrics()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":339},"id":"Sd0g-_rnxWA4","executionInfo":{"status":"error","timestamp":1764364594910,"user_tz":-60,"elapsed":1727,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"323ab43b-f4a5-4f6e-ec7b-cba6685ecd45"},"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1045042187.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Valid Loss: {loss.item():2.4f} | Accuracy {correct/valid_N:2.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mget_valid_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipython-input-1045042187.py\u001b[0m in \u001b[0;36mget_valid_metrics\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mrbg_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrbg_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCEWithLogitsLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mbatch_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2394233897.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, imgs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mimg_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_embedder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mproj_lidar_embs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprojector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_encodings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_embs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproj_lidar_embs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2220002316.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Convolution + nonlinearity + downsampling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (B,50,32,32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (B,100,16,16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# (B,200,8,8)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n","\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.FloatTensor) and weight type (torch.cuda.FloatTensor) should be the same or input should be a MKLDNN tensor and weight is a dense tensor"]}]},{"cell_type":"code","source":["bce_logits_loss = nn.BCEWithLogitsLoss()\n","\n","def rgb2lidar_loss_fn(outputs, target):\n","    \"\"\"\n","    outputs: (B,) or (B,1)\n","    target: (B,) with integer 0/1 from dataloader\n","    \"\"\"\n","    # ensure float and shape match outputs\n","    target = target.float().to(outputs.device)\n","    if target.shape != outputs.shape:\n","        target = target.view_as(outputs)\n","    return bce_logits_loss(outputs, target)"],"metadata":{"id":"6F5Voamuxlq3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(classifier.parameters(), lr=1e-3)\n","epochs = 5  # or whatever you need\n","\n","train_losses, valid_losses, epoch_times, max_gpu_mb = train_model(\n","    model=classifier,\n","    optimizer=optimizer,\n","    input_fn=rgb_only_input_fn,\n","    epochs=epochs,\n","    loss_fn=rgb2lidar_loss_fn,\n","    train_dataloader=train_dataloader,\n","    valid_dataloader=val_dataloader,\n","    target_idx=2,                    # class_idx is at position 2 in the batch\n","    log_to_wandb=False,               # if you want W&B logging\n","    model_name=\"RGB2LiDARClassifier\"\n",")\n"],"metadata":{"id":"srtFQnE9yDVq"},"execution_count":null,"outputs":[]}]}