{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyN770dRR9jFIJnBo1AUYsEe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Model Architecture\n","\n","```\n","RGB ----> RGB Encoder ----\\\n","                            ----> Fusion ---> Classifier ---> cube/sphere\n","LiDAR -> LiDAR Encoder ----/\n","```\n","\n"],"metadata":{"id":"oWxsDGcq0xVI"}},{"cell_type":"markdown","source":["Multimodal fusion refers to how we combine information from different modalities (e.g., RGB and LiDAR).\n","There are three canonical levels of fusion:\n","\n","Early fusion – combine raw or early-level features\n","\n","Intermediate fusion – combine learned feature representations\n","\n","Late fusion – combine decisions or latent vectors at the end of the pipeline\n","\n","Each level has different strengths + limitations."],"metadata":{"id":"tVHp70DLYlaY"}},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"w79TQtD755-G"}},{"cell_type":"code","source":["%%capture\n","%pip install wandb weave"],"metadata":{"id":"dodCKTTU7C_t","executionInfo":{"status":"ok","timestamp":1764572269088,"user_tz":-60,"elapsed":21001,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["%%capture\n","%pip install fiftyone==1.10.0 sympy==1.12 torch==2.9.0 torchvision==0.20.0 numpy open-clip-torch"],"metadata":{"id":"KtirMD1x7EU8","executionInfo":{"status":"ok","timestamp":1764572288573,"user_tz":-60,"elapsed":19480,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ukkECgFX5HQT","executionInfo":{"status":"ok","timestamp":1764572305888,"user_tz":-60,"elapsed":17309,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"outputs":[],"source":["import os\n","from pathlib import Path\n","from google.colab import userdata\n","import time\n","\n","from PIL import Image\n","from tqdm import tqdm\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms.v2 as transforms\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torch.optim import Adam\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as colors\n","import pandas as pd\n","\n","import wandb\n","import cv2\n","import albumentations as A"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","STORAGE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5rX_aoY7M_g","executionInfo":{"status":"ok","timestamp":1764572492313,"user_tz":-60,"elapsed":8406,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"2df7cad0-11d0-4bb5-88de-251d5e55f9f2"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["DATA_PATH = STORAGE_PATH / \"multimodal_training_workshop/data\"\n","print(f\"Data path: {DATA_PATH}\")\n","print(f\"Data path exists: {DATA_PATH.exists()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QNsp1jO5-nwJ","executionInfo":{"status":"ok","timestamp":1764572495628,"user_tz":-60,"elapsed":3316,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"0928a212-775d-43e2-8875-03fd351b6e4a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Data path: /content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/multimodal_training_workshop/data\n","Data path exists: True\n"]}]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.cuda.is_available()"],"metadata":{"id":"9bGm3AQAGbBP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764572495668,"user_tz":-60,"elapsed":31,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"10158a1b-71da-44d2-adca-e3265e3e19ea"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["SEED = 51\n","NUM_WORKERS = os.cpu_count()  # Number of CPU cores\n","\n","BATCH_SIZE = 32\n","IMG_SIZE = 64\n","\n","CLASSES = [\"cubes\", \"spheres\"]\n","LABEL_MAP = {\"cubes\": 0, \"spheres\": 1}"],"metadata":{"id":"PTMc18ZD5_lr","executionInfo":{"status":"ok","timestamp":1764572498440,"user_tz":-60,"elapsed":8,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["## Antonio\n","VALID_BATCHES = 10\n","N = 1000"],"metadata":{"id":"2IY6KMqnRl6j","executionInfo":{"status":"ok","timestamp":1764572498442,"user_tz":-60,"elapsed":1,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Integrate Wandb"],"metadata":{"id":"oq3nlu7A7Qr4"}},{"cell_type":"code","source":["# Load W&B API key from .env file and make it available as env variable\n","# from dotenv import load_dotenv\n","# load_dotenv()  # loads .env automatically\n","\n","# os.environ[\"WANDB_API_KEY\"]"],"metadata":{"id":"PCMnAmeJ7S72","executionInfo":{"status":"ok","timestamp":1764572498456,"user_tz":-60,"elapsed":7,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Load W&B API key from Colab Secrets and make it available as env variable\n","wandb_key = userdata.get('WANDB_API_KEY')\n","os.environ[\"WANDB_API_KEY\"] = wandb_key"],"metadata":{"id":"LQPywS8q7UrH","executionInfo":{"status":"ok","timestamp":1764572499321,"user_tz":-60,"elapsed":866,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["wandb.login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZHH81Y7M7XRW","executionInfo":{"status":"ok","timestamp":1764572501656,"user_tz":-60,"elapsed":2334,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"a15fcab8-1e9f-4adb-c815-e96cbb8a9dbb"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n","  | |_| | '_ \\/ _` / _` |  _/ -_)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichele-marschner\u001b[0m (\u001b[33mmichele-marschner-university-of-potsdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["def init_wandb(model, fusion_name, num_params, opt_name, batch_size=BATCH_SIZE, epochs=15):\n","  config = {\n","    # \"embedding_size\": embedding_size,      ## TODO: ändert die sich? hab ich die bei fusion?\n","    \"optimizer_type\": opt_name,\n","    \"fusion_strategy\": fusion_name,\n","    \"model_architecture\": model.__class__.__name__,\n","    \"batch_size\": batch_size,\n","    \"num_epochs\": epochs,\n","    \"num_parameters\": num_params\n","  }\n","\n","  run = wandb.init(\n","    project=\"cilp-extended-assessment\",\n","    name=f\"{fusion_name}_run\",\n","    config=config,\n","    reinit=True,                          # allows multiple runs in one script\n","  )\n","\n","  return"],"metadata":{"id":"61o0qKeGiDGo","executionInfo":{"status":"ok","timestamp":1764572567320,"user_tz":-60,"elapsed":8,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["# Reproducibility"],"metadata":{"id":"E8vIz6Sx0RFX"}},{"cell_type":"code","source":["def set_seeds(seed=SEED):\n","    \"\"\"\n","    Set seeds for complete reproducibility across all libraries and operations.\n","\n","    Args:\n","        seed (int): Random seed value\n","    \"\"\"\n","    # Set environment variables before other imports\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n","\n","    # Python random module\n","    random.seed(seed)\n","\n","    # NumPy\n","    np.random.seed(seed)\n","\n","    # PyTorch CPU\n","    torch.manual_seed(seed)\n","\n","    # PyTorch GPU (all devices)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n","\n","        # CUDA deterministic operations\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","    # OpenCV\n","    cv2.setRNGSeed(seed)\n","\n","    # Albumentations (for data augmentation)\n","    try:\n","        A.seed_everything(seed)\n","    except AttributeError:\n","        # Older versions of albumentations\n","        pass\n","\n","    # PyTorch deterministic algorithms (may impact performance)\n","    try:\n","        torch.use_deterministic_algorithms(True)\n","    except RuntimeError:\n","        # Some operations don't have deterministic implementations\n","        print(\"Warning: Some operations may not be deterministic\")\n","\n","    print(f\"All random seeds set to {seed} for reproducibility\")\n","\n","\n","\n","# Usage: Call this function at the beginning and before each training phase\n","set_seeds(SEED)\n","\n","# Additional reproducibility considerations:\n","\n","def create_deterministic_training_dataloader(dataset, batch_size, shuffle=True, **kwargs):\n","    \"\"\"\n","    Create a DataLoader with deterministic behavior.\n","\n","    Args:\n","        dataset: PyTorch Dataset instance\n","        batch_size: Batch size\n","        shuffle: Whether to shuffle data\n","        **kwargs: Additional DataLoader arguments\n","\n","    Returns:\n","        Training DataLoader with reproducible behavior\n","    \"\"\"\n","    # Use a generator with fixed seed for reproducible shuffling\n","    generator = torch.Generator()\n","    generator.manual_seed(51)\n","\n","    return torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        generator=generator if shuffle else None,\n","        **kwargs\n","    )\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lccenR_P0TMW","executionInfo":{"status":"ok","timestamp":1764572572360,"user_tz":-60,"elapsed":11,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"6b7019ee-fed5-4d07-9bb3-6b641886a716"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["All random seeds set to 51 for reproducibility\n"]}]},{"cell_type":"markdown","source":["# Utility Functions"],"metadata":{"id":"ayZXNwxXGqUm"}},{"cell_type":"code","source":["def format_time(seconds):\n","    m = int(seconds // 60)\n","    s = int(seconds % 60)\n","    return f\"{m:02d}m {s:02d}s\""],"metadata":{"id":"dyT2KASL0s5H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def hex_to_RGB(hex_str):\n","    \"\"\" #FFFFFF -> [255,255,255]\"\"\"\n","    #Pass 16 to the integer function for change of base\n","    return [int(hex_str[i:i+2], 16) for i in range(1,6,2)]\n","\n","\n","def get_color_gradient(c1, c2, c3, n1, n2):\n","    \"\"\"\n","    Given two hex colors, returns a color gradient\n","    with n colors.\n","    \"\"\"\n","    c1_rgb = np.array(hex_to_RGB(c1))/255\n","    c2_rgb = np.array(hex_to_RGB(c2))/255\n","    c3_rgb = np.array(hex_to_RGB(c3))/255\n","    mix_pcts_c1_c2 = [x/(n1-1) for x in range(n1)]\n","    mix_pcts_c2_c3 = [x/(n2-1) for x in range(n2)]\n","    rgb_c1_c2 = [((1-mix)*c1_rgb + (mix*c2_rgb)) for mix in mix_pcts_c1_c2]\n","    rgb_c2_c3 = [((1-mix)*c2_rgb + (mix*c3_rgb)) for mix in mix_pcts_c2_c3]\n","    rgb_colors = rgb_c1_c2 + rgb_c2_c3\n","    return [\"#\" + \"\".join([format(int(round(val*255)), \"02x\") for val in item]) for item in rgb_colors]\n","\n","\n","cmap = colors.ListedColormap(get_color_gradient(\"#000000\", \"#76b900\", \"#f1ffd9\", 64, 128))\n"],"metadata":{"id":"JOTJBHeKR11p","executionInfo":{"status":"ok","timestamp":1764572572383,"user_tz":-60,"elapsed":13,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["def get_outputs(model, batch, inputs_idx):\n","    inputs = batch[inputs_idx].to(device)\n","    target = batch[-1].to(device)\n","    outputs = model(inputs)\n","    return outputs, target"],"metadata":{"id":"iSKYSrE-RzSs","executionInfo":{"status":"ok","timestamp":1764572572384,"user_tz":-60,"elapsed":0,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def get_torch_xyza(lidar_depth, azimuth, zenith):\n","    x = lidar_depth * torch.sin(-azimuth[:, None]) * torch.cos(-zenith[None, :])\n","    y = lidar_depth * torch.cos(-azimuth[:, None]) * torch.cos(-zenith[None, :])\n","    z = lidar_depth * torch.sin(-zenith[None, :])\n","    a = torch.where(lidar_depth < 50.0, torch.ones_like(lidar_depth), torch.zeros_like(lidar_depth))\n","    xyza = torch.stack((x, y, z, a))\n","    return xyza"],"metadata":{"id":"0H2knVe2SQYd","executionInfo":{"status":"ok","timestamp":1764572572393,"user_tz":-60,"elapsed":8,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["def format_positions(positions):\n","    return ['{0: .3f}'.format(x) for x in positions]"],"metadata":{"id":"LeFuy7B7b3Hz","executionInfo":{"status":"ok","timestamp":1764572572434,"user_tz":-60,"elapsed":40,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["def train_model(model, optimizer, input_fn, loss_fn, epochs, train_dataloader, val_dataloader, model_save_path, target_idx=-1, log_to_wandb=False, model_name=None):\n","    train_losses = []\n","    valid_losses = []\n","    epoch_times = []\n","\n","    best_val_loss = float('inf')\n","    best_model = None\n","\n","    # for GPU memory tracking\n","    max_gpu_mem_mb = 0.0\n","    use_cuda = torch.cuda.is_available()\n","\n","    if use_cuda:\n","        torch.cuda.reset_peak_memory_stats()\n","\n","    for epoch in range(epochs):\n","        start_time = time.time()                  # to track the train time per model\n","        print(f\"Epoch and start time: {epoch} und {start_time}\")\n","        model.train()\n","        train_loss = 0\n","        for step, batch in enumerate(train_dataloader):\n","\n","            rgb, lidar_xyza, position = batch\n","            rgb = rgb.to(device)\n","            lidar_xyza = lidar_xyza.to(device)\n","            position = position.to(device)\n","\n","            optimizer.zero_grad()\n","            target = batch[target_idx].to(device)\n","            outputs = model(*input_fn(batch))\n","\n","            loss = loss_fn(outputs, target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        train_loss = train_loss / (step + 1)\n","        train_losses.append(train_loss)\n","        print_loss(epoch, train_loss, outputs, target, is_train=True)\n","\n","        # ----- validation -----\n","        model.eval()\n","        valid_loss = 0\n","        with torch.no_grad():\n","          for step, batch in enumerate(val_dataloader):\n","              target = batch[target_idx].to(device)\n","              outputs = model(*input_fn(batch))\n","              valid_loss += loss_fn(outputs, target).item()\n","        valid_loss = valid_loss / (step + 1)\n","        valid_losses.append(valid_loss)\n","        print_loss(epoch, valid_loss, outputs, target, is_train=False)\n","\n","        if valid_loss < best_val_loss:\n","          best_val_loss = valid_loss\n","          best_model = model\n","          # Save the best model\n","          torch.save(best_model.state_dict(), model_save_path)\n","          print('Found and saved better weights for the model')\n","\n","        # timing\n","        epoch_time = time.time() - start_time\n","        epoch_time_formatted = format_time(epoch_time)\n","        epoch_times.append(epoch_time_formatted)\n","\n","        # GPU memory\n","        if use_cuda:\n","            gpu_mem_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)\n","            max_gpu_mem_mb = max(max_gpu_mem_mb, gpu_mem_mb)\n","\n","        # wandb logging\n","        if log_to_wandb:\n","            wandb.log(\n","                {\n","                    \"model\": model.__class__.__name__,\n","                    \"epoch\": epoch + 1,\n","                    \"train_loss\": train_loss,\n","                    \"valid_loss\": valid_loss,\n","                    \"lr\": optimizer.param_groups[0][\"lr\"],\n","                    \"epoch_time\": epoch_time_formatted,\n","                    \"max_gpu_mem_mb_epoch\": gpu_mem_mb if use_cuda else 0.0,\n","                }\n","            )\n","\n","    return train_losses, valid_losses, epoch_times, max_gpu_mem_mb"],"metadata":{"id":"UmHRslBJGsCp","executionInfo":{"status":"ok","timestamp":1764573607095,"user_tz":-60,"elapsed":43,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["def print_loss(epoch, loss, outputs, target, is_train=True, is_debug=False):\n","    loss_type = \"train loss:\" if is_train else \"valid loss:\"\n","    print(\"epoch\", str(epoch), loss_type, str(loss))\n","    if is_debug:\n","        print(\"example pred:\", format_positions(outputs[0].tolist()))\n","        print(\"example real:\", format_positions(target[0].tolist()))"],"metadata":{"id":"BOW8rdnSG1Ni","executionInfo":{"status":"ok","timestamp":1764572572443,"user_tz":-60,"elapsed":0,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def plot_losses(losses, title=\"Training & Validation Loss Comparison\", figsize=(10,6)):\n","    plt.figure(figsize=figsize)\n","\n","    for model_name, log in losses.items():\n","        train = log[\"train_losses\"]\n","        valid = log[\"valid_losses\"]\n","\n","        # plot train + valid with different line styles\n","        plt.plot(train, label=f\"{model_name} - train\", linewidth=2)\n","        plt.plot(valid, label=f\"{model_name} - valid\", linestyle=\"--\", linewidth=2)\n","\n","    plt.title(title, fontsize=16)\n","    plt.xlabel(\"Epochs\", fontsize=14)\n","    plt.ylabel(\"Loss\", fontsize=14)\n","    plt.grid(True, alpha=0.3)\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"OWej9tHY4Wjg","executionInfo":{"status":"ok","timestamp":1764572572444,"user_tz":-60,"elapsed":0,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","source":["img_transforms = transforms.Compose([\n","    transforms.Resize(IMG_SIZE),\n","    transforms.ToTensor(),  # Scales data into [0,1]    ## TODO: transforms.v2?\n","])"],"metadata":{"id":"zs0ywaXqHFbh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764572572449,"user_tz":-60,"elapsed":4,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"1ea411ec-b4e9-43c0-f710-eee3a0b5c2d6"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torchvision/transforms/v2/_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["img_transforms = transforms.Compose([\n","    transforms.ToImage(),   # Scales data into [0,1]    ## TODO: transforms.v2?\n","    transforms.Resize(IMG_SIZE),\n","    transforms.ToDtype(torch.float32, scale=True),\n","    ## normalize\n","])"],"metadata":{"id":"OErXfds6EC8O","executionInfo":{"status":"ok","timestamp":1764572572450,"user_tz":-60,"elapsed":1,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":["# Load and prepare Data"],"metadata":{"id":"Zf2LJnXb795e"}},{"cell_type":"code","source":["class ReplicatorDataset(Dataset):\n","    def __init__(self, root_dir, start_idx, stop_idx):\n","        self.root_dir = Path(root_dir)\n","\n","        # indices this dataset will cover\n","        self.indices = list(range(start_idx, stop_idx))\n","\n","        # positions: (N, 3) or (N, 4) depending on file\n","        all_positions = pd.read_csv(\n","            \"https://github.com/andandandand/practical-computer-vision/blob/main/artifacts/positions.csv?raw=1\"\n","        ).values\n","        self.positions = all_positions[start_idx:stop_idx]\n","\n","        # azimuth / zenith loaded once\n","        azimuth_path = self.root_dir / \"azimuth.npy\"\n","        zenith_path = self.root_dir / \"zenith.npy\"\n","\n","        if not azimuth_path.exists():\n","            raise FileNotFoundError(f\"azimuth.npy not found at {azimuth_path}\")\n","        if not zenith_path.exists():\n","            raise FileNotFoundError(f\"zenith.npy not found at {zenith_path}\")\n","\n","        azimuth = np.load(azimuth_path)\n","        zenith = np.load(zenith_path)\n","        # keep them as CPU tensors; move to device in training if needed\n","        self.azimuth = torch.from_numpy(azimuth)      # shape (H,)\n","        self.zenith = torch.from_numpy(zenith)        # shape (W,)\n","\n","        # dirs\n","        self.rgb_dir = self.root_dir / \"rgb\"\n","        self.lidar_dir = self.root_dir / \"lidar\"\n","\n","    def __len__(self):\n","        return len(self.indices)\n","\n","    def __getitem__(self, idx):\n","        # map dataset index -> real frame index\n","        frame_idx = self.indices[idx]\n","        file_number = f\"{frame_idx:04d}\"\n","\n","        # --- RGB ---\n","        rgb_path = self.rgb_dir / f\"{file_number}.png\"\n","        rgb_img = Image.open(rgb_path)\n","        rgb_tensor = img_transforms(rgb_img)   # still on CPU\n","\n","        # --- LiDAR depth ---\n","        lidar_path = self.lidar_dir / f\"{file_number}.npy\"\n","        lidar_depth = np.load(lidar_path)      # (H, W)\n","        lidar_depth = torch.from_numpy(lidar_depth).to(torch.float32)  # CPU\n","\n","        # --- XYZA ---\n","        lidar_xyza = get_torch_xyza(lidar_depth, self.azimuth, self.zenith)  # (4, H, W)\n","\n","        # --- position ---\n","        position_np = self.positions[idx]     # numpy row\n","        position = torch.from_numpy(position_np).to(torch.float32)  # CPU\n","\n","        return rgb_tensor, lidar_xyza, position"],"metadata":{"id":"6eYk2coTNsEB","executionInfo":{"status":"ok","timestamp":1764572572451,"user_tz":-60,"elapsed":1,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def get_replicator_dataloaders(root_dir):\n","    train_data = ReplicatorDataset(root_dir, 0, N-VALID_BATCHES*BATCH_SIZE)\n","    train_dataloader = create_deterministic_training_dataloader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","    valid_data = ReplicatorDataset(root_dir, N-VALID_BATCHES*BATCH_SIZE, N)\n","    val_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE, shuffle=False, drop_last=True)\n","    return train_data, train_dataloader, valid_data, val_dataloader\n"],"metadata":{"id":"YC_3r7bXSM9U","executionInfo":{"status":"ok","timestamp":1764572572474,"user_tz":-60,"elapsed":23,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["train_data, train_dataloader, valid_data, val_dataloader = get_replicator_dataloaders(str(DATA_PATH / \"replicator_data_cubes/\"))\n","\n","for i, sample in enumerate(train_data):\n","    print(i, *(x.shape for x in sample))\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hiXY_6-CUYOs","executionInfo":{"status":"ok","timestamp":1764572622468,"user_tz":-60,"elapsed":49993,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"d11f22e7-8d09-451a-a586-5ef2cb4e2c64"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["0 torch.Size([4, 64, 64]) torch.Size([4, 64, 64]) torch.Size([9])\n"]}]},{"cell_type":"markdown","source":["# Create Models\n","\n","Take the Net architecture from the workshop and turn it into an encoder that outputs an embedding instead of 9 positions."],"metadata":{"id":"tF_MGrCQ6MDd"}},{"cell_type":"markdown","source":["## Early Fusion Model"],"metadata":{"id":"b1ST72Ch_1ev"}},{"cell_type":"markdown","source":["**Concept:** Fuse modalities before any deep processing — usually by concatenating channels or inputs.\n","\n","```\n","input = concat(RGB, XYZ)  → shape (8, H, W)\n","-> shared CNN processes everything together\n","```\n","\n"],"metadata":{"id":"7Bt4ppI4Zuv3"}},{"cell_type":"markdown","source":["**Advantages:**\n","\n","* **Captures Early Cross-Modal Interactions:** Learns joint low-level correlations directly from raw signals.\n","* **Simple & Lightweight**: Easiest fusion method to implement; minimal architectural overhead.\n","* **Effective with Perfect Alignment:** Works well when modalities are tightly synchronized and spatially aligned.\n","\n","**Limitations:**\n","\n","* **Noise Sensitivity:** One noisy or corrupted modality directly contaminates the shared feature space.\n","* **Strict Alignment Requirement:** Modalities must have matching spatial resolution, alignment, and synchronization.\n","* **Feature Space Mismatch:** Raw modalities differ in scale, units, and distribution; one modality can dominate without careful normalization.\n","* **High Input Dimensionality:** Channel concatenation increases the input size and can require more data and compute to train effectively.\n","* **Limited Flexibility:** Assumes combining low-level signals is beneficial; may underperform when modalities carry different types of information."],"metadata":{"id":"Q_G-ymxeaDtf"}},{"cell_type":"code","source":["num_positions = 9\n","\n","class Net(nn.Module):\n","    def __init__(self, in_ch):\n","        kernel_size = 3\n","        super().__init__()\n","        flattened_size = 200 * 8 * 8\n","        self.conv1 = nn.Conv2d(in_ch, 50, kernel_size, padding=1)\n","        self.conv2 = nn.Conv2d(50, 100, kernel_size, padding=1)\n","        self.conv3 = nn.Conv2d(100, 200, kernel_size, padding=1)\n","        self.pool = nn.MaxPool2d(2)\n","        self.fc1 = nn.Linear(flattened_size, 1000)\n","        self.fc2 = nn.Linear(1000, 100)\n","        self.fc3 = nn.Linear(100, num_positions)\n","\n","    def forward(self, x):\n","        x = self.pool(F.relu(self.conv1(x)))\n","        x = self.pool(F.relu(self.conv2(x)))\n","        x = self.pool(F.relu(self.conv3(x)))\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"],"metadata":{"id":"nL0fCuzT6Pce","executionInfo":{"status":"ok","timestamp":1764572622476,"user_tz":-60,"elapsed":15,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":["## Intermediate Fusion Models"],"metadata":{"id":"7QgOUeNP_44r"}},{"cell_type":"markdown","source":["**Concept:** Each modality has its own encoder / feature extractor, and fusion happens after some layers but before classification.\n","\n","```\n","RGB → RGB_conv → RGB_features (C, H, W)\n","LiDAR → LiDAR_conv → LiDAR_features (C, H, W)\n","\n","Fusion → joint_features → FC → output\n","```\n","\n"],"metadata":{"id":"Zt0sT9SoasEG"}},{"cell_type":"markdown","source":["**Advantages:**\n","\n","* **Specialized Processing:** Each modality gets its own encoder, tailored to its characteristics.\n","* **Learned Representations:** Fusion occurs on higher-level, more discriminative features rather than raw data.\n","* **Flexible Design:** The fusion point can be chosen at different network depths, allowing fine-grained architectural control.\n","* **Easily Extendable:** New modalities can be added by including additional modality-specific branches.\n","\n","\n","**Limitations:**\n","\n","* **Architectural Complexity:** Requires designing separate modality-specific encoders and choosing an appropriate fusion point.\n","* **Higher Computational Cost:** More expensive than early fusion due to duplicated feature extractors.\n","* **Fusion Design Sensitivity:** Performance depends on the chosen fusion mechanism (concat, addition, multiplicative, bilinear, attention), which often requires experimentation.\n","* **Depth Selection Challenge:** Deciding how much unimodal processing to perform before fusion can be non-trivial and task-dependent."],"metadata":{"id":"jiBvpcPQa2pb"}},{"cell_type":"markdown","source":["Implemented 4 variants:\n","\n","*   Concatenation\n","*   Addition\n","* Hadamard Product (element-wise multiplication)\n","* Matrix-Multiplication\n","\n"],"metadata":{"id":"899VHVwubGAE"}},{"cell_type":"markdown","source":["| Fusion Method | Advantages | Limitations |\n","|---------------|------------|-------------|\n","| **Concatenation** | - Very expressive and flexible<br>- Lets the network learn arbitrary cross-modal interactions<br>- Robust and widely used baseline | - Doubles channel count → more parameters & memory<br>- Computationally heavier<br>- Fusion is unguided; model must discover interactions itself |\n","| **Addition** | - Lightweight (no increase in channels)<br>- Fast and parameter-efficient<br>- Enforces similar feature spaces between modalities | - Assumes features are aligned and comparable<br>- One noisy modality corrupts the other<br>- Sensitive to scale differences between modalities |\n","| **Multiplicative (Hadamard Product)** | - Gating effect: highlights features important in *both* modalities<br>- More expressive than addition, cheaper than concat<br>- Natural for attention-like fusion | - Suppresses features when one modality has low magnitude<br>- Requires careful normalization<br>- Can amplify noise if both activations are high |\n","| **Matrix Multiplication (Bilinear-like)** | - Captures rich pairwise correlations between modalities<br>- Most expressive among all four<br>- Enables true 2nd-order interaction learning | - Very heavy in compute & memory<br>- Requires flattening or dimensionality reduction<br>- Easily overfits; harder to train and tune |\n"],"metadata":{"id":"EdnoTDCgbe8b"}},{"cell_type":"code","source":["class ConcatIntermediateNet(nn.Module):\n","    def __init__(self, rgb_ch, xyz_ch):\n","        kernel_size = 3\n","        num_positions = 9\n","        super().__init__()\n","        self.rgb_conv1 = nn.Conv2d(rgb_ch, 25, kernel_size, padding=1)\n","        self.rgb_conv2 = nn.Conv2d(25, 50, kernel_size, padding=1)\n","        self.rgb_conv3 = nn.Conv2d(50, 100, kernel_size, padding=1)\n","\n","        self.xyz_conv1 = nn.Conv2d(xyz_ch, 25, kernel_size, padding=1)\n","        self.xyz_conv2 = nn.Conv2d(25, 50, kernel_size, padding=1)\n","        self.xyz_conv3 = nn.Conv2d(50, 100, kernel_size, padding=1)\n","\n","        # this downsampling can be done with convolutions of stride 2\n","        self.pool = nn.MaxPool2d(2)\n","        self.fc1 = nn.Linear(200 * 8 * 8, 1000)\n","        self.fc2 = nn.Linear(1000, 100)\n","        self.fc3 = nn.Linear(100, num_positions)\n","\n","    def forward(self, x_rgb, x_xyz):\n","        x_rgb = self.pool(F.relu(self.rgb_conv1(x_rgb)))\n","        x_rgb = self.pool(F.relu(self.rgb_conv2(x_rgb)))\n","        x_rgb = self.pool(F.relu(self.rgb_conv3(x_rgb)))\n","\n","        x_xyz = self.pool(F.relu(self.xyz_conv1(x_xyz)))\n","        x_xyz = self.pool(F.relu(self.xyz_conv2(x_xyz)))\n","        x_xyz = self.pool(F.relu(self.xyz_conv3(x_xyz)))\n","\n","        x = torch.cat((x_rgb, x_xyz), 1)\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"],"metadata":{"id":"E9xeOk_SS4_i","executionInfo":{"status":"ok","timestamp":1764572622478,"user_tz":-60,"elapsed":15,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["class AddIntermediateNet(nn.Module):\n","    def __init__(self, rgb_ch, xyz_ch):\n","        super().__init__()\n","        kernel_size = 3\n","        num_positions = 9\n","\n","        # same twin towers as before\n","        self.rgb_conv1 = nn.Conv2d(rgb_ch, 25, kernel_size, padding=1)\n","        self.rgb_conv2 = nn.Conv2d(25, 50, kernel_size, padding=1)\n","        self.rgb_conv3 = nn.Conv2d(50, 100, kernel_size, padding=1)\n","\n","        self.xyz_conv1 = nn.Conv2d(xyz_ch, 25, kernel_size, padding=1)\n","        self.xyz_conv2 = nn.Conv2d(25, 50, kernel_size, padding=1)\n","        self.xyz_conv3 = nn.Conv2d(50, 100, kernel_size, padding=1)\n","\n","        self.pool = nn.MaxPool2d(2)\n","\n","        # now we keep 100 channels (not 200), so:\n","        self.fc1 = nn.Linear(100 * 8 * 8, 1000)\n","        self.fc2 = nn.Linear(1000, 100)\n","        self.fc3 = nn.Linear(100, num_positions)\n","\n","    def forward(self, x_rgb, x_xyz):\n","        x_rgb = self.pool(F.relu(self.rgb_conv1(x_rgb)))\n","        x_rgb = self.pool(F.relu(self.rgb_conv2(x_rgb)))\n","        x_rgb = self.pool(F.relu(self.rgb_conv3(x_rgb)))      # (B, 100, 8, 8)\n","\n","        x_xyz = self.pool(F.relu(self.xyz_conv1(x_xyz)))\n","        x_xyz = self.pool(F.relu(self.xyz_conv2(x_xyz)))\n","        x_xyz = self.pool(F.relu(self.xyz_conv3(x_xyz)))      # (B, 100, 8, 8)\n","\n","        # intermediate fusion via addition\n","        x = x_rgb + x_xyz                                     # (B, 100, 8, 8)\n","\n","        x = torch.flatten(x, 1)\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n"],"metadata":{"id":"IkorIZs7SENq","executionInfo":{"status":"ok","timestamp":1764572622479,"user_tz":-60,"elapsed":14,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["class MatmulIntermediateNet(nn.Module):\n","    def __init__(self, rgb_ch, xyz_ch):\n","        kernel_size = 3\n","        color_chs = 9\n","        num_positions = 9\n","        super().__init__()\n","        self.rgb_conv1 = nn.Conv2d(rgb_ch, 25, kernel_size, padding=1)\n","        self.rgb_conv2 = nn.Conv2d(25, 50, kernel_size, padding=1)\n","        self.rgb_conv3 = nn.Conv2d(50, 100, kernel_size, padding=1)\n","\n","        self.xyz_conv1 = nn.Conv2d(xyz_ch, 25, kernel_size, padding=1)\n","        self.xyz_conv2 = nn.Conv2d(25, 50, kernel_size, padding=1)\n","        self.xyz_conv3 = nn.Conv2d(50, 100, kernel_size, padding=1)\n","\n","        self.pool = nn.MaxPool2d(2)\n","        self.fc1 = nn.Linear(100 * 8 * 8, 1000)\n","        self.fc2 = nn.Linear(1000, 100)\n","        self.fc3 = nn.Linear(100, num_positions)\n","\n","    def forward(self, x_rgb, x_xyz):\n","        x_rgb = self.pool(F.relu(self.rgb_conv1(x_rgb)))\n","        x_rgb = self.pool(F.relu(self.rgb_conv2(x_rgb)))\n","        x_rgb = self.pool(F.relu(self.rgb_conv3(x_rgb)))\n","\n","        x_xyz = self.pool(F.relu(self.xyz_conv1(x_xyz)))\n","        x_xyz = self.pool(F.relu(self.xyz_conv2(x_xyz)))\n","        x_xyz = self.pool(F.relu(self.xyz_conv3(x_xyz)))\n","\n","        #x = torch.matmul(x_rgb, x_xyz)\n","        x = x_rgb * x_xyz\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x"],"metadata":{"id":"CSI0lHy5S-8u","executionInfo":{"status":"ok","timestamp":1764572622480,"user_tz":-60,"elapsed":14,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":["## Late Fusion Model"],"metadata":{"id":"r1tl2t3s_8m9"}},{"cell_type":"markdown","source":["**Concept:** Each modality is processed completely separately, and only the final predictions or high-level embeddings are fused.\n","\n","```\n","RGB → RGB-Net → logits_rgb\n","LiDAR → LiDAR-Net → logits_lidar\n","\n","Fusion → final decision\n","```"],"metadata":{"id":"ACKIMjiocf8-"}},{"cell_type":"markdown","source":["**Advantages:**\n","\n","* **Robust to Missing Modalities:** The system can still operate if one modality is noisy, unreliable, or absent.\n","* **Best for Heterogeneous Modalities:** Works well when modalities differ greatly.\n","* **Modular & Simple:** Unimodal models can be trained, debugged, and replaced independently.\n","* **Leverages Existing Models:** Allows the reuse of strong off-the-shelf unimodal experts without architectural changes.\n","\n","\n","**Limitations:**\n","\n","* **Missed Interactions:** No joint feature learning — modalities never influence each other during representation learning.\n","* **Limited Expressiveness:** Simple fusion rules (e.g., averaging, weighted sum) cannot capture complex cross-modal relationships.\n","* **Information Loss:** By the time unimodal predictors output logits/embeddings, rich spatial and semantic details may already be discarded, limiting the power of fusion."],"metadata":{"id":"RSADk-61csKM"}},{"cell_type":"code","source":["rgb_net = Net(4).to(device)\n","xyz_net = Net(4).to(device)\n","\n","class LateNet(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.rgb = rgb_net\n","        self.xyz = xyz_net\n","        self.fc1 = nn.Linear(num_positions * 2, num_positions * 10)\n","        self.fc2 = nn.Linear(num_positions * 10, num_positions)\n","\n","    def forward(self, x_rgb, x_xyz):\n","        x_rgb = self.rgb(x_rgb)\n","        x_xyz = self.xyz(x_xyz)\n","        # this concatenates the features from the two branches\n","        x = torch.cat((x_rgb, x_xyz), 1)\n","        x = F.relu(self.fc1(x))\n","        x = self.fc2(x)\n","        return x"],"metadata":{"id":"YkVKO4UVSwKG","executionInfo":{"status":"ok","timestamp":1764572622904,"user_tz":-60,"elapsed":423,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":33,"outputs":[]},{"cell_type":"code","source":["### TODO aufpassen: bei Antonio zusätzlich vorher ConvEnc trainiert und gradienten eingefroern - chatty macht das hier nicht??\n","### aber auch im nächsten lab trainiert antonio nicht vor um besser vergleichen zu können mit early fusion\n","### old\n","class LateFusionModel(nn.Module):\n","    \"\"\"\n","    Late fusion:\n","    - RGB and LiDAR are encoded completely separately.\n","    - Only at the very end we concatenate their embeddings and optionally project.\n","\n","    This matches the idea:\n","        \"Separate encoders → final embeddings → fusion → similarity\"\n","    \"\"\"\n","\n","    def __init__(self, emb_dim=128, hidden_dim=256, out_dim=2):\n","        \"\"\"\n","        Args:\n","            emb_dim: size of each individual modality embedding\n","            fused_dim: size of the final fused embedding\n","        \"\"\"\n","        super().__init__()\n","\n","        # Separate encoders for each modality\n","        self.rgb_enc = ConvEncoder(in_ch=4, emb_dim=emb_dim)      ## TODO: mit Antonio abgleichen\n","        self.lidar_enc = ConvEncoder(in_ch=4, emb_dim=emb_dim)    ## TODO: mit Antonio abgleichen\n","\n","        # Linear layer to mix and reduce concatenated embeddings\n","        # Input is [rgb_emb, lidar_emb] of size 2 * emb_dim\n","        self.fusion_fc1 = nn.Linear(2 * emb_dim, hidden_dim)\n","        self.fusion_fc2 = nn.Linear(hidden_dim, out_dim)\n","\n","    def forward(self, rgb, lidar):\n","        \"\"\"\n","        Args:\n","            rgb:   (B, 4, 64, 64)\n","            lidar: (B, 4, 64, 64)\n","\n","        Returns:\n","            fused_emb: (B, fused_dim)  # used for similarity / contrastive loss\n","            rgb_emb:   (B, emb_dim)    # optional, for analysis\n","            lidar_emb: (B, emb_dim)    # optional, for analysis\n","        \"\"\"\n","        # 1) Encode each modality with its own ConvEncoder\n","        rgb_emb = self.rgb_enc(rgb)           # (B, emb_dim)\n","        lidar_emb = self.lidar_enc(lidar)     # (B, emb_dim)\n","\n","        # 2) Late fusion: concatenate embeddings\n","        x = torch.cat((rgb_emb, lidar_emb), dim=1)  # (B, 2*emb_dim)\n","\n","        # 3) Optional projection to a joint fused space   ## TODO auch hier abweichung zu antonio, hat relu + das zweite linear\n","        x = F.relu(self.fusion_fc1(x))                 # (B, fused_dim)\n","        out = self.fusion_fc2(x)                       # (B, out_dim)\n","\n","        return out\n"],"metadata":{"id":"IpOKOUFP-GSX","executionInfo":{"status":"ok","timestamp":1764572622911,"user_tz":-60,"elapsed":3,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["## old - aber vielleicht wichtig für Vergleich/Optimization\n","class IntermediateFusionModel(nn.Module):\n","    \"\"\"\n","    Intermediate fusion:\n","    - Each modality has its own early conv layers (conv1, conv2).\n","    - Their feature maps are then concatenated.\n","    - Shared later layers (conv3 + FCs) operate on the fused feature maps.\n","\n","    This lets RGB and LiDAR interact earlier and at a more local spatial level.\n","    \"\"\"\n","\n","    def __init__(self, emb_dim=128, hidden_dim=256, out_dim=2):    ## TODO Antonio gibt channels als parameter rein\n","        super().__init__()\n","        k = 3\n","        # this downsampling can be done with convolutions of stride 2\n","        self.pool = nn.MaxPool2d(2)\n","\n","        # --- Modality-specific early convolutions ---\n","\n","        # RGB branch: takes 4-channel input and produces feature maps\n","        self.rgb_conv1 = nn.Conv2d(4, 50, k, padding=1)\n","        self.rgb_conv2 = nn.Conv2d(50, 100, k, padding=1)     ## TODO Antonio hat ein Conv2d more, das erste auch nur 4,25?\n","\n","        # LiDAR branch: same structure but separate weights\n","        self.lidar_conv1 = nn.Conv2d(4, 50, k, padding=1)\n","        self.lidar_conv2 = nn.Conv2d(50, 100, k, padding=1)\n","\n","        # --- Shared later layers ---        ## TODO baut das hier komplett anders auf als Antonio, außerdem hat Matmul, also nicht concatenate sondern multiplication den besten val_loss\n","\n","        # After conv2+pool in each branch:\n","        # RGB feature maps:   (B, 100, 16, 16)\n","        # LiDAR feature maps: (B, 100, 16, 16)\n","        # Concatenated:       (B, 200, 16, 16)\n","        self.shared_conv3 = nn.Conv2d(200, 200, k, padding=1)\n","\n","        # After another pool: (B, 200, 8, 8)\n","        self.fc1 = nn.Linear(200 * 8 * 8, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, out_dim)\n","\n","    def forward(self, rgb, lidar):\n","        \"\"\"\n","        Args:\n","            rgb:   (B, 4, 64, 64)\n","            lidar: (B, 4, 64, 64)\n","\n","        Returns:\n","            emb: (B, emb_dim)  # fused embedding for similarity / contrastive loss\n","        \"\"\"\n","        # --- RGB early branch ---\n","        x_rgb = self.pool(F.relu(self.rgb_conv1(rgb)))     # (B, 50, 32, 32)\n","        x_rgb = self.pool(F.relu(self.rgb_conv2(x_rgb)))   # (B, 100, 16, 16)\n","\n","        # --- LiDAR early branch ---\n","        x_lid = self.pool(F.relu(self.lidar_conv1(lidar))) # (B, 50, 32, 32)\n","        x_lid = self.pool(F.relu(self.lidar_conv2(x_lid))) # (B, 100, 16, 16)\n","\n","        # --- Intermediate fusion on feature maps ---\n","        # Concatenate along channel dimension\n","        x = torch.cat([x_rgb, x_lid], dim=1)               # (B, 200, 16, 16)\n","\n","        # Shared conv and pooling\n","        x = self.pool(F.relu(self.shared_conv3(x)))        # (B, 200, 8, 8)\n","\n","        # Flatten and project to embedding\n","        x = torch.flatten(x, 1)                            # (B, 200*8*8)\n","        x = F.relu(self.fc1(x))                            # (B, 1000)\n","        out = self.fc2(x)                                  # (B, out_dim)\n","\n","        return out\n"],"metadata":{"id":"5RLrALQ4AW3C","executionInfo":{"status":"ok","timestamp":1764572622921,"user_tz":-60,"elapsed":5,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["# Model Training"],"metadata":{"id":"MJLmVUfdGeXB"}},{"cell_type":"code","source":["def get_early_inputs(batch):\n","    inputs_rgb = batch[0].to(device)\n","    inputs_xyz = batch[1].to(device)\n","    inputs_mm_early = torch.cat((inputs_rgb, inputs_xyz), 1)\n","    return (inputs_mm_early,)"],"metadata":{"id":"XEav2OQVSpc-","executionInfo":{"status":"ok","timestamp":1764572622927,"user_tz":-60,"elapsed":1,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["def get_inputs(batch):\n","    inputs_rgb = batch[0].to(device)\n","    inputs_xyz = batch[1].to(device)\n","    return (inputs_rgb, inputs_xyz)"],"metadata":{"id":"npcH0IrmSy76","executionInfo":{"status":"ok","timestamp":1764572622970,"user_tz":-60,"elapsed":42,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["set_seeds(SEED)\n","\n","EPOCHS = 20\n","LR = 0.0001\n","\n","loss_func = nn.MSELoss()\n","metrics = {}   # store losses for each model\n","\n","models_to_train = {\n","    \"early_fusion\": Net(8).to(device),\n","    \"intermediate_fusion_concat\": ConcatIntermediateNet(4, 4).to(device),\n","    \"intermediate_fusion_matmul\": MatmulIntermediateNet(4, 4).to(device),\n","    \"late_fusion\": LateNet().to(device),\n","}\n","\n","for name, model in models_to_train.items():\n","  model_save_path = STORAGE_PATH / f\"checkpoints/{name}.pth\"\n","\n","  # metrics for comparison table\n","  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","  opt = Adam(model.parameters(), lr=LR)\n","\n","  # initialize wandb\n","  init_wandb(\n","      model=model,\n","      fusion_name=name,\n","      num_params=num_params,\n","      opt_name = opt.__class__.__name__)\n","\n","  if name == \"early_fusion\":\n","    input_fn = get_early_inputs\n","  else:\n","    input_fn = get_inputs\n","\n","  train_losses, valid_losses, epoch_times, max_gpu_mem_mb = train_model(\n","    model=model,\n","    optimizer=opt,\n","    input_fn=input_fn,\n","    epochs=EPOCHS,\n","    loss_fn=loss_func,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    model_save_path=model_save_path,\n","    target_idx=-1,   # last element in batch is target\n","    log_to_wandb=True,\n","    model_name=name\n","  )\n","\n","  metrics[name] = {\n","      \"train_losses\": train_losses,\n","      \"valid_losses\": valid_losses,\n","      \"epoch_times\": epoch_times,\n","      \"best_valid_loss\": min(valid_losses),\n","      \"max_gpu_mem_mb\": max_gpu_mem_mb,\n","      \"num_params\": num_params,\n","  }\n","\n","  # End wandb run\n","  wandb.finish()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"_tt2AIIOA6Jj","executionInfo":{"status":"ok","timestamp":1764574313618,"user_tz":-60,"elapsed":698742,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"d76c9143-0398-4a5f-b6b6-18ee8a09cb11"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["All random seeds set to 51 for reproducibility\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Finishing previous runs because reinit is set to True."]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">early_fusion_run</strong> at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/o8e16znh' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/o8e16znh</a><br> View project at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251201_071937-o8e16znh/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.23.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251201_072015-b9f0srav</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/b9f0srav' target=\"_blank\">early_fusion_run</a></strong> to <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/b9f0srav' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/b9f0srav</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch and start time: 0 und 1764573617.0311217\n","epoch 0 train loss: 8.515963849567232\n","epoch 0 valid loss: 8.233570241928101\n","Found and saved better weights for the model\n","Epoch and start time: 1 und 1764573624.7899916\n","epoch 1 train loss: 8.209987685793923\n","epoch 1 valid loss: 7.8153589248657225\n","Found and saved better weights for the model\n","Epoch and start time: 2 und 1764573634.77336\n","epoch 2 train loss: 7.531391893114362\n","epoch 2 valid loss: 7.271495199203491\n","Found and saved better weights for the model\n","Epoch and start time: 3 und 1764573643.4777575\n","epoch 3 train loss: 7.028215748923166\n","epoch 3 valid loss: 7.0777812004089355\n","Found and saved better weights for the model\n","Epoch and start time: 4 und 1764573654.0677788\n","epoch 4 train loss: 6.792626176561628\n","epoch 4 valid loss: 7.054382944107056\n","Found and saved better weights for the model\n","Epoch and start time: 5 und 1764573662.348445\n","epoch 5 train loss: 6.631522610073998\n","epoch 5 valid loss: 6.857812833786011\n","Found and saved better weights for the model\n","Epoch and start time: 6 und 1764573673.532555\n","epoch 6 train loss: 6.441127322968983\n","epoch 6 valid loss: 6.809678936004639\n","Found and saved better weights for the model\n","Epoch and start time: 7 und 1764573683.342608\n","epoch 7 train loss: 6.301363331930978\n","epoch 7 valid loss: 6.760102558135986\n","Found and saved better weights for the model\n","Epoch and start time: 8 und 1764573691.0492728\n","epoch 8 train loss: 6.099969091869536\n","epoch 8 valid loss: 6.7533238410949705\n","Found and saved better weights for the model\n","Epoch and start time: 9 und 1764573700.6793091\n","epoch 9 train loss: 5.93265167872111\n","epoch 9 valid loss: 6.669239521026611\n","Found and saved better weights for the model\n","Epoch and start time: 10 und 1764573708.6476583\n","epoch 10 train loss: 5.798638775235131\n","epoch 10 valid loss: 6.652339601516724\n","Found and saved better weights for the model\n","Epoch and start time: 11 und 1764573717.6640046\n","epoch 11 train loss: 5.643898260025751\n","epoch 11 valid loss: 6.695098066329956\n","Epoch and start time: 12 und 1764573725.415818\n","epoch 12 train loss: 5.4889069738842196\n","epoch 12 valid loss: 6.692636489868164\n","Epoch and start time: 13 und 1764573734.499862\n","epoch 13 train loss: 5.357819148472378\n","epoch 13 valid loss: 6.853032112121582\n","Epoch and start time: 14 und 1764573742.2593758\n","epoch 14 train loss: 5.266057900020054\n","epoch 14 valid loss: 6.835264348983765\n","Epoch and start time: 15 und 1764573749.7825665\n","epoch 15 train loss: 5.061941373915899\n","epoch 15 valid loss: 6.778056621551514\n","Epoch and start time: 16 und 1764573757.5028949\n","epoch 16 train loss: 4.8630992117382235\n","epoch 16 valid loss: 6.9174822807312015\n","Epoch and start time: 17 und 1764573765.2238874\n","epoch 17 train loss: 4.714040847051711\n","epoch 17 valid loss: 6.880306339263916\n","Epoch and start time: 18 und 1764573772.672917\n","epoch 18 train loss: 4.495632455462501\n","epoch 18 valid loss: 7.019999217987061\n","Epoch and start time: 19 und 1764573780.376958\n","epoch 19 train loss: 4.371234530494327\n","epoch 19 valid loss: 7.1444460391998295\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>▂▆▃▇▃█▅▁▅▂▄▂▄▂▁▂▂▁▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▁▁</td></tr><tr><td>valid_loss</td><td>█▆▄▃▃▂▂▁▁▁▁▁▁▂▂▂▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>epoch_time_sec</td><td>7.4402</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>1107.18457</td></tr><tr><td>model</td><td>Net</td></tr><tr><td>train_loss</td><td>4.37123</td></tr><tr><td>valid_loss</td><td>7.14445</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">early_fusion_run</strong> at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/b9f0srav' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/b9f0srav</a><br> View project at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251201_072015-b9f0srav/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.23.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251201_072308-aejbicpt</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/aejbicpt' target=\"_blank\">intermediate_fusion_concat_run</a></strong> to <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/aejbicpt' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/aejbicpt</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch and start time: 0 und 1764573789.9061744\n","epoch 0 train loss: 8.507612932296027\n","epoch 0 valid loss: 8.294523239135742\n","Found and saved better weights for the model\n","Epoch and start time: 1 und 1764573797.8143268\n","epoch 1 train loss: 8.34981784366426\n","epoch 1 valid loss: 8.084389305114746\n","Found and saved better weights for the model\n","Epoch and start time: 2 und 1764573805.707572\n","epoch 2 train loss: 7.910055228642055\n","epoch 2 valid loss: 7.4321009635925295\n","Found and saved better weights for the model\n","Epoch and start time: 3 und 1764573813.700791\n","epoch 3 train loss: 7.155026367732456\n","epoch 3 valid loss: 6.95517930984497\n","Found and saved better weights for the model\n","Epoch and start time: 4 und 1764573821.8236983\n","epoch 4 train loss: 6.675247714633033\n","epoch 4 valid loss: 6.647566604614258\n","Found and saved better weights for the model\n","Epoch and start time: 5 und 1764573830.836064\n","epoch 5 train loss: 6.2017669677734375\n","epoch 5 valid loss: 6.106414413452148\n","Found and saved better weights for the model\n","Epoch and start time: 6 und 1764573838.8004363\n","epoch 6 train loss: 5.514777864728655\n","epoch 6 valid loss: 5.594792318344116\n","Found and saved better weights for the model\n","Epoch and start time: 7 und 1764573848.5622168\n","epoch 7 train loss: 4.933508623213995\n","epoch 7 valid loss: 5.027890729904175\n","Found and saved better weights for the model\n","Epoch and start time: 8 und 1764573856.2616632\n","epoch 8 train loss: 4.418968405042376\n","epoch 8 valid loss: 4.589952969551087\n","Found and saved better weights for the model\n","Epoch and start time: 9 und 1764573865.268553\n","epoch 9 train loss: 3.9448458353678384\n","epoch 9 valid loss: 4.270275950431824\n","Found and saved better weights for the model\n","Epoch and start time: 10 und 1764573874.864647\n","epoch 10 train loss: 3.54715287117731\n","epoch 10 valid loss: 4.01478328704834\n","Found and saved better weights for the model\n","Epoch and start time: 11 und 1764573883.503295\n","epoch 11 train loss: 3.3222170897892545\n","epoch 11 valid loss: 3.8547791242599487\n","Found and saved better weights for the model\n","Epoch and start time: 12 und 1764573891.4625626\n","epoch 12 train loss: 3.089537121000744\n","epoch 12 valid loss: 3.7261764287948607\n","Found and saved better weights for the model\n","Epoch and start time: 13 und 1764573900.8423567\n","epoch 13 train loss: 2.954301368622553\n","epoch 13 valid loss: 3.679693651199341\n","Found and saved better weights for the model\n","Epoch and start time: 14 und 1764573908.6634235\n","epoch 14 train loss: 2.8453123228890553\n","epoch 14 valid loss: 3.6271507024765013\n","Found and saved better weights for the model\n","Epoch and start time: 15 und 1764573916.746967\n","epoch 15 train loss: 2.670633486339024\n","epoch 15 valid loss: 3.506545567512512\n","Found and saved better weights for the model\n","Epoch and start time: 16 und 1764573925.596159\n","epoch 16 train loss: 2.5814529259999595\n","epoch 16 valid loss: 3.487256956100464\n","Found and saved better weights for the model\n","Epoch and start time: 17 und 1764573934.490234\n","epoch 17 train loss: 2.4463413386117843\n","epoch 17 valid loss: 3.365605044364929\n","Found and saved better weights for the model\n","Epoch and start time: 18 und 1764573943.6831157\n","epoch 18 train loss: 2.3297020367213657\n","epoch 18 valid loss: 3.241134691238403\n","Found and saved better weights for the model\n","Epoch and start time: 19 und 1764573951.473112\n","epoch 19 train loss: 2.1787115392230807\n","epoch 19 valid loss: 3.3181948184967043\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>▂▂▂▂▅▂█▁▅▇▄▂▇▁▂▅▅▆▁▂</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>██▇▇▆▅▅▄▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>valid_loss</td><td>██▇▆▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>epoch_time_sec</td><td>8.014</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>1062.55469</td></tr><tr><td>model</td><td>ConcatIntermediateNe...</td></tr><tr><td>train_loss</td><td>2.17871</td></tr><tr><td>valid_loss</td><td>3.31819</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">intermediate_fusion_concat_run</strong> at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/aejbicpt' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/aejbicpt</a><br> View project at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251201_072308-aejbicpt/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.23.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251201_072600-csqq4l1k</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/csqq4l1k' target=\"_blank\">intermediate_fusion_matmul_run</a></strong> to <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/csqq4l1k' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/csqq4l1k</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch and start time: 0 und 1764573961.521104\n","epoch 0 train loss: 8.461432411557151\n","epoch 0 valid loss: 8.26976523399353\n","Found and saved better weights for the model\n","Epoch and start time: 1 und 1764573968.9938958\n","epoch 1 train loss: 8.315308480035691\n","epoch 1 valid loss: 7.995764970779419\n","Found and saved better weights for the model\n","Epoch and start time: 2 und 1764573976.7329223\n","epoch 2 train loss: 7.531715892610096\n","epoch 2 valid loss: 7.134623336791992\n","Found and saved better weights for the model\n","Epoch and start time: 3 und 1764573984.741451\n","epoch 3 train loss: 6.984503768739247\n","epoch 3 valid loss: 6.8982240676879885\n","Found and saved better weights for the model\n","Epoch and start time: 4 und 1764573992.270693\n","epoch 4 train loss: 6.630430584862118\n","epoch 4 valid loss: 6.493797063827515\n","Found and saved better weights for the model\n","Epoch and start time: 5 und 1764574000.9977586\n","epoch 5 train loss: 5.845290842510405\n","epoch 5 valid loss: 5.4144923210144045\n","Found and saved better weights for the model\n","Epoch and start time: 6 und 1764574008.473771\n","epoch 6 train loss: 4.737914993649437\n","epoch 6 valid loss: 4.587513065338134\n","Found and saved better weights for the model\n","Epoch and start time: 7 und 1764574016.2379084\n","epoch 7 train loss: 3.824527479353405\n","epoch 7 valid loss: 3.826892876625061\n","Found and saved better weights for the model\n","Epoch and start time: 8 und 1764574024.621996\n","epoch 8 train loss: 3.1552680674053373\n","epoch 8 valid loss: 3.597093343734741\n","Found and saved better weights for the model\n","Epoch and start time: 9 und 1764574032.322545\n","epoch 9 train loss: 2.861067703792027\n","epoch 9 valid loss: 3.461684513092041\n","Found and saved better weights for the model\n","Epoch and start time: 10 und 1764574041.3612344\n","epoch 10 train loss: 2.5496676024936495\n","epoch 10 valid loss: 3.291454291343689\n","Found and saved better weights for the model\n","Epoch and start time: 11 und 1764574049.424565\n","epoch 11 train loss: 2.377158034415472\n","epoch 11 valid loss: 3.2897631168365478\n","Found and saved better weights for the model\n","Epoch and start time: 12 und 1764574057.209038\n","epoch 12 train loss: 2.162074957575117\n","epoch 12 valid loss: 3.3540494203567506\n","Epoch and start time: 13 und 1764574064.6530235\n","epoch 13 train loss: 2.0618650799705867\n","epoch 13 valid loss: 3.2108956813812255\n","Found and saved better weights for the model\n","Epoch and start time: 14 und 1764574073.1984522\n","epoch 14 train loss: 1.8619909967694963\n","epoch 14 valid loss: 3.1872390031814577\n","Found and saved better weights for the model\n","Epoch and start time: 15 und 1764574080.8668091\n","epoch 15 train loss: 1.628783350899106\n","epoch 15 valid loss: 3.119277763366699\n","Found and saved better weights for the model\n","Epoch and start time: 16 und 1764574088.438259\n","epoch 16 train loss: 1.4139826127461024\n","epoch 16 valid loss: 3.1168563604354858\n","Found and saved better weights for the model\n","Epoch and start time: 17 und 1764574096.8422172\n","epoch 17 train loss: 1.2549041112263997\n","epoch 17 valid loss: 2.962045741081238\n","Found and saved better weights for the model\n","Epoch and start time: 18 und 1764574104.2749736\n","epoch 18 train loss: 1.1449303002584548\n","epoch 18 valid loss: 2.954700064659119\n","Found and saved better weights for the model\n","Epoch and start time: 19 und 1764574112.2133465\n","epoch 19 train loss: 0.9722164954457965\n","epoch 19 valid loss: 3.047060418128967\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>▁▃▄▂▇▁▃▅▂█▄▃▁▆▂▂▅▁▃▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>██▇▇▆▆▅▄▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>valid_loss</td><td>██▇▆▆▄▃▂▂▂▁▁▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>epoch_time_sec</td><td>7.37403</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>1036.26074</td></tr><tr><td>model</td><td>MatmulIntermediateNe...</td></tr><tr><td>train_loss</td><td>0.97222</td></tr><tr><td>valid_loss</td><td>3.04706</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">intermediate_fusion_matmul_run</strong> at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/csqq4l1k' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/csqq4l1k</a><br> View project at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251201_072600-csqq4l1k/logs</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.23.0"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20251201_072840-2ie2fxir</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/2ie2fxir' target=\"_blank\">late_fusion_run</a></strong> to <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/2ie2fxir' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/2ie2fxir</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Epoch and start time: 0 und 1764574121.6844091\n","epoch 0 train loss: 8.465805121830531\n","epoch 0 valid loss: 8.286694240570068\n","Found and saved better weights for the model\n","Epoch and start time: 1 und 1764574130.3891132\n","epoch 1 train loss: 8.411811419895717\n","epoch 1 valid loss: 8.186436462402344\n","Found and saved better weights for the model\n","Epoch and start time: 2 und 1764574139.7838795\n","epoch 2 train loss: 8.13121934164138\n","epoch 2 valid loss: 7.802686166763306\n","Found and saved better weights for the model\n","Epoch and start time: 3 und 1764574148.613451\n","epoch 3 train loss: 7.401416506086077\n","epoch 3 valid loss: 7.208476161956787\n","Found and saved better weights for the model\n","Epoch and start time: 4 und 1764574157.3967774\n","epoch 4 train loss: 6.7741608165559315\n","epoch 4 valid loss: 6.693150186538697\n","Found and saved better weights for the model\n","Epoch and start time: 5 und 1764574166.3995264\n","epoch 5 train loss: 6.339687006814139\n","epoch 5 valid loss: 6.282789707183838\n","Found and saved better weights for the model\n","Epoch and start time: 6 und 1764574175.047337\n","epoch 6 train loss: 5.907238279070173\n","epoch 6 valid loss: 6.084729099273682\n","Found and saved better weights for the model\n","Epoch and start time: 7 und 1764574183.8282242\n","epoch 7 train loss: 5.578236557188488\n","epoch 7 valid loss: 5.688926935195923\n","Found and saved better weights for the model\n","Epoch and start time: 8 und 1764574192.8183253\n","epoch 8 train loss: 5.146277336847215\n","epoch 8 valid loss: 5.18179292678833\n","Found and saved better weights for the model\n","Epoch and start time: 9 und 1764574201.502198\n","epoch 9 train loss: 4.7148129145304365\n","epoch 9 valid loss: 4.779222846031189\n","Found and saved better weights for the model\n","Epoch and start time: 10 und 1764574210.9884987\n","epoch 10 train loss: 4.251463288352603\n","epoch 10 valid loss: 4.376831221580505\n","Found and saved better weights for the model\n","Epoch and start time: 11 und 1764574223.8927584\n","epoch 11 train loss: 3.931916770480928\n","epoch 11 valid loss: 4.193343019485473\n","Found and saved better weights for the model\n","Epoch and start time: 12 und 1764574232.9777057\n","epoch 12 train loss: 3.737325214204334\n","epoch 12 valid loss: 3.982948970794678\n","Found and saved better weights for the model\n","Epoch and start time: 13 und 1764574242.46197\n","epoch 13 train loss: 3.60333818481082\n","epoch 13 valid loss: 3.8615922212600706\n","Found and saved better weights for the model\n","Epoch and start time: 14 und 1764574253.550569\n","epoch 14 train loss: 3.416645981016613\n","epoch 14 valid loss: 3.8982186317443848\n","Epoch and start time: 15 und 1764574264.8836834\n","epoch 15 train loss: 3.317432028906686\n","epoch 15 valid loss: 3.7053241968154906\n","Found and saved better weights for the model\n","Epoch and start time: 16 und 1764574276.342293\n","epoch 16 train loss: 3.15306450071789\n","epoch 16 valid loss: 3.666919469833374\n","Found and saved better weights for the model\n","Epoch and start time: 17 und 1764574284.8136394\n","epoch 17 train loss: 3.0437558037894115\n","epoch 17 valid loss: 3.6951812982559202\n","Epoch and start time: 18 und 1764574295.003742\n","epoch 18 train loss: 2.896745647702898\n","epoch 18 valid loss: 3.58796067237854\n","Found and saved better weights for the model\n","Epoch and start time: 19 und 1764574304.541709\n","epoch 19 train loss: 2.8087223143804643\n","epoch 19 valid loss: 3.5352874517440798\n","Found and saved better weights for the model\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>epoch_time_sec</td><td>▁▂▂▁▂▁▁▂▁▃█▂▃▅▆▆▁▄▃▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>███▇▆▅▅▄▄▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>valid_loss</td><td>██▇▆▆▅▅▄▃▃▂▂▂▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>epoch_time_sec</td><td>8.51392</td></tr><tr><td>lr</td><td>0.0001</td></tr><tr><td>max_gpu_mem_mb_epoch</td><td>1409.93359</td></tr><tr><td>model</td><td>LateNet</td></tr><tr><td>train_loss</td><td>2.80872</td></tr><tr><td>valid_loss</td><td>3.53529</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">late_fusion_run</strong> at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/2ie2fxir' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment/runs/2ie2fxir</a><br> View project at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/cilp-extended-assessment</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20251201_072840-2ie2fxir/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":["set_seeds(SEED)\n","\n","matmul_net2 = MatmulIntermediateNet(4, 4).to(device)\n","matmul_net2_opt = Adam(matmul_net2.parameters(), lr=LR)\n","model_save_path = STORAGE_PATH / \"checkpoints/intermediate_fusion_hadamard.pth\"\n","\n","matmul_net2_train_losses, matmul_net2_valid_losses, epoch_times, max_gpu_mem_mb = train_model(\n","    model=matmul_net2,\n","    optimizer=matmul_net2_opt,\n","    input_fn=get_inputs,\n","    epochs=EPOCHS,\n","    loss_fn=loss_func,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    model_save_path=model_save_path,\n","    target_idx=-1,   # last element in batch is target\n","    log_to_wandb=True,\n","    model_name=\"intermediate_fusion_hadamard\"\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":443},"id":"fZW8X_XvSSM7","executionInfo":{"status":"error","timestamp":1764574323167,"user_tz":-60,"elapsed":9340,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"5718157e-e6af-4940-b545-51234197c83d"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["All random seeds set to 51 for reproducibility\n","Epoch and start time: 0 und 1764574313.9564257\n","epoch 0 train loss: 8.448761167980376\n","epoch 0 valid loss: 8.241648721694947\n","Found and saved better weights for the model\n"]},{"output_type":"error","ename":"Error","evalue":"You must call wandb.init() before wandb.log()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-774281185.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSTORAGE_PATH\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m\"models/intermediate_fusion_hadamard.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m matmul_net2_train_losses, matmul_net2_valid_losses, epoch_times, max_gpu_mem_mb = train_model(\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatmul_net2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmatmul_net2_opt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2823085151.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, input_fn, loss_fn, epochs, train_dataloader, val_dataloader, model_save_path, target_idx, log_to_wandb, model_name)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# wandb logging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlog_to_wandb\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             wandb.log(\n\u001b[0m\u001b[1;32m     72\u001b[0m                 {\n\u001b[1;32m     73\u001b[0m                     \u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/lib/preinit.py\u001b[0m in \u001b[0;36mpreinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m ) -> Callable:\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"You must call wandb.init() before {name}()\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mpreinit_wrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.log()"]}]},{"cell_type":"code","source":["set_seeds(SEED)\n","model_save_path = STORAGE_PATH / \"checkpoints/intermediate_fusion_add.pth\"\n","add_net = AddIntermediateNet(4, 4).to(device)\n","add_net_opt = Adam(add_net.parameters(), lr=LR)\n","add_net_train_losses, add_net_valid_losses, epoch_times, max_gpu_mem_mb = train_model(\n","    model=add_net,\n","    optimizer=add_net_opt,\n","    input_fn=get_inputs,\n","    epochs=EPOCHS,\n","    loss_fn=loss_func,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    model_save_path=model_save_path,\n","    target_idx=-1,   # last element in batch is target\n","    log_to_wandb=True,\n","    model_name=\"intermediate_fusion_add\"\n",")"],"metadata":{"id":"4oq9JounSpMd","executionInfo":{"status":"aborted","timestamp":1764573454823,"user_tz":-60,"elapsed":882636,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["single_mode_data = pd.read_csv('https://raw.githubusercontent.com/andandandand/practical-computer-vision/refs/heads/main/artifacts/cubes_only_single_mode_results.csv').values\n","\n","plot_x = range(len(single_mode_data))\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Average Loss\")\n","plt.plot(plot_x, add_net_valid_losses, \"goldenrod\", label = \"intermediate_fusion_add\")\n","plt.plot(plot_x, matmul_net2_valid_losses, \"green\", label = \"intermediate_fusion_hadamard\")\n","plt.legend()\n","plt.show()"],"metadata":{"id":"Y5sSKtovTq-w","executionInfo":{"status":"aborted","timestamp":1764573454823,"user_tz":-60,"elapsed":882635,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"xux2ArSO_Nnm"}},{"cell_type":"code","source":["def plot_losses(loss_dict, title=\"Validation Loss per Model\", ylabel=\"Loss\", xlabel=\"Epoch\"):\n","    \"\"\"\n","    loss_dict: dict of { \"model_name\": list_of_losses }\n","               Every list must have the same length.\n","\n","    Example:\n","        loss_dict = {\n","            \"EarlyNet\": early_valid_losses,\n","            \"LateNet\": late_valid_losses,\n","            \"CatNet\": cat_net_valid_losses,\n","            \"MatmulNet\": matmul_net_valid_losses,\n","        }\n","    \"\"\"\n","\n","    plt.figure(figsize=(8,5))\n","\n","    # Auto-generate x-axis based on first model\n","    any_key = next(iter(loss_dict))\n","    epochs = range(len(loss_dict[any_key]))\n","\n","    for model_name, losses in loss_dict.items():\n","        plt.plot(epochs, losses, label=model_name)\n","\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.title(title)\n","    plt.legend()\n","    plt.grid(True, linestyle=\"--\", alpha=0.3)\n","    plt.show()"],"metadata":{"id":"l9P8dcwrDP7y","executionInfo":{"status":"aborted","timestamp":1764573454824,"user_tz":-60,"elapsed":882635,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_dict = {\n","    \"EarlyNet\": metrics[\"early_fusion\"][\"valid_losses\"],\n","    \"CatNet\": metrics[\"intermediate_fusion_concat\"][\"valid_losses\"],\n","    \"MatmulNet\": metrics[\"intermediate_fusion_\"][\"valid_losses\"],\n","    \"AddNet\": add_net_valid_losses,\n","    \"MatmulNet2\": matmul_net2_valid_losses,\n","    \"LateNet\": metrics[\"late_fusion\"][\"valid_losses\"]\n","}\n","\n","plot_losses(loss_dict, title=\"Validation Loss Comparison\")"],"metadata":{"id":"RSfWbBea6u4x","executionInfo":{"status":"aborted","timestamp":1764573454871,"user_tz":-60,"elapsed":882681,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute avg_epoch_time\n","avg_epoch_time = sum(epoch_times) / len(epoch_times)"],"metadata":{"id":"5O6OQ7SEbSN9","executionInfo":{"status":"aborted","timestamp":1764573454872,"user_tz":-60,"elapsed":882681,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","\n","# Optional: nice display names for each key in `metrics`\n","name_map = {\n","    \"early_fusion\": \"Early Fusion\",\n","    \"late_fusion\": \"Late Fusion\",\n","    \"intermediate_fusion_concat\": \"Intermediate (Concat)\",\n","    \"intermediate_fusion_matmul\": \"Intermediate (Multiplicative)\",\n","    \"intermediate_fusion_add\": \"Intermediate (Add)\",   # if you have it\n","}\n","\n","rows = []\n","\n","for key, m in metrics.items():\n","    avg_train_loss = float(np.mean(m[\"train_losses\"]))\n","    avg_valid_loss = float(np.mean(m[\"valid_losses\"]))\n","    avg_epoch_time = float(np.mean(m[\"epoch_times\"]))\n","\n","    rows.append({\n","        \"Fusion Strategy\": name_map.get(key, key),\n","        \"Avg Valid Loss\": avg_valid_loss,\n","        \"Best Valid Loss\": float(m[\"best_valid_loss\"]),\n","        \"Num of params\": int(m[\"num_params\"]),\n","        \"Avg time per epoch (min:s)\": avg_epoch_time,\n","        \"GPU Memory (MB, max)\": float(m[\"max_gpu_mem_mb\"]),\n","    })\n","\n","df_comparison = pd.DataFrame(rows)\n","df_comparison\n"],"metadata":{"id":"eQkndCwi72Df","executionInfo":{"status":"aborted","timestamp":1764573454872,"user_tz":-60,"elapsed":882681,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# logs the comparison table to wandb\n","wandb.init(\n","    project=\"cilp-extended-assessment\",   # your project name\n","    name=\"fusion_comparison_all\",\n","    job_type=\"analysis\",\n",")\n","\n","fusion_comparison_table = wandb.Table(dataframe=df_comparison)\n","wandb.log({\"fusion_comparison\": fusion_comparison_table})\n","\n","wandb.finish()"],"metadata":{"id":"aL_anEO28AnC","executionInfo":{"status":"aborted","timestamp":1764573454873,"user_tz":-60,"elapsed":882681,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**When to use**\n","\n","**Early Fusion:**\n","* Aligned, closely related low-level modalities and comparable features\n","* Simple setup; avoid if sensors are noisy\n","\n","**Intermediate Fusion:**\n","* Modalities with different structure that benefit from separate early processing in order to learn modality-specific features   \n","* best overall balance of performance and flexibility\n","\n","**Late Fusion:**\n","* Strong, independent unimodal predictors, to combine their strengths\n","* ideal for heterogeneous or missing modalities\n","* robust fallback when one modality fails"],"metadata":{"id":"Ad-R30zkczAj"}}]}