{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWxsDGcq0xVI"
      },
      "source": [
        "# Overview: Fusion Architecture Comparison\n",
        "\n",
        "This notebook implements and evaluates several multimodal fusion strategies for RGB + LiDAR classification (cube vs. sphere).\n",
        "We compare early fusion, intermediate fusion (with multiple variants), and late fusion to understand their trade-offs in parameter count, performance, and training behavior.\n",
        "\n",
        "The goal of this notebook is to:\n",
        "* Build modality-specific or shared encoders\n",
        "* Implement different fusion strategies\n",
        "* Train models using identical settings\n",
        "* Log results with Weights & Biases\n",
        "* Produce a comparison table and loss curves\n",
        "\n",
        "---\n",
        "This section provides a high-level overview of the multimodal architecture used throughout the notebook. It illustrates how RGB and LiDAR inputs flow through their respective encoders and where fusion occurs. The diagrams help clarify the motivation behind comparing early, intermediate, and late fusion strategies.\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "RGB ----> RGB Encoder ----\\\n",
        "                            ----> Fusion ---> Classifier ---> cube/sphere\n",
        "LiDAR -> LiDAR Encoder ----/\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cFYDTuJt5AF"
      },
      "source": [
        "**The Architecture Flow:**\n",
        "\n",
        "```\n",
        "RGB Input (4ch)       LiDAR Input (4ch)\n",
        "      │                     │\n",
        "[RGB Encoder]         [XYZ Encoder]    <-- Learn specific features independently\n",
        "      │                     │\n",
        "  RGB Features          XYZ Features   <-- (e.g. 128 channels each)\n",
        "      └──────────┬──────────┘\n",
        "                 │\n",
        "           Concatenation               <-- Fuse at the \"Feature Level\"\n",
        "                 │\n",
        "         [Regression Head]             <-- Learn relationships between features\n",
        "                 │\n",
        "           Output (x,y,z)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tVHp70DLYlaY"
      },
      "source": [
        "Multimodal fusion refers to how we combine information from different modalities (e.g., RGB and LiDAR).\n",
        "There are three canonical levels of fusion:\n",
        "\n",
        "Early fusion – combine raw or early-level features\n",
        "\n",
        "Intermediate fusion – combine learned feature representations\n",
        "\n",
        "Late fusion – combine decisions or latent vectors at the end of the pipeline;  it's almost like we're creating an ensemble model, where each model has a weighted vote in the final result.\n",
        "\n",
        "Each level has different strengths + limitations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w79TQtD755-G"
      },
      "source": [
        "# Setup\n",
        "\n",
        "This section installs required packages, imports all necessary modules, and prepares utility functions from the project's src/ folder. It ensures that the notebook runs cleanly in Colab or a local environment and establishes a consistent base for all subsequent experiments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GObSvKFDxblW"
      },
      "source": [
        "## Installations & Imports\n",
        "\n",
        "Here we import PyTorch, torchvision, W&B, the fusion models, training utilities, and dataset handlers. This cell initializes all dependencies required for running the fusion experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\"\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtirMD1x7EU8"
      },
      "outputs": [],
      "source": [
        "# %pip install wandb fiftyone==1.10.0 sympy==1.12 torch==2.9.0 torchvision==0.20.0 numpy open-clip-torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukkECgFX5HQT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from torch.optim import Adam\n",
        "\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6GLz8wSxY2s"
      },
      "source": [
        "## Data Paths\n",
        "\n",
        "This section defines the filesystem layout used throughout the notebook. Google Drive is mounted to access pretrained checkpoints and datasets, and paths for data loading and result storage are established. All model loading, image saving, and logging rely on these paths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf /content/data\n",
        "!cp -r \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data_transformed\" /content/data\n",
        "# !cp -r \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data/assessment\" /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w73asd8TRGP"
      },
      "outputs": [],
      "source": [
        "from src.config import (SEED, NUM_WORKERS, BATCH_SIZE, IMG_SIZE, N, LABEL_MAP,\n",
        "                        CLASSES, NUM_CLASSES, RAW_DATA, CHECKPOINTS, DEVICE, VALID_BATCHES)\n",
        "from src.utility import set_seeds\n",
        "from src.datasets import compute_dataset_mean_std, get_dataloaders\n",
        "from src.training import compute_class_weights, get_early_inputs, get_inputs, train_model, init_wandb\n",
        "from src.visualization import build_fusion_comparison_df, plot_losses\n",
        "from src.models import EarlyFusionModel, ConcatIntermediateNet, AddIntermediateNet, MatmulIntermediateNet, HadamardIntermediateNet, LateNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gr2RzKsxfDK"
      },
      "source": [
        "## Constants\n",
        "\n",
        "We define the global configuration parameters used throughout the notebook, such as batch size, image size, number of epochs, learning rate, and label mappings. These constants ensure that every fusion model is trained under identical, reproducible settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTMc18ZD5_lr"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 15\n",
        "LR = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79pbllrpTRGX"
      },
      "outputs": [],
      "source": [
        "# Usage: Call this function at the beginning and before each training phase\n",
        "set_seeds(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oq3nlu7A7Qr4"
      },
      "source": [
        "# Integration of Wandb\n",
        "\n",
        "We authenticate with Weights & Biases using a stored API key and initialize project logging. This enables automatic tracking of training progress, hyperparameters, losses, and comparison tables for all fusion models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQPywS8q7UrH",
        "outputId": "da2ff919-f787-48cb-b42b-31b8ccda0d4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichele-marschner\u001b[0m (\u001b[33mmichele-marschner-university-of-potsdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load W&B API key from Colab Secrets and make it available as env variable\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf2LJnXb795e"
      },
      "source": [
        "# Loading and preparation of Data\n",
        "\n",
        "This section computes normalization statistics, defines input transforms, and constructs training, validation, and test dataloaders. It ensures that all fusion models receive consistent and correctly processed input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9jcaSSpS3kd",
        "outputId": "36ee50bd-afce-4e19-974f-153931cc2f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning dataset in /content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data...\n",
            "cubes: 2501 RGB files found. Matching XYZA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spheres: 9999 RGB files found. Matching XYZA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preloading LiDAR XYZA tensors into RAM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading XYZA:   0%|          | 38/12500 [00:28<2:35:57,  1.33it/s]"
          ]
        }
      ],
      "source": [
        "## Final: dynamisch\n",
        "mean, std = compute_dataset_mean_std(root_dir=RAW_DATA, img_size=IMG_SIZE)\n",
        "# mean, std = compute_dataset_mean_std_neu(root_dir=root, img_size=IMG_SIZE, seed=SEED)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OErXfds6EC8O"
      },
      "outputs": [],
      "source": [
        "## Final: dynamisch\n",
        "img_transforms = transforms.Compose([\n",
        "    transforms.ToImage(),   # Scales data into [0,1]\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize(([0.0051, 0.0052, 0.0051, 1.0000]), ([5.8023e-02, 5.8933e-02, 5.8108e-02, 2.4509e-07]))     ## assessment dataset\n",
        "    # transforms.Normalize(mean.tolist(), std.tolist())     ## assessment dataset\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiXY_6-CUYOs"
      },
      "outputs": [],
      "source": [
        "set_seeds(SEED)\n",
        "\n",
        "train_data, train_dataloader, valid_data, val_dataloader, test_data, test_dataloader = get_dataloaders(\n",
        "    str(RAW_DATA),\n",
        "    VALID_BATCHES,\n",
        "    test_frac=0.10,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    img_transforms=img_transforms,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "for i, sample in enumerate(train_data):\n",
        "    print(i, *(x.shape for x in sample))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF_MGrCQ6MDd"
      },
      "source": [
        "# Models\n",
        "\n",
        "This section introduces the different multimodal fusion strategies implemented in the experiment. It explains the conceptual differences between early fusion, intermediate fusion (with several variants), and late fusion, setting the stage for the comparative evaluation.\n",
        "\n",
        "The detailed use cases, advantages, and limitations of each fusion strategy are described in the sections below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1ST72Ch_1ev"
      },
      "source": [
        "## Early Fusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bt4ppI4Zuv3"
      },
      "source": [
        "**Concept:** Fuse modalities before any deep processing — usually by concatenating channels or inputs.\n",
        "\n",
        "```\n",
        "input = concat(RGB, XYZ)  → shape (8, H, W)\n",
        "-> shared CNN processes everything together\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_G-ymxeaDtf"
      },
      "source": [
        "**Advantages:**\n",
        "\n",
        "* **Captures Early Cross-Modal Interactions:** Learns joint low-level correlations directly from raw signals.\n",
        "* **Simple & Lightweight**: Easiest fusion method to implement; minimal architectural overhead.\n",
        "* **Effective with Perfect Alignment:** Works well when modalities are tightly synchronized and spatially aligned.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "* **Noise Sensitivity:** One noisy or corrupted modality directly contaminates the shared feature space.\n",
        "* **Strict Alignment Requirement:** Modalities must have matching spatial resolution, alignment, and synchronization.\n",
        "* **Feature Space Mismatch:** Raw modalities differ in scale, units, and distribution; one modality can dominate without careful normalization.\n",
        "* **High Input Dimensionality:** Channel concatenation increases the input size and can require more data and compute to train effectively.\n",
        "* **Limited Flexibility:** Assumes combining low-level signals is beneficial; may underperform when modalities carry different types of information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QgOUeNP_44r"
      },
      "source": [
        "## Intermediate Fusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt0sT9SoasEG"
      },
      "source": [
        "**Concept:** Each modality has its own encoder / feature extractor, and fusion happens after some layers but before classification.\n",
        "\n",
        "```\n",
        "RGB → RGB_conv → RGB_features (C, H, W)\n",
        "LiDAR → LiDAR_conv → LiDAR_features (C, H, W)\n",
        "\n",
        "Fusion → joint_features → FC → output\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiBvpcPQa2pb"
      },
      "source": [
        "**Advantages:**\n",
        "\n",
        "* **Specialized Processing:** Each modality gets its own encoder, tailored to its characteristics.\n",
        "* **Learned Representations:** Fusion occurs on higher-level, more discriminative features rather than raw data.\n",
        "* **Flexible Design:** The fusion point can be chosen at different network depths, allowing fine-grained architectural control.\n",
        "* **Easily Extendable:** New modalities can be added by including additional modality-specific branches.\n",
        "\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "* **Architectural Complexity:** Requires designing separate modality-specific encoders and choosing an appropriate fusion point.\n",
        "* **Higher Computational Cost:** More expensive than early fusion due to duplicated feature extractors.\n",
        "* **Fusion Design Sensitivity:** Performance depends on the chosen fusion mechanism (concat, addition, multiplicative, bilinear, attention), which often requires experimentation.\n",
        "* **Depth Selection Challenge:** Deciding how much unimodal processing to perform before fusion can be non-trivial and task-dependent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "899VHVwubGAE"
      },
      "source": [
        "Implemented 4 variants:\n",
        "\n",
        "*   Concatenation\n",
        "*   Addition\n",
        "* Hadamard Product (element-wise multiplication)\n",
        "* Matrix-Multiplication\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdnoTDCgbe8b"
      },
      "source": [
        "| Fusion Method | Advantages | Limitations |\n",
        "|---------------|------------|-------------|\n",
        "| **Concatenation** | - Very expressive and flexible<br>- Lets the network learn arbitrary cross-modal interactions<br>- Robust and widely used baseline | - Doubles channel count → more parameters & memory<br>- Computationally heavier<br>- Fusion is unguided; model must discover interactions itself |\n",
        "| **Addition** | - Lightweight (no increase in channels)<br>- Fast and parameter-efficient<br>- Enforces similar feature spaces between modalities | - Assumes features are aligned and comparable<br>- One noisy modality corrupts the other<br>- Sensitive to scale differences between modalities |\n",
        "| **Multiplicative (Hadamard Product)** | - Gating effect: highlights features important in *both* modalities<br>- More expressive than addition, cheaper than concat<br>- Natural for attention-like fusion | - Suppresses features when one modality has low magnitude<br>- Requires careful normalization<br>- Can amplify noise if both activations are high |\n",
        "| **Matrix Multiplication (Bilinear-like)** | - Captures rich pairwise correlations between modalities<br>- Most expressive among all four<br>- Enables true 2nd-order interaction learning | - Very heavy in compute & memory<br>- Requires flattening or dimensionality reduction<br>- Easily overfits; harder to train and tune |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1tl2t3s_8m9"
      },
      "source": [
        "## Late Fusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACKIMjiocf8-"
      },
      "source": [
        "**Concept:** Each modality is processed completely separately, and only the final predictions or high-level embeddings are fused.\n",
        "\n",
        "```\n",
        "RGB → RGB-Embedder → logits_rgb\n",
        "LiDAR → LiDAR-Embedder → logits_lidar\n",
        "\n",
        "Fusion → final decision\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSADk-61csKM"
      },
      "source": [
        "**Advantages:**\n",
        "\n",
        "* **Robust to Missing Modalities:** The system can still operate if one modality is noisy, unreliable, or absent.\n",
        "* **Best for Heterogeneous Modalities:** Works well when modalities differ greatly.\n",
        "* **Modular & Simple:** Unimodal models can be trained, debugged, and replaced independently.\n",
        "* **Leverages Existing Models:** Allows the reuse of strong off-the-shelf unimodal experts without architectural changes.\n",
        "\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "* **Missed Interactions:** No joint feature learning — modalities never influence each other during representation learning.\n",
        "* **Limited Expressiveness:** Simple fusion rules (e.g., averaging, weighted sum) cannot capture complex cross-modal relationships.\n",
        "* **Information Loss:** By the time unimodal predictors output logits/embeddings, rich spatial and semantic details may already be discarded, limiting the power of fusion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJLmVUfdGeXB"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "Here we train each fusion model using the same dataset, optimizer, and loss function. The training loop logs metrics to W&B, saves checkpoints, and stores results for later comparison. This is the core experimental section of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tt2AIIOA6Jj"
      },
      "outputs": [],
      "source": [
        "FEATURE_DIM = 128\n",
        "\n",
        "set_seeds(SEED)\n",
        "\n",
        "class_weights = compute_class_weights(train_data, NUM_CLASSES).to(DEVICE)\n",
        "loss_func = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))\n",
        "\n",
        "metrics = {}   # store losses for each model\n",
        "\n",
        "# Defines fusion models to train and compare\n",
        "models_to_train = {\n",
        "    \"early_fusion\": EarlyFusionModel(in_ch=8, output_dim=NUM_CLASSES).to(DEVICE),\n",
        "    \"intermediate_fusion_concat\": ConcatIntermediateNet(4, 4, output_dim=NUM_CLASSES, feature_dim=FEATURE_DIM).to(DEVICE),\n",
        "    \"intermediate_fusion_matmul\": MatmulIntermediateNet(4, 4, output_dim=NUM_CLASSES, feature_dim=FEATURE_DIM).to(DEVICE),\n",
        "    \"intermediate_fusion_hadamard\": HadamardIntermediateNet(4, 4, output_dim=NUM_CLASSES, feature_dim=FEATURE_DIM).to(DEVICE),\n",
        "    \"intermediate_fusion_add\": AddIntermediateNet(4, 4, output_dim=NUM_CLASSES, feature_dim=FEATURE_DIM).to(DEVICE),\n",
        "    \"late_fusion\": LateNet(4, 4, output_dim=NUM_CLASSES).to(DEVICE),\n",
        "}\n",
        "\n",
        "# === Main experiment loop over all fusion strategies ===\n",
        "for name, model in models_to_train.items():\n",
        "  model_save_path = CHECKPOINTS / f\"{name}.pth\"\n",
        "\n",
        "  # Number of trainable parameters (for the comparison table)\n",
        "  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "  opt = Adam(model.parameters(), lr=LR)\n",
        "\n",
        "  # Initialize a new Weights & Biases run for this model.\n",
        "  init_wandb(\n",
        "      model=model,\n",
        "      fusion_name=name,\n",
        "      num_params=num_params,\n",
        "      opt_name = opt.__class__.__name__)\n",
        "\n",
        "  # Choose the proper input function depending on the fusion strategy:\n",
        "  if name.startswith(\"early_fusion\"):\n",
        "    input_fn = get_early_inputs\n",
        "  else:\n",
        "    input_fn = get_inputs\n",
        "\n",
        "  results = train_model(\n",
        "    model=model,\n",
        "    optimizer=opt,\n",
        "    input_fn=input_fn,\n",
        "    epochs=EPOCHS,\n",
        "    loss_fn=loss_func,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    model_save_path=model_save_path,\n",
        "    target_idx=-1,   # last element in batch is target\n",
        "    log_to_wandb=True,\n",
        "    device=DEVICE\n",
        "  )\n",
        "\n",
        "  metrics[name] = results\n",
        "\n",
        "  # End wandb run before starting the next model\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xux2ArSO_Nnm"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "This section visualizes validation losses across all fusion strategies, builds the fusion comparison table, and logs the aggregated results to W&B.\n",
        "\n",
        "It provides the final quantitative comparison between early, intermediate, and late fusion methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx9KFu3pTRGn"
      },
      "outputs": [],
      "source": [
        "name_map = {\n",
        "    \"early_fusion\": \"Early Fusion\",\n",
        "    \"late_fusion\": \"Late Fusion\",\n",
        "    \"intermediate_fusion_concat\": \"Intermediate (Concat)\",\n",
        "    \"intermediate_fusion_matmul\": \"Intermediate (Multiplicative)\",\n",
        "    \"intermediate_fusion_hadamard\": \"Intermediate (Hadamard)\",\n",
        "    \"intermediate_fusion_add\": \"Intermediate (Add)\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSfWbBea6u4x"
      },
      "outputs": [],
      "source": [
        "loss_dict = {name: m[\"valid_losses\"] for name, m in metrics.items()}\n",
        "fig = plot_losses(loss_dict, title=\"Validation Loss per Model\")\n",
        "plt.show()\n",
        "\n",
        "df_comparison = build_fusion_comparison_df(metrics, name_map)\n",
        "df_comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZAhiYi7TRGz"
      },
      "outputs": [],
      "source": [
        "# logs the comparison table to wandb\n",
        "wandb.init(\n",
        "    project=\"cilp-extended-assessment\",   # your project name\n",
        "    name=\"fusion_comparison_all\",\n",
        "    job_type=\"analysis\",\n",
        ")\n",
        "\n",
        "fusion_comparison_table = wandb.Table(dataframe=df_comparison)\n",
        "wandb.log({\"fusion_comparison\": fusion_comparison_table})\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad-R30zkczAj"
      },
      "source": [
        "## When to Use Each Fusion Strategy\n",
        "The notebook concludes by summarizing when each fusion type is most appropriate, based on their strengths, limitations, and performance observed in the experiment:\n",
        "\n",
        "**Early Fusion:**\n",
        "* Aligned, closely related low-level modalities and comparable features\n",
        "* Simple setup; avoid if sensors are noisy\n",
        "\n",
        "**Intermediate Fusion:**\n",
        "* Modalities with different structure that benefit from separate early processing in order to learn modality-specific features   \n",
        "* best overall balance of performance and flexibility\n",
        "\n",
        "**Late Fusion:**\n",
        "* Strong, independent unimodal predictors, to combine their strengths\n",
        "* ideal for heterogeneous or missing modalities\n",
        "* robust fallback when one modality fails"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
