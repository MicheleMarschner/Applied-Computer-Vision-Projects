{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","toc_visible":true,"authorship_tag":"ABX9TyPGLEuPgJ9cJMl9bMZjrx+p"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Model Architecture\n","\n","```\n","RGB ----> RGB Encoder ----\\\n","                            ----> Fusion ---> Classifier ---> cube/sphere\n","LiDAR -> LiDAR Encoder ----/\n","```\n","\n"],"metadata":{"id":"oWxsDGcq0xVI"}},{"cell_type":"markdown","source":["**The Architecture Flow:**\n","\n","```\n","RGB Input (4ch)       LiDAR Input (4ch)\n","      │                     │\n","[RGB Encoder]         [XYZ Encoder]    <-- Learn specific features independently\n","      │                     │\n","  RGB Features          XYZ Features   <-- (e.g. 128 channels each)\n","      └──────────┬──────────┘\n","                 │\n","           Concatenation               <-- Fuse at the \"Feature Level\"\n","                 │\n","         [Regression Head]             <-- Learn relationships between features\n","                 │\n","           Output (x,y,z)\n","```"],"metadata":{"id":"8cFYDTuJt5AF"}},{"cell_type":"markdown","source":["Multimodal fusion refers to how we combine information from different modalities (e.g., RGB and LiDAR).\n","There are three canonical levels of fusion:\n","\n","Early fusion – combine raw or early-level features\n","\n","Intermediate fusion – combine learned feature representations\n","\n","Late fusion – combine decisions or latent vectors at the end of the pipeline;  it's almost like we're creating an ensemble model, where each model has a weighted vote in the final result.\n","\n","Each level has different strengths + limitations."],"metadata":{"id":"tVHp70DLYlaY"}},{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"w79TQtD755-G"}},{"cell_type":"markdown","source":["## Installations & Imports"],"metadata":{"id":"GObSvKFDxblW"}},{"cell_type":"code","source":["%%capture\n","%pip install wandb weave"],"metadata":{"id":"dodCKTTU7C_t","executionInfo":{"status":"ok","timestamp":1764879960825,"user_tz":-60,"elapsed":22083,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["%%capture\n","%pip install fiftyone==1.10.0 sympy==1.12 torch==2.9.0 torchvision==0.20.0 numpy open-clip-torch"],"metadata":{"id":"KtirMD1x7EU8","executionInfo":{"status":"ok","timestamp":1764879981648,"user_tz":-60,"elapsed":20821,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"ukkECgFX5HQT","executionInfo":{"status":"ok","timestamp":1764879993693,"user_tz":-60,"elapsed":12035,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"outputs":[],"source":["import os\n","from pathlib import Path\n","from google.colab import userdata\n","import time\n","\n","from PIL import Image\n","from tqdm import tqdm\n","import random\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms.v2 as transforms\n","from torch.utils.data import Dataset, DataLoader, Subset\n","from torch.optim import Adam\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.colors as colors\n","import pandas as pd\n","\n","import wandb\n","import cv2\n","import albumentations as A"]},{"cell_type":"markdown","source":["## Storage"],"metadata":{"id":"A6GLz8wSxY2s"}},{"cell_type":"code","source":["## stays - ggf. wo gebraucht\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","STORAGE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\")\n","\n","DATA_PATH = STORAGE_PATH / \"multimodal_training_workshop/data\"\n","print(f\"Data path: {DATA_PATH}\")\n","print(f\"Data path exists: {DATA_PATH.exists()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-5rX_aoY7M_g","executionInfo":{"status":"ok","timestamp":1764880037423,"user_tz":-60,"elapsed":43728,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"e66686c2-ed25-44ff-8b50-aeb4181c5440"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Data path: /content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/multimodal_training_workshop/data\n","Data path exists: True\n"]}]},{"cell_type":"code","source":["# !rsync -ah --progress \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data\" \"/content/data/\"\n"],"metadata":{"id":"0qQeea1oLVvJ","executionInfo":{"status":"ok","timestamp":1764880037426,"user_tz":-60,"elapsed":1,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["## stays\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.cuda.is_available()"],"metadata":{"id":"9bGm3AQAGbBP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1764880037452,"user_tz":-60,"elapsed":6,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"05f8f7ea-5563-42ca-821f-fa32ce30031c"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## Constants"],"metadata":{"id":"8gr2RzKsxfDK"}},{"cell_type":"code","source":["## stays\n","\n","SEED = 51\n","NUM_WORKERS = os.cpu_count()  # Number of CPU cores\n","\n","BATCH_SIZE = 32\n","IMG_SIZE = 64\n","\n","CLASSES = [\"cubes\", \"spheres\"]\n","NUM_CLASSES = len(CLASSES)\n","LABEL_MAP = {\"cubes\": 0, \"spheres\": 1}\n","\n","VALID_BATCHES = 10\n","N = 6500\n","EPOCHS = 10\n","LR = 0.0001"],"metadata":{"id":"PTMc18ZD5_lr","executionInfo":{"status":"ok","timestamp":1764880037455,"user_tz":-60,"elapsed":2,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["# Integration of Wandb"],"metadata":{"id":"oq3nlu7A7Qr4"}},{"cell_type":"code","source":["## stays\n","\n","# Load W&B API key from Colab Secrets and make it available as env variable\n","wandb_key = userdata.get('WANDB_API_KEY')\n","os.environ[\"WANDB_API_KEY\"] = wandb_key\n","wandb.login()"],"metadata":{"id":"LQPywS8q7UrH","executionInfo":{"status":"ok","timestamp":1764880043280,"user_tz":-60,"elapsed":5824,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"da2ff919-f787-48cb-b42b-31b8ccda0d4e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n","  | |_| | '_ \\/ _` / _` |  _/ -_)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichele-marschner\u001b[0m (\u001b[33mmichele-marschner-university-of-potsdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["## move: training.py\n","\n","def init_wandb(model, fusion_name, num_params, opt_name, batch_size=BATCH_SIZE, epochs=15):\n","  \"\"\"\n","  Initialize a Weights & Biases run for a given fusion model.\n","\n","  Args:\n","      model (nn.Module): The PyTorch model to track.\n","      fusion_name (str): Short name of the fusion strategy (e.g. \"early_fusion\").\n","      num_params (int): Total number of trainable parameters of the model.\n","      opt_name (str): Name of the optimizer (e.g. \"Adam\").\n","      batch_size (int, optional): Batch size used during training.\n","      epochs (int, optional): Number of training epochs.\n","\n","  Returns:\n","      wandb.sdk.wandb_run.Run: The initialized W&B run object.\n","  \"\"\"\n","\n","  config = {\n","    # \"embedding_size\": embedding_size,      ## TODO: ändert die sich? hab ich die bei fusion?\n","    \"optimizer_type\": opt_name,\n","    \"fusion_strategy\": fusion_name,\n","    \"model_architecture\": model.__class__.__name__,\n","    \"batch_size\": batch_size,\n","    \"num_epochs\": epochs,\n","    \"num_parameters\": num_params\n","  }\n","\n","  run = wandb.init(\n","    project=\"cilp-extended-assessment\",\n","    name=f\"{fusion_name}_run\",\n","    config=config,\n","    reinit='finish_previous',                           # allows starting a new run inside one script\n","  )\n","\n","  return run"],"metadata":{"id":"61o0qKeGiDGo","executionInfo":{"status":"ok","timestamp":1764880043307,"user_tz":-60,"elapsed":26,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["# Utility Functions\n","\n","alle in utility.py reinschieben\n","\n"],"metadata":{"id":"ayZXNwxXGqUm"}},{"cell_type":"code","source":["def set_seeds(seed=SEED):\n","    \"\"\"\n","    Set seeds for complete reproducibility across all libraries and operations.\n","\n","    Args:\n","        seed (int): Random seed value\n","    \"\"\"\n","    # Set environment variables before other imports\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n","\n","    # Python random module\n","    random.seed(seed)\n","\n","    # NumPy\n","    np.random.seed(seed)\n","\n","    # PyTorch CPU\n","    torch.manual_seed(seed)\n","\n","    # PyTorch GPU (all devices)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n","\n","        # CUDA deterministic operations\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","    # OpenCV\n","    cv2.setRNGSeed(seed)\n","\n","    # Albumentations (for data augmentation)\n","    try:\n","        A.seed_everything(seed)\n","    except AttributeError:\n","        # Older versions of albumentations\n","        pass\n","\n","    # PyTorch deterministic algorithms (may impact performance)\n","    try:\n","        torch.use_deterministic_algorithms(True)\n","    except RuntimeError:\n","        # Some operations don't have deterministic implementations\n","        print(\"Warning: Some operations may not be deterministic\")\n","\n","    print(f\"All random seeds set to {seed} for reproducibility\")\n","\n","\n","\n","# Usage: Call this function at the beginning and before each training phase\n","set_seeds(SEED)\n","\n","# Additional reproducibility considerations:\n","\n","def create_deterministic_training_dataloader(dataset, batch_size, shuffle=True, **kwargs):\n","    \"\"\"\n","    Create a DataLoader with deterministic shuffling behaviour.\n","\n","    Args:\n","        dataset (Dataset): PyTorch Dataset instance.\n","        batch_size (int): Batch size.\n","        shuffle (bool): Whether to shuffle data.\n","        **kwargs: Additional DataLoader keyword arguments.\n","\n","    Returns:\n","        DataLoader: Training DataLoader with reproducible sampling.\n","    \"\"\"\n","    # Use a generator with fixed seed for reproducible shuffling\n","    generator = torch.Generator()\n","    generator.manual_seed(SEED)\n","\n","    return torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        generator=generator if shuffle else None,\n","        **kwargs\n","    )\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lccenR_P0TMW","executionInfo":{"status":"ok","timestamp":1764880043368,"user_tz":-60,"elapsed":35,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"83de3ec2-b7c7-4ef8-b180-6389ac9f5c00"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["All random seeds set to 51 for reproducibility\n"]}]},{"cell_type":"code","source":["def format_time(seconds):\n","    \"\"\"\n","    Convert a duration in seconds to a human-readable 'MMm SSs' string.\n","\n","    Args:\n","        seconds (float): Duration in seconds.\n","\n","    Returns:\n","        str: Formatted duration, e.g. \"02m 15s\".\n","    \"\"\"\n","    m = int(seconds // 60)\n","    s = int(seconds % 60)\n","    return f\"{m:02d}m {s:02d}s\""],"metadata":{"id":"dyT2KASL0s5H","executionInfo":{"status":"ok","timestamp":1764880043382,"user_tz":-60,"elapsed":13,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["def get_torch_xyza(lidar_depth, azimuth, zenith):\n","    \"\"\"\n","    Convert LiDAR depth + angular coordinates into an XYZA tensor.\n","\n","    Args:\n","        lidar_depth (torch.Tensor): Radial distances of shape (H, W).\n","        azimuth (torch.Tensor): Azimuth angles in radians, shape (H,).\n","        zenith (torch.Tensor): Zenith angles in radians, shape (W,).\n","\n","    Returns:\n","        torch.Tensor: Stacked tensor of shape (4, H, W) containing\n","            X, Y, Z coordinates and a validity mask A.\n","    \"\"\"\n","    # Broadcast azimuth (per row) and zenith (per column) to full image grid\n","    # and convert polar coordinates into Cartesian coordinates.\n","    x = lidar_depth * torch.sin(-azimuth[:, None]) * torch.cos(-zenith[None, :])\n","    y = lidar_depth * torch.cos(-azimuth[:, None]) * torch.cos(-zenith[None, :])\n","    z = lidar_depth * torch.sin(-zenith[None, :])\n","\n","    # A is a binary mask: 1 for valid points, 0 for \"no return\" / far-away\n","    a = torch.where(lidar_depth < 50.0,\n","                    torch.ones_like(lidar_depth),\n","                    torch.zeros_like(lidar_depth))\n","\n","    xyza = torch.stack([x, y, z, a], dim=0)\n","    return xyza"],"metadata":{"id":"0H2knVe2SQYd","executionInfo":{"status":"ok","timestamp":1764880043401,"user_tz":-60,"elapsed":16,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["def format_positions(positions):\n","    \"\"\"\n","    Format a sequence of numerical positions as nicely aligned strings.\n","\n","    Args:\n","        positions (Iterable[float]): Sequence of scalar values.\n","\n","    Returns:\n","        list[str]: List of strings with 4 decimal places.\n","    \"\"\"\n","    return ['{0: .4f}'.format(x) for x in positions]"],"metadata":{"id":"LeFuy7B7b3Hz","executionInfo":{"status":"ok","timestamp":1764880043457,"user_tz":-60,"elapsed":55,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["def create_subset(size, dataset):\n","    \"\"\"\n","    Create a random subset of a given dataset.\n","\n","    Args:\n","        size (int): Desired number of samples in the subset.\n","        dataset (torch.utils.data.Dataset): Dataset to sample from.\n","\n","    Returns:\n","        torch.utils.data.Subset: Random subset of the given dataset.\n","    \"\"\"\n","    # Sample unique indices uniformly at random from the dataset\n","    indices = np.random.choice(size, size=size, replace=False)\n","    return Subset(dataset, indices)"],"metadata":{"id":"g0D8-5Yjy3qt","executionInfo":{"status":"ok","timestamp":1764880043458,"user_tz":-60,"elapsed":0,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["def print_loss(epoch, loss, outputs, target, is_train=True, is_debug=False):\n","    \"\"\"\n","    Print a formatted loss line and optionally one example prediction.\n","\n","    Args:\n","        epoch (int): Current epoch index.\n","        loss (float or Tensor): Loss value for this epoch.\n","        outputs (torch.Tensor): Model predictions for the current batch.\n","        target (torch.Tensor): Ground-truth targets for the current batch.\n","        is_train (bool): If True, label as training loss; else validation.\n","        is_debug (bool): If True, also print one prediction/target pair.\n","    \"\"\"\n","\n","    loss_type = \"train loss:\" if is_train else \"valid loss:\"\n","    print(\"epoch\", str(epoch), loss_type, str(loss))\n","\n","    if is_debug:\n","        print(\"example pred:\", format_positions(outputs[0].tolist()))\n","        print(\"example real:\", format_positions(target[0].tolist()))"],"metadata":{"id":"BOW8rdnSG1Ni","executionInfo":{"status":"ok","timestamp":1764880043459,"user_tz":-60,"elapsed":0,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["## Final: löschen\n","## move: visualization.py\n","\n","def plot_losses(losses, title=\"Training & Validation Loss Comparison\", figsize=(10,6)):\n","    \"\"\"\n","    Legacy plotting helper to show train/valid losses for multiple models.\n","\n","    Args:\n","        losses (dict): Mapping model_name -> {\"train_losses\": [...],\n","                                              \"valid_losses\": [...]}.\n","        title (str): Plot title.\n","        figsize (tuple): Matplotlib figure size.\n","    \"\"\"\n","    plt.figure(figsize=figsize)\n","\n","    for model_name, log in losses.items():\n","        train = log[\"train_losses\"]\n","        valid = log[\"valid_losses\"]\n","\n","        # plot train + valid with different line styles\n","        plt.plot(train, label=f\"{model_name} - train\", linewidth=2)\n","        plt.plot(valid, label=f\"{model_name} - valid\", linestyle=\"--\", linewidth=2)\n","\n","    plt.title(title, fontsize=16)\n","    plt.xlabel(\"Epochs\", fontsize=14)\n","    plt.ylabel(\"Loss\", fontsize=14)\n","    plt.grid(True, alpha=0.3)\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"OWej9tHY4Wjg","executionInfo":{"status":"ok","timestamp":1764880043460,"user_tz":-60,"elapsed":0,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# Loading and preparation of Data"],"metadata":{"id":"Zf2LJnXb795e"}},{"cell_type":"code","source":["## Final: überdenken woher datenset kommen soll\n","## move: datasets.py\n","\n","class AssessmentXYZADataset(Dataset):\n","    \"\"\"\n","    Dataset for the CILP XYZ + RGB assessment data.\n","\n","    It expects the following folder structure:\n","\n","        root/\n","          cubes/\n","            rgb/*.png\n","            lidar_xyza/*.npy\n","          spheres/\n","            rgb/*.png\n","            lidar_xyza/*.npy\n","\n","    Each sample consists of an RGB image, a LiDAR XYZA tensor and a class label.\n","    \"\"\"\n","    def __init__(self, root_dir, start_idx=0, end_idx=None,\n","                 transform_rgb=None, transform_lidar=None, shuffle=True):\n","        \"\"\"\n","        Args:\n","            root_dir (str or Path): Root directory of the dataset.\n","            start_idx (int): Start index (inclusive) for slicing the dataset.\n","            end_idx (int or None): End index (exclusive); if None use all.\n","            transform_rgb (callable or None): Transform applied to RGB images.\n","            transform_lidar (callable or None): Transform applied to LiDAR tensors.\n","            shuffle (bool): If True, shuffle the full list of samples once.\n","        \"\"\"\n","        self.root_dir = Path(root_dir)\n","        self.transform_rgb = transform_rgb\n","        self.transform_lidar = transform_lidar\n","\n","        self.classes = [\"cubes\", \"spheres\"]\n","        self.label_map = {\"cubes\": 0, \"spheres\": 1}\n","\n","        samples = []\n","\n","        print(f\"Scanning dataset in {root_dir}...\")\n","        for cls in self.classes:\n","            cls_dir = self.root_dir / cls\n","            rgb_dir = cls_dir / \"rgb\"\n","            lidar_dir = cls_dir / \"lidar_xyza\"\n","\n","            rgb_files = sorted(rgb_dir.glob(\"*.png\"))\n","            print(f\"{cls}: {len(rgb_files)} RGB files found. Matching XYZA...\")\n","\n","            for rgb_path in tqdm(rgb_files, desc=f\"{cls} matching\", leave=False):\n","                stem = rgb_path.stem\n","                lidar_path = lidar_dir / f\"{stem}.npy\"\n","                if lidar_path.exists():\n","                    samples.append({\n","                        \"rgb\": rgb_path,\n","                        \"lidar_xyza\": lidar_path,\n","                        \"label\": self.label_map[cls],\n","                    })\n","\n","        if shuffle:\n","            rng = random.Random(SEED)\n","            rng.shuffle(samples)\n","\n","        if end_idx is None:\n","            end_idx = len(samples)\n","        self.samples = samples[start_idx:end_idx]\n","\n","        # Preload LiDAR tensors into memory since they are small and fast to cache\n","        print(f\"Preloading LiDAR XYZA tensors into RAM...\")\n","        self.lidar_tensors = []\n","        for item in tqdm(self.samples, desc=\"Loading XYZA\", leave=False):\n","            lidar_np = np.load(item[\"lidar_xyza\"])        # (4, H, W)\n","            lidar_t  = torch.from_numpy(lidar_np).float() # CPU tensor\n","            self.lidar_tensors.append(lidar_t)\n","\n","        print(\n","            f\"Dataset ready: {len(self.samples)} samples loaded.\\n\"\n","            f\"Slice [{start_idx}:{end_idx}]\"\n","        )\n","\n","    def __len__(self):\n","        \"\"\"Return the number of samples in this dataset slice.\"\"\"\n","        return len(self.samples)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        Load a single (rgb, lidar, label) triplet.\n","\n","        Returns:\n","            tuple: (rgb_tensor, lidar_tensor, label_tensor)\n","        \"\"\"\n","        item  = self.samples[idx]\n","        lidar = self.lidar_tensors[idx]\n","\n","        # RGB image is loaded on the fly\n","        rgb = Image.open(item[\"rgb\"])\n","        if self.transform_rgb:\n","            rgb = self.transform_rgb(rgb)\n","\n","        if self.transform_lidar:\n","            lidar = self.transform_lidar(lidar)\n","\n","        label = torch.tensor(item[\"label\"], dtype=torch.long)\n","        return rgb, lidar, label\n"],"metadata":{"id":"_9trlaBL2SHi","executionInfo":{"status":"ok","timestamp":1764880043500,"user_tz":-60,"elapsed":39,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["## move: datasets.py\n","\n","def compute_dataset_mean_std(root_dir):\n","    \"\"\"\n","    Estimate the per-channel mean and std for the RGB+LiDAR data.\n","\n","    Args:\n","        root_dir (str or Path): Root directory passed to AssessmentXYZADataset.\n","\n","    Returns:\n","        tuple[torch.Tensor, torch.Tensor]:\n","            mean and std tensors with shape (C,).\n","    \"\"\"\n","\n","    stats_transforms = transforms.Compose([\n","      transforms.Resize(IMG_SIZE),\n","      transforms.ToImage(),\n","      transforms.ToDtype(torch.float32, scale=True),  # [0,1], 4 channels\n","    ])\n","\n","    stats_dataset = AssessmentXYZADataset(\n","        root_dir=root_dir,\n","        start_idx=0,\n","        end_idx=None,          # or e.g. 1000 to subsample\n","        transform_rgb=stats_transforms,\n","    )\n","\n","    subset_size = min(2000, len(stats_dataset)*0.3)\n","    subset_for_stats = create_subset(size=subset_size)\n","\n","    loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=2)\n","\n","    mean = 0.\n","    std = 0.\n","    total = 0\n","\n","    for images, _, _ in tqdm(loader, desc=\"Computing mean/std\"):\n","        images = images.float()       # B, C, H, W\n","        batch_size = images.size(0)\n","\n","        # compute mean over batch (channels only!)\n","        mean += images.mean(dim=[0, 2, 3]) * batch_size\n","\n","        # compute std over batch\n","        std += images.std(dim=[0, 2, 3]) * batch_size\n","\n","        total += batch_size\n","\n","    mean /= total\n","    std /= total\n","\n","    return mean, std\n"],"metadata":{"id":"Rd_eqja4y-Y3","executionInfo":{"status":"ok","timestamp":1764880043518,"user_tz":-60,"elapsed":18,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["def compute_dataset_mean_std_neu(root_dir):\n","    \"\"\"\n","    Estimate the per-channel mean and std for the RGB+LiDAR data.\n","\n","    Args:\n","        root_dir (str or Path): Root directory passed to AssessmentXYZADataset.\n","\n","    Returns:\n","        tuple[torch.Tensor, torch.Tensor]:\n","            mean and std tensors with shape (C,).\n","    \"\"\"\n","    stats_transforms = transforms.Compose([\n","        transforms.Resize(IMG_SIZE),\n","        transforms.ToImage(),\n","        transforms.ToDtype(torch.float32, scale=True),  # [0,1], 4 channels\n","    ])\n","\n","    stats_dataset = AssessmentXYZADataset(\n","        root_dir=root_dir,\n","        transform_rgb=stats_transforms,\n","        transform_lidar=None,\n","        shuffle=False,\n","    )\n","\n","    loader = DataLoader(stats_dataset, batch_size=64, shuffle=False)\n","\n","    # Accumulate running sum and sum of squares to compute mean/std\n","    channel_sum = torch.zeros(4)\n","    channel_sq_sum = torch.zeros(4)\n","    num_pixels = 0\n","\n","    for rgb, _, _ in tqdm(loader, desc=\"Computing mean/std\"):\n","        # rgb shape: (B, C, H, W)\n","        b, c, h, w = rgb.shape\n","        num_pixels += b * h * w\n","        channel_sum += rgb.sum(dim=[0, 2, 3])\n","        channel_sq_sum += (rgb ** 2).sum(dim=[0, 2, 3])\n","\n","    mean = channel_sum / num_pixels\n","    std = torch.sqrt(channel_sq_sum / num_pixels - mean ** 2)\n","    return mean, std\n"],"metadata":{"id":"Fx0MbBTy8CrO","executionInfo":{"status":"ok","timestamp":1764880043776,"user_tz":-60,"elapsed":24,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["## Final: dynamisch\n","root = STORAGE_PATH / \"data\"\n","mean, std = compute_dataset_mean_std(root_dir=root)"],"metadata":{"id":"y9jcaSSpS3kd","colab":{"base_uri":"https://localhost:8080/"},"outputId":"36ee50bd-afce-4e19-974f-153931cc2f9f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Scanning dataset in /content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data...\n","cubes: 2501 RGB files found. Matching XYZA...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["spheres: 9999 RGB files found. Matching XYZA...\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["Preloading LiDAR XYZA tensors into RAM...\n"]},{"output_type":"stream","name":"stderr","text":["Loading XYZA:   0%|          | 38/12500 [00:28<2:35:57,  1.33it/s]"]}]},{"cell_type":"code","source":["## Final: dynamisch\n","img_transforms = transforms.Compose([\n","    transforms.ToImage(),   # Scales data into [0,1]\n","    transforms.Resize(IMG_SIZE),\n","    transforms.ToDtype(torch.float32, scale=True),\n","    transforms.Normalize(([0.0051, 0.0052, 0.0051, 1.0000]), ([5.8023e-02, 5.8933e-02, 5.8108e-02, 2.4509e-07]))     ## assessment dataset\n","    # transforms.Normalize(mean.tolist(), std.tolist())     ## assessment dataset\n","])"],"metadata":{"id":"OErXfds6EC8O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## move: datasets.py\n","\n","def get_dataloaders(root_dir):\n","    \"\"\"\n","    Create train and validation datasets + dataloaders.\n","\n","    The split is done by taking the first part for training and the last\n","    VALID_BATCHES * BATCH_SIZE samples for validation.\n","\n","    Args:\n","        root_dir (str or Path): Root path to the cubes/spheres data.\n","\n","    Returns:\n","        tuple: (train_dataset, train_dataloader,\n","                valid_dataset, val_dataloader)\n","    \"\"\"\n","    train_data = AssessmentXYZADataset(\n","        root_dir,\n","        0,\n","        N-VALID_BATCHES*BATCH_SIZE,\n","        img_transforms\n","    )\n","\n","    train_dataloader = create_deterministic_training_dataloader(\n","        train_data,\n","        batch_size=BATCH_SIZE,\n","        shuffle=True,\n","        drop_last=True\n","    )\n","\n","    valid_data = AssessmentXYZADataset(\n","        root_dir,\n","        N-VALID_BATCHES*BATCH_SIZE,\n","        N,\n","        img_transforms\n","    )\n","\n","    val_dataloader = DataLoader(\n","        valid_data,\n","        batch_size=BATCH_SIZE,\n","        shuffle=False,\n","        drop_last=True\n","    )\n","\n","    return train_data, train_dataloader, valid_data, val_dataloader\n"],"metadata":{"id":"YC_3r7bXSM9U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data, train_dataloader, valid_data, val_dataloader = get_dataloaders(str(STORAGE_PATH / \"data\"))\n","\n","for i, sample in enumerate(train_data):\n","    print(i, *(x.shape for x in sample))\n","    break"],"metadata":{"id":"hiXY_6-CUYOs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Models\n","\n","Take the EmbedderMaxPool architecture from the workshop and turn it into an encoder that outputs an embedding instead of 9 positions."],"metadata":{"id":"tF_MGrCQ6MDd"}},{"cell_type":"markdown","source":["## Embedder"],"metadata":{"id":"uUB25fn4x3Th"}},{"cell_type":"code","source":["## move: model.py\n","\n","class EmbedderMaxPool(nn.Module):\n","    \"\"\"\n","    Convolutional encoder that down-samples via MaxPool2d and outputs a flat feature vector.\n","\n","    This is used as a shared building block for the early and intermediate\n","    fusion architectures.\n","    \"\"\"\n","    def __init__(self, in_ch, feature_dim=200):\n","        \"\"\"\n","        Args:\n","            in_ch (int): Number of input channels.\n","            feature_dim (int): Number of output channels in the last conv layer.\n","        \"\"\"\n","        kernel_size = 3\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_ch, 50, kernel_size, padding=1)\n","        self.conv2 = nn.Conv2d(50, 100, kernel_size, padding=1)\n","        self.conv3 = nn.Conv2d(100, feature_dim, kernel_size, padding=1)\n","        self.pool = nn.MaxPool2d(2)\n","\n","        # For 64x64 input and 3 pooling steps we end up at 8x8 spatial size.\n","        self.flatten_dim = 200 * 8 * 8\n","\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (torch.Tensor): Input tensor of shape (B, C, H, W).\n","\n","        Returns:\n","            torch.Tensor: Flattened feature tensor of shape (B, flatten_dim).\n","        \"\"\"\n","        x = self.pool(F.relu(self.conv1(x)))    # 64x64 -> 32x32\n","        x = self.pool(F.relu(self.conv2(x)))    # 32x32 -> 16x16\n","        x = self.pool(F.relu(self.conv3(x)))    # 16x16 -> 8x8\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","\n","        return x"],"metadata":{"id":"nL0fCuzT6Pce"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Early Fusion Model"],"metadata":{"id":"b1ST72Ch_1ev"}},{"cell_type":"markdown","source":["**Concept:** Fuse modalities before any deep processing — usually by concatenating channels or inputs.\n","\n","```\n","input = concat(RGB, XYZ)  → shape (8, H, W)\n","-> shared CNN processes everything together\n","```\n","\n"],"metadata":{"id":"7Bt4ppI4Zuv3"}},{"cell_type":"markdown","source":["**Advantages:**\n","\n","* **Captures Early Cross-Modal Interactions:** Learns joint low-level correlations directly from raw signals.\n","* **Simple & Lightweight**: Easiest fusion method to implement; minimal architectural overhead.\n","* **Effective with Perfect Alignment:** Works well when modalities are tightly synchronized and spatially aligned.\n","\n","**Limitations:**\n","\n","* **Noise Sensitivity:** One noisy or corrupted modality directly contaminates the shared feature space.\n","* **Strict Alignment Requirement:** Modalities must have matching spatial resolution, alignment, and synchronization.\n","* **Feature Space Mismatch:** Raw modalities differ in scale, units, and distribution; one modality can dominate without careful normalization.\n","* **High Input Dimensionality:** Channel concatenation increases the input size and can require more data and compute to train effectively.\n","* **Limited Flexibility:** Assumes combining low-level signals is beneficial; may underperform when modalities carry different types of information."],"metadata":{"id":"Q_G-ymxeaDtf"}},{"cell_type":"code","source":["## move: model.py\n","\n","class FullyConnectedHead(nn.Module):\n","    \"\"\"\n","    Fully-connected classification head mapping features to class logits.\n","    \"\"\"\n","    def __init__(self, input_dim, output_dim=2):\n","        \"\"\"\n","        Args:\n","            input_dim (int): Dimensionality of the flattened feature vector.\n","            output_dim (int): Number of output classes.\n","        \"\"\"\n","        super().__init__()\n","        self.fc1 = nn.Linear(input_dim, 1000)\n","        self.fc2 = nn.Linear(1000, 100)\n","        self.fc3 = nn.Linear(100, output_dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (torch.Tensor): Input features of shape (B, input_dim).\n","\n","        Returns:\n","            torch.Tensor: Class logits of shape (B, output_dim).\n","        \"\"\"\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","\n","        return x"],"metadata":{"id":"Tr0SNgd11sPy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## move: model.py\n","\n","class EarlyFusionModel(nn.Module):\n","    \"\"\"\n","    Early fusion model that concatenates RGB and LiDAR channels at the input.\n","\n","    The 8-channel tensor (4 RGB-like + 4 XYZA) is passed through a shared\n","    CNN embedder and a fully-connected classification head.\n","    \"\"\"\n","    def __init__(self, in_ch=8, output_dim=2):\n","        \"\"\"\n","        Args:\n","            in_ch (int): Number of input channels after concatenation.\n","            output_dim (int): Number of output classes.\n","        \"\"\"\n","        super().__init__()\n","\n","        # Shared embedder for all channels\n","        self.embedder = EmbedderMaxPool(in_ch)\n","\n","        # Fully-connected head on top of the shared embedding\n","        self.fullyConnected = FullyConnectedHead(\n","            input_dim=self.embedder.flatten_dim,\n","            output_dim=output_dim\n","        )\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Args:\n","            x (torch.Tensor): Input tensor of shape (B, in_ch, 64, 64).\n","\n","        Returns:\n","            torch.Tensor: Class logits of shape (B, output_dim).\n","        \"\"\"\n","        features = self.embedder(x)     # → (B, 12800)\n","        preds = self.fullyConnected(features)  # → (B, output_dim)\n","        return preds"],"metadata":{"id":"n0AaZ2PXc7zy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Intermediate Fusion Model"],"metadata":{"id":"7QgOUeNP_44r"}},{"cell_type":"markdown","source":["**Concept:** Each modality has its own encoder / feature extractor, and fusion happens after some layers but before classification.\n","\n","```\n","RGB → RGB_conv → RGB_features (C, H, W)\n","LiDAR → LiDAR_conv → LiDAR_features (C, H, W)\n","\n","Fusion → joint_features → FC → output\n","```\n","\n"],"metadata":{"id":"Zt0sT9SoasEG"}},{"cell_type":"markdown","source":["**Advantages:**\n","\n","* **Specialized Processing:** Each modality gets its own encoder, tailored to its characteristics.\n","* **Learned Representations:** Fusion occurs on higher-level, more discriminative features rather than raw data.\n","* **Flexible Design:** The fusion point can be chosen at different network depths, allowing fine-grained architectural control.\n","* **Easily Extendable:** New modalities can be added by including additional modality-specific branches.\n","\n","\n","**Limitations:**\n","\n","* **Architectural Complexity:** Requires designing separate modality-specific encoders and choosing an appropriate fusion point.\n","* **Higher Computational Cost:** More expensive than early fusion due to duplicated feature extractors.\n","* **Fusion Design Sensitivity:** Performance depends on the chosen fusion mechanism (concat, addition, multiplicative, bilinear, attention), which often requires experimentation.\n","* **Depth Selection Challenge:** Deciding how much unimodal processing to perform before fusion can be non-trivial and task-dependent."],"metadata":{"id":"jiBvpcPQa2pb"}},{"cell_type":"markdown","source":["Implemented 4 variants:\n","\n","*   Concatenation\n","*   Addition\n","* Hadamard Product (element-wise multiplication)\n","* Matrix-Multiplication\n","\n"],"metadata":{"id":"899VHVwubGAE"}},{"cell_type":"markdown","source":["| Fusion Method | Advantages | Limitations |\n","|---------------|------------|-------------|\n","| **Concatenation** | - Very expressive and flexible<br>- Lets the network learn arbitrary cross-modal interactions<br>- Robust and widely used baseline | - Doubles channel count → more parameters & memory<br>- Computationally heavier<br>- Fusion is unguided; model must discover interactions itself |\n","| **Addition** | - Lightweight (no increase in channels)<br>- Fast and parameter-efficient<br>- Enforces similar feature spaces between modalities | - Assumes features are aligned and comparable<br>- One noisy modality corrupts the other<br>- Sensitive to scale differences between modalities |\n","| **Multiplicative (Hadamard Product)** | - Gating effect: highlights features important in *both* modalities<br>- More expressive than addition, cheaper than concat<br>- Natural for attention-like fusion | - Suppresses features when one modality has low magnitude<br>- Requires careful normalization<br>- Can amplify noise if both activations are high |\n","| **Matrix Multiplication (Bilinear-like)** | - Captures rich pairwise correlations between modalities<br>- Most expressive among all four<br>- Enables true 2nd-order interaction learning | - Very heavy in compute & memory<br>- Requires flattening or dimensionality reduction<br>- Easily overfits; harder to train and tune |\n"],"metadata":{"id":"EdnoTDCgbe8b"}},{"cell_type":"code","source":["## move: model.py\n","\n","class ConcatIntermediateNet(nn.Module):\n","    \"\"\"\n","    Intermediate fusion model using feature concatenation.\n","\n","    Two separate EmbedderMaxPool encoders are applied to RGB and XYZA\n","    inputs, their flattened features are concatenated, and a shared\n","    FullyConnectedHead maps the joint representation to class logits.\n","    \"\"\"\n","    def __init__(self, rgb_ch, xyz_ch, output_dim=2):\n","        super().__init__()\n","\n","        # Independent Encoders\n","        # RGB learns textures/colors\n","        self.rgb_encoder = EmbedderMaxPool(in_ch=4, feature_dim=200)    ## TODO: warum original feature_dim=128\n","        # LiDAR learns geometry/depth\n","        self.xyz_encoder = EmbedderMaxPool(in_ch=4, feature_dim=200)    ## TODO: warum original feature_dim=128\n","\n","        # Calculate combined dimension\n","        # (200 * 8 * 8) + (200 * 8 * 8)\n","        combined_dim = self.rgb_encoder.flatten_dim + self.xyz_encoder.flatten_dim\n","\n","        # Shared FullyConnected Head\n","        self.head = FullyConnectedHead(input_dim=combined_dim, output_dim=output_dim)\n","\n","    def forward(self, x_rgb, x_xyz):\n","        # Extract features independently\n","        x_rgb = self.rgb_encoder(x_rgb)                                 # (B, D)\n","        x_xyz = self.xyz_encoder(x_xyz)                                 # (B, D)\n","\n","        # Fuse (Concatenate) at the feature level\n","        x_fused = torch.cat((x_rgb, x_xyz), dim=1)                      # (B, 2*D)\n","\n","        # Predict\n","        output = self.head(x_fused)\n","\n","        return output"],"metadata":{"id":"OWorX2Vc2mRC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## move: model.py\n","\n","class AddIntermediateNet(nn.Module):\n","    \"\"\"\n","    Intermediate fusion model using element-wise addition.\n","\n","    Two separate encoders process each modality independently.\n","    The resulting feature vectors must have the same size; they are\n","    added element-wise and passed to a shared FullyConnectedHead.\n","    \"\"\"\n","    def __init__(self, rgb_ch, xyz_ch, output_dim):\n","        super().__init__()\n","\n","        # Independent Encoders\n","        # RGB learns textures/colors\n","        self.rgb_encoder = EmbedderMaxPool(in_ch=4, feature_dim=200)    ## TODO: warum original feature_dim=128\n","        # LiDAR learns geometry/depth\n","        self.xyz_encoder = EmbedderMaxPool(in_ch=4, feature_dim=200)    ## TODO: warum original feature_dim=128\n","\n","        # For addition, shapes must match\n","        fused_dim = self.rgb_encoder.flatten_dim                        # same size after addition\n","\n","        # Shared FullyConnected Head\n","        self.head = FullyConnectedHead(input_dim=fused_dim, output_dim=output_dim)\n","\n","    def forward(self, x_rgb, x_xyz):\n","        # Extract features independently\n","        x_rgb = self.rgb_encoder(x_rgb)                                 # (B, D)\n","        x_xyz = self.xyz_encoder(x_xyz)                                 # (B, D)\n","\n","        # Additive fusion in feature space\n","        x_fused = x_rgb + x_xyz                                         # (B, D)\n","\n","        # Predict\n","        output = self.head(x_fused)                                     # (B, output_dim)\n","\n","        return output"],"metadata":{"id":"0ho7ej8I6dv8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## move: model.py\n","\n","class MatmulIntermediateNet(nn.Module):\n","    \"\"\"\n","    Intermediate fusion model using matrix multiplication.\n","\n","    The two modality-specific embeddings are reshaped into matrices\n","    and combined via a bilinear interaction (matrix product) before\n","    the fully-connected head.\n","    \"\"\"\n","    def __init__(self, rgb_ch, xyz_ch, output_dim):\n","        super().__init__()\n","\n","        # Independent Encoders\n","        # RGB learns textures/colors\n","        self.rgb_encoder = EmbedderMaxPool(in_ch=4, feature_dim=200)    ## TODO: warum original feature_dim=128\n","        # LiDAR learns geometry/depth\n","        self.xyz_encoder = EmbedderMaxPool(in_ch=4, feature_dim=200)    ## TODO: warum original feature_dim=128\n","\n","        # For multiplication, shapes must match\n","        embedding_dim = self.rgb_encoder.flatten_dim\n","        fused_dim = embedding_dim * embedding_dim                       # D * D after matmul\n","\n","        # Shared FullyConnected Head\n","        self.head = FullyConnectedHead(input_dim=fused_dim, output_dim=output_dim)\n","\n","    def forward(self, x_rgb, x_xyz):\n","        # Extract features independently\n","        x_rgb = self.rgb_encoder(x_rgb)                                 # (B, D)\n","        x_xyz = self.xyz_encoder(x_xyz)                                 # (B, D)\n","\n","        # Matrix multiplication: (B, D, 1) @ (B, 1, D)\n","        x_fused = torch.matmul(x_rgb.unsqueeze(2), x_xyz.unsqueeze(1))  # (B, D, D)\n","        x_fused = x_fused.flatten(start_dim=1)                          # (B, D*D)\n","\n","        # Predict\n","        output = self.head(x_fused)                                     # (B, output_dim)\n","\n","        return output"],"metadata":{"id":"N2qRIii27b4e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## move: model.py\n","\n","class HadamardIntermediateNet(nn.Module):\n","    \"\"\"\n","    Intermediate fusion model using the Hadamard (element-wise) product.\n","\n","    After independent encoding, the feature vectors are multiplied\n","    element-wise to capture multiplicative interactions between\n","    modalities, then fed to the classification head.\n","    \"\"\"\n","    def __init__(self, rgb_ch, xyz_ch, output_dim):\n","        super().__init__()\n","\n","        # Independent Encoders\n","        # RGB learns textures/colors\n","        self.rgb_encoder = EmbedderMaxPool(in_ch=4, feature_dim=200)    ## TODO: warum original feature_dim=128\n","        # LiDAR learns geometry/depth\n","        self.xyz_encoder = EmbedderMaxPool(in_ch=4, feature_dim=200)    ## TODO: warum original feature_dim=128\n","\n","        # For elementwise multiplication, shapes must match\n","        fused_dim = self.rgb_encoder.flatten_dim                        # same size after addition\n","\n","        # Shared FullyConnected Head\n","        self.head = FullyConnectedHead(input_dim=fused_dim, output_dim=output_dim)\n","\n","    def forward(self, x_rgb, x_xyz):\n","        # Extract features independently\n","        x_rgb = self.rgb_encoder(x_rgb)                                 # (B, D)\n","        x_xyz = self.xyz_encoder(x_xyz)                                 # (B, D)\n","\n","        # Multiplicative / gating-like fusion\n","        x_fused = x_rgb * x_xyz                                         # shape: (B, D)\n","\n","        # Predict\n","        output = self.head(x_fused)                                     # (B, output_dim)\n","\n","        return output"],"metadata":{"id":"nEz80edk7pfW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Late Fusion Model"],"metadata":{"id":"r1tl2t3s_8m9"}},{"cell_type":"markdown","source":["**Concept:** Each modality is processed completely separately, and only the final predictions or high-level embeddings are fused.\n","\n","```\n","RGB → RGB-Embedder → logits_rgb\n","LiDAR → LiDAR-Embedder → logits_lidar\n","\n","Fusion → final decision\n","```"],"metadata":{"id":"ACKIMjiocf8-"}},{"cell_type":"markdown","source":["**Advantages:**\n","\n","* **Robust to Missing Modalities:** The system can still operate if one modality is noisy, unreliable, or absent.\n","* **Best for Heterogeneous Modalities:** Works well when modalities differ greatly.\n","* **Modular & Simple:** Unimodal models can be trained, debugged, and replaced independently.\n","* **Leverages Existing Models:** Allows the reuse of strong off-the-shelf unimodal experts without architectural changes.\n","\n","\n","**Limitations:**\n","\n","* **Missed Interactions:** No joint feature learning — modalities never influence each other during representation learning.\n","* **Limited Expressiveness:** Simple fusion rules (e.g., averaging, weighted sum) cannot capture complex cross-modal relationships.\n","* **Information Loss:** By the time unimodal predictors output logits/embeddings, rich spatial and semantic details may already be discarded, limiting the power of fusion."],"metadata":{"id":"RSADk-61csKM"}},{"cell_type":"code","source":["## move: model.py\n","\n","rgb_net = EmbedderMaxPool(4).to(device)\n","xyz_net = EmbedderMaxPool(4).to(device)\n","\n","## TODO: passiert das woanders nicht?\n","networks = [rgb_net, xyz_net]\n","\n","class LateNet(nn.Module):\n","    \"\"\"\n","    Late fusion model combining unimodal logits.\n","\n","    Two independent classifiers are trained for RGB and LiDAR.\n","    Their logits are then averaged (or combined) at the decision level\n","    to obtain the final prediction.\n","    \"\"\"\n","    def __init__(self, output_dim):\n","        super().__init__()\n","        self.rgb = rgb_net\n","        self.xyz = xyz_net\n","\n","        # each embedder outputs flatten_dim (e.g. 12800)\n","        fusion_dim = self.rgb.flatten_dim * 2  # rgb + xyz\n","\n","        # single FullyConnected head in which data is fused\n","        self.fullyConnected = FullyConnectedHead(\n","            input_dim=fusion_dim,\n","            output_dim=output_dim,\n","        )\n","\n","    def forward(self, x_rgb, x_xyz):\n","        # Extract features independently\n","        x_rgb = self.rgb(x_rgb)     # (B, 12800)\n","        x_xyz = self.xyz(x_xyz)     # (B, 12800)\n","\n","        # this concatenates the features from the two branches\n","        x_fused = torch.cat((x_rgb, x_xyz), dim=1)    # (B, 25600)\n","\n","        # Predict\n","        preds = self.fullyConnected(x_fused)           # (B, output_dim)\n","        return preds"],"metadata":{"id":"YkVKO4UVSwKG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model Training"],"metadata":{"id":"MJLmVUfdGeXB"}},{"cell_type":"code","source":["## move: training.py\n","\n","def train_model(model, optimizer, input_fn, loss_fn, epochs, train_dataloader, val_dataloader, model_save_path, target_idx=-1, log_to_wandb=False, model_name=None):\n","    \"\"\"\n","    Generic training loop for all fusion models.\n","\n","    Args:\n","        model (nn.Module): Model to train.\n","        optimizer (torch.optim.Optimizer): Optimizer instance.\n","        input_fn (callable): Function that maps a batch to model inputs.\n","                             Takes a batch tuple and returns a tuple of tensors.\n","        loss_fn (callable): Loss function (e.g. CrossEntropyLoss).\n","        epochs (int): Number of training epochs.\n","        train_dataloader (DataLoader): Dataloader for training data.\n","        val_dataloader (DataLoader): Dataloader for validation data.\n","        model_save_path (str or Path): Where to save the best model checkpoint.\n","        target_idx (int): If using multi-target labels, index of the target\n","                          to use (-1 for all / default).\n","        log_to_wandb (bool): If True, log metrics to Weights & Biases.\n","        model_name (str or None): Optional label for logging / printing.\n","\n","    Returns:\n","        dict: Dictionary containing training history:\n","              {\n","                \"train_losses\": [...],\n","                \"valid_losses\": [...],\n","                \"epoch_times\": [...],\n","                \"best_valid_loss\": float,\n","                \"best_model_state_dict\": dict,\n","                \"num_params\": int,\n","                \"max_gpu_mem_mb\": float,\n","              }\n","    \"\"\"\n","    train_losses = []\n","    valid_losses = []\n","    epoch_times = []\n","\n","    best_val_loss = float('inf')\n","    best_model = None\n","\n","    # Track peak GPU memory usage (if CUDA is available)\n","    max_gpu_mem_mb = 0.0\n","    use_cuda = torch.cuda.is_available()\n","\n","    if use_cuda:\n","        torch.cuda.reset_peak_memory_stats()\n","\n","    for epoch in tqdm(range(epochs)):\n","        start_time = time.time()                  # to track the train time per model\n","        print(f\"Epoch and start time: {epoch} und {start_time}\")\n","\n","        # ----- Training loop -----\n","        model.train()\n","        train_loss = 0\n","        for step, batch in enumerate(train_dataloader):\n","\n","            rgb, lidar_xyza, position = batch\n","            rgb = rgb.to(device)\n","            lidar_xyza = lidar_xyza.to(device)\n","            position = position.to(device)\n","\n","            optimizer.zero_grad()\n","            target = batch[target_idx].to(device)\n","            outputs = model(*input_fn(batch))\n","\n","            loss = loss_fn(outputs, target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        train_loss = train_loss / (step + 1)\n","        train_losses.append(train_loss)\n","        print_loss(epoch, train_loss, outputs, target, is_train=True)\n","\n","        # ----- Validation loop -----\n","        model.eval()\n","        valid_loss = 0\n","        with torch.no_grad():\n","          for step, batch in enumerate(val_dataloader):\n","              target = batch[target_idx].to(device)\n","              outputs = model(*input_fn(batch))\n","              valid_loss += loss_fn(outputs, target).item()\n","        valid_loss = valid_loss / (step + 1)\n","        valid_losses.append(valid_loss)\n","        print_loss(epoch, valid_loss, outputs, target, is_train=False)\n","\n","        # Save best model based on validation loss\n","        if valid_loss < best_val_loss:\n","          best_val_loss = valid_loss\n","          best_model = model\n","          torch.save(best_model.state_dict(), model_save_path)\n","          print('Found and saved better weights for the model')\n","\n","        # calculate epoch times\n","        epoch_time = time.time() - start_time\n","        epoch_time_formatted = format_time(epoch_time)\n","        epoch_times.append(epoch_time_formatted)\n","\n","        # GPU memory usage\n","        if use_cuda:\n","            gpu_mem_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)\n","            max_gpu_mem_mb = max(max_gpu_mem_mb, gpu_mem_mb)\n","\n","        # wandb logging\n","        if log_to_wandb:\n","            wandb.log(\n","                {\n","                    \"model\": model.__class__.__name__,\n","                    \"epoch\": epoch + 1,\n","                    \"train_loss\": train_loss,\n","                    \"valid_loss\": valid_loss,\n","                    \"lr\": optimizer.param_groups[0][\"lr\"],\n","                    \"epoch_time\": epoch_time_formatted,\n","                    \"max_gpu_mem_mb_epoch\": gpu_mem_mb if use_cuda else 0.0,\n","                }\n","            )\n","\n","    return train_losses, valid_losses, epoch_times, max_gpu_mem_mb"],"metadata":{"id":"UmHRslBJGsCp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## move: training.py\n","\n","def get_early_inputs(batch):\n","    \"\"\"\n","    Prepare inputs for the early fusion model.\n","\n","    Concatenates RGB and XYZA along the channel dimension to obtain\n","    an 8-channel tensor.\n","\n","    Args:\n","        batch (tuple): (rgb, xyz, label) from the dataset.\n","\n","    Returns:\n","        tuple[torch.Tensor]: Single-element tuple (inputs_mm_early,).\n","    \"\"\"\n","    inputs_rgb = batch[0].to(device)\n","    inputs_xyz = batch[1].to(device)\n","\n","    # Concatenate along channel dimension: (B, 4, H, W) + (B, 4, H, W) -> (B, 8, H, W)\n","    inputs_mm_early = torch.cat((inputs_rgb, inputs_xyz), 1)\n","    return (inputs_mm_early,)"],"metadata":{"id":"XEav2OQVSpc-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## move: training.py\n","\n","def get_inputs(batch):\n","    \"\"\"\n","    Prepare inputs for intermediate/late fusion models.\n","\n","    Returns RGB and XYZA tensors separately so that each modality\n","    can be passed to its own encoder.\n","\n","    Args:\n","        batch (tuple): (rgb, xyz, label) from the dataset.\n","\n","    Returns:\n","        tuple[torch.Tensor, torch.Tensor]: (inputs_rgb, inputs_xyz)\n","    \"\"\"\n","    inputs_rgb = batch[0].to(device)\n","    inputs_xyz = batch[1].to(device)\n","    return (inputs_rgb, inputs_xyz)"],"metadata":{"id":"npcH0IrmSy76"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## move: training.py\n","\n","def compute_class_weights(dataset):\n","  \"\"\"\n","  Compute inverse-frequency class weights to handle imbalance.\n","\n","  If train_labels is not provided, hard-coded counts are used\n","  (as in the assignment description).\n","\n","  Args:\n","      train_labels (torch.Tensor or None): Optional 1D tensor of labels\n","          from the training set.\n","\n","  Returns:\n","      torch.Tensor: Normalized class weights of shape (num_classes,).\n","  \"\"\"\n","  # Extract all labels from the dataset\n","  labels = [dataset[i][2] for i in range(len(dataset))]\n","  labels = torch.tensor(labels, dtype=torch.long)\n","\n","  # Count occurrences of each class\n","  unique, counts = torch.unique(labels, return_counts=True)\n","  class_counts = counts.float()\n","\n","  # Compute inverse-frequency weights (rarer class -> higher weight)\n","  class_weights = class_counts.sum() / (class_counts + 1e-6)\n","  class_weights = class_weights / class_weights.mean()\n","\n","  return class_weights"],"metadata":{"id":"Glijei9Pbh8y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## stay\n","\n","set_seeds(SEED)\n","\n","class_weights = compute_class_weights(train_data).to(device)\n","loss_func = nn.CrossEntropyLoss(weight=class_weights.to(device))\n","\n","metrics = {}   # store losses for each model\n","\n","# Defines fusion models to train and compare\n","models_to_train = {\n","    \"early_fusion\": EarlyFusionModel(in_ch=8, output_dim=NUM_CLASSES).to(device),\n","    #\"intermediate_fusion_concat\": ConcatIntermediateNet(4, 4, output_dim=NUM_CLASSES).to(device),\n","    #\"intermediate_fusion_matmul\": MatmulIntermediateNet(4, 4, output_dim=NUM_CLASSES).to(device),\n","    #\"intermediate_fusion_hadamard\": HadamardIntermediateNet(4, 4, output_dim=NUM_CLASSES).to(device),\n","    #\"intermediate_fusion_add\": AddIntermediateNet(4, 4, output_dim=NUM_CLASSES).to(device),\n","    #\"late_fusion\": LateNet(output_dim=NUM_CLASSES).to(device),\n","}\n","\n","# Directory where best model is saved\n","checkpoint_dir = STORAGE_PATH / \"checkpoints\"\n","checkpoint_dir.mkdir(exist_ok=True)\n","\n","\n","# === Main experiment loop over all fusion strategies ===\n","for name, model in models_to_train.items():\n","  model_save_path = checkpoint_dir / f\"{name}.pth\"\n","\n","  # Number of trainable parameters (for the comparison table)\n","  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","  opt = Adam(model.parameters(), lr=LR)\n","\n","  # Initialize a new Weights & Biases run for this model.\n","  init_wandb(\n","      model=model,\n","      fusion_name=name,\n","      num_params=num_params,\n","      opt_name = opt.__class__.__name__)\n","\n","  # Choose the proper input function depending on the fusion strategy:\n","  if name == \"early_fusion\":\n","    input_fn = get_early_inputs\n","  else:\n","    input_fn = get_inputs\n","\n","  train_losses, valid_losses, epoch_times, max_gpu_mem_mb = train_model(\n","    model=model,\n","    optimizer=opt,\n","    input_fn=input_fn,\n","    epochs=EPOCHS,\n","    loss_fn=loss_func,\n","    train_dataloader=train_dataloader,\n","    val_dataloader=val_dataloader,\n","    model_save_path=model_save_path,\n","    target_idx=-1,   # last element in batch is target\n","    log_to_wandb=True,\n","    model_name=name\n","  )\n","\n","  metrics[name] = {\n","      \"train_losses\": train_losses,\n","      \"valid_losses\": valid_losses,\n","      \"epoch_times\": epoch_times,\n","      \"best_valid_loss\": min(valid_losses),\n","      \"max_gpu_mem_mb\": max_gpu_mem_mb,\n","      \"num_params\": num_params,\n","  }\n","\n","  # End wandb run before starting the next model\n","  wandb.finish()"],"metadata":{"id":"_tt2AIIOA6Jj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Evaluation"],"metadata":{"id":"xux2ArSO_Nnm"}},{"cell_type":"code","source":["## move: visualization.py\n","\n","def plot_losses(loss_dict, title=\"Validation Loss per Model\", ylabel=\"Loss\", xlabel=\"Epoch\"):\n","    \"\"\"\n","    Plot validation loss curves for multiple models.\n","\n","    Args:\n","        loss_dict (dict): Mapping \"model_name\" -> list_of_losses (same length).\n","        title (str): Plot title.\n","        ylabel (str): Label for y-axis.\n","        xlabel (str): Label for x-axis.\n","    \"\"\"\n","    plt.figure(figsize=(8,5))\n","\n","    # Auto-generate x-axis based on first model\n","    any_key = next(iter(loss_dict))\n","    epochs = range(len(loss_dict[any_key]))\n","\n","    for model_name, losses in loss_dict.items():\n","        plt.plot(epochs, losses, label=model_name)\n","\n","    plt.xlabel(xlabel)\n","    plt.ylabel(ylabel)\n","    plt.title(title)\n","    plt.legend()\n","    plt.grid(True, linestyle=\"--\", alpha=0.3)\n","    plt.show()"],"metadata":{"id":"l9P8dcwrDP7y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# compute avg_epoch_time\n","avg_epoch_time = sum(epoch_times) / len(epoch_times)"],"metadata":{"id":"5O6OQ7SEbSN9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def build_fusion_comparison_df(metrics, name_map=None):\n","    rows = []\n","    for key, m in metrics.items():\n","        avg_train_loss = float(np.mean(m[\"train_losses\"]))\n","        avg_valid_loss = float(np.mean(m[\"valid_losses\"]))\n","        avg_epoch_time = float(np.mean(m[\"epoch_times\"]))\n","        rows.append({\n","            \"Fusion Strategy\": name_map.get(key, key) if name_map else key,\n","            \"Avg Valid Loss\": avg_valid_loss,\n","            \"Best Valid Loss\": float(m[\"best_valid_loss\"]),\n","            \"Num of params\": int(m[\"num_params\"]),\n","            \"Avg time per epoch (min:s)\": avg_epoch_time,\n","            \"GPU Memory (MB, max)\": float(m[\"max_gpu_mem_mb\"]),\n","        })\n","    return pd.DataFrame(rows)"],"metadata":{"id":"eQkndCwi72Df"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_dict = {name: m[\"valid_losses\"] for name, m in metrics.items()}\n","plot_losses(loss_dict, title=\"Validation Loss per Model\")\n","\n","df_comparison = build_fusion_comparison_df(metrics, name_map)\n","df_comparison"],"metadata":{"id":"RSfWbBea6u4x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# logs the comparison table to wandb\n","wandb.init(\n","    project=\"cilp-extended-assessment\",   # your project name\n","    name=\"fusion_comparison_all\",\n","    job_type=\"analysis\",\n",")\n","\n","fusion_comparison_table = wandb.Table(dataframe=df_comparison)\n","wandb.log({\"fusion_comparison\": fusion_comparison_table})\n","\n","wandb.finish()"],"metadata":{"id":"aL_anEO28AnC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**When to use**\n","\n","**Early Fusion:**\n","* Aligned, closely related low-level modalities and comparable features\n","* Simple setup; avoid if sensors are noisy\n","\n","**Intermediate Fusion:**\n","* Modalities with different structure that benefit from separate early processing in order to learn modality-specific features   \n","* best overall balance of performance and flexibility\n","\n","**Late Fusion:**\n","* Strong, independent unimodal predictors, to combine their strengths\n","* ideal for heterogeneous or missing modalities\n","* robust fallback when one modality fails"],"metadata":{"id":"Ad-R30zkczAj"}}]}