{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["E8vIz6Sx0RFX"],"authorship_tag":"ABX9TyNQdWYNrheT1snG0lqcpAKm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"w79TQtD755-G"}},{"cell_type":"code","source":["%%capture\n","%pip install wandb weave"],"metadata":{"id":"dodCKTTU7C_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","%pip install fiftyone==1.10.0 sympy==1.12 torch torchvision numpy open-clip-torch"],"metadata":{"id":"KtirMD1x7EU8"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ukkECgFX5HQT"},"outputs":[],"source":["import os\n","from pathlib import Path\n","from google.colab import userdata\n","import time\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","import wandb"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"-5rX_aoY7M_g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEED = 51\n","NUM_WORKERS = os.cpu_count()  # Number of CPU cores\n","BATCH_SIZE = 64\n","IMG_SIZE = 64\n","EPOCHS = 15\n","LR = 0.0001"],"metadata":{"id":"PTMc18ZD5_lr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Integrate Wandb"],"metadata":{"id":"oq3nlu7A7Qr4"}},{"cell_type":"code","source":["# Load W&B API key from .env file and make it available as env variable\n","from dotenv import load_dotenv\n","load_dotenv()  # loads .env automatically\n","\n","os.environ[\"WANDB_API_KEY\"]"],"metadata":{"id":"PCMnAmeJ7S72"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load W&B API key from Colab Secrets and make it available as env variable\n","wandb_key = userdata.get('WANDB_API_KEY')\n","os.environ[\"WANDB_API_KEY\"] = wandb_key"],"metadata":{"id":"LQPywS8q7UrH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.login()"],"metadata":{"id":"ZHH81Y7M7XRW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = {\n","        \"embedding_size\": 128,\n","        \"optimizer_type\": \"AdamW\",\n","        \"fusion_strategy\": \"late\",\n","        \"model_architecture\": \"CILP_LateFusion\",\n","        \"lr\": LR, ## why every epoch as hyperparameter?\n","        \"batch_size\": BATCH_SIZE,\n","        \"num_epochs\": EPOCHS,\n","        # Number of parameters\n","    }"],"metadata":{"id":"AnXrBqMx7Y46"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["metrics = {\n","        \"epoch\": 1,\n","        \"train_loss\": 0.5,\n","        \"val_loss\": 0.5,\n","    }"],"metadata":{"id":"8hrlEEv17aoK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wandb.init(\n","    project=\"clip-extended-assessment\",\n","    config=config\n",")"],"metadata":{"id":"lKvdYO2Q7b-N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Reproducibility"],"metadata":{"id":"E8vIz6Sx0RFX"}},{"cell_type":"code","source":["def set_seeds(seed=SEED):\n","    \"\"\"\n","    Set seeds for complete reproducibility across all libraries and operations.\n","\n","    Args:\n","        seed (int): Random seed value\n","    \"\"\"\n","    # Set environment variables before other imports\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    os.environ['CUBLAS_WORKSPACE_CONFIG'] = ':4096:8'\n","\n","    # Python random module\n","    random.seed(seed)\n","\n","    # NumPy\n","    np.random.seed(seed)\n","\n","    # PyTorch CPU\n","    torch.manual_seed(seed)\n","\n","    # PyTorch GPU (all devices)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed(seed)\n","        torch.cuda.manual_seed_all(seed)  # For multi-GPU setups\n","\n","        # CUDA deterministic operations\n","        torch.backends.cudnn.deterministic = True\n","        torch.backends.cudnn.benchmark = False\n","\n","    # OpenCV\n","    cv2.setRNGSeed(seed)\n","\n","    # Albumentations (for data augmentation)\n","    try:\n","        A.seed_everything(seed)\n","    except AttributeError:\n","        # Older versions of albumentations\n","        pass\n","\n","    # PyTorch deterministic algorithms (may impact performance)\n","    try:\n","        torch.use_deterministic_algorithms(True)\n","    except RuntimeError:\n","        # Some operations don't have deterministic implementations\n","        print(\"Warning: Some operations may not be deterministic\")\n","\n","    print(f\"All random seeds set to {seed} for reproducibility\")\n","\n","\n","\n","# Usage: Call this function at the beginning and before each training phase\n","set_seeds(SEED)\n","\n","# Additional reproducibility considerations:\n","\n","def create_deterministic_training_dataloader(dataset, batch_size, shuffle=True, **kwargs):\n","    \"\"\"\n","    Create a DataLoader with deterministic behavior.\n","\n","    Args:\n","        dataset: PyTorch Dataset instance\n","        batch_size: Batch size\n","        shuffle: Whether to shuffle data\n","        **kwargs: Additional DataLoader arguments\n","\n","    Returns:\n","        Training DataLoader with reproducible behavior\n","    \"\"\"\n","    # Use a generator with fixed seed for reproducible shuffling\n","    generator = torch.Generator()\n","    generator.manual_seed(51)\n","\n","    return torch.utils.data.DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        generator=generator if shuffle else None,\n","        **kwargs\n","    )\n","\n"],"metadata":{"id":"lccenR_P0TMW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Utility Functions"],"metadata":{"id":"ayZXNwxXGqUm"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","torch.cuda.is_available()"],"metadata":{"id":"9bGm3AQAGbBP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_func = nn.MSELoss()\n","\n","def train_model(model, optimizer, input_fn, epochs, loss_fn, train_dataloader, valid_dataloader, target_idx=-1, log_to_wandb=False, model_name=None):\n","    train_losses = []\n","    valid_losses = []\n","    epoch_times = []\n","\n","    # for GPU memory tracking\n","    max_gpu_mem_mb = 0.0\n","    use_cuda = torch.cuda.is_available()\n","\n","    if use_cuda:\n","        torch.cuda.reset_peak_memory_stats()\n","\n","    for epoch in range(epochs):\n","        start_time = time.time()                  # to track the train time per model\n","        model.train()\n","        train_loss = 0\n","\n","        for step, batch in enumerate(train_dataloader):\n","            optimizer.zero_grad()\n","            target = batch[target_idx].to(device)  # labels: 0/1 for cube/sphere\n","            outputs = model(*input_fn(batch))      # e.g. model(rgb, lidar)\n","\n","            loss = loss_fn(outputs, target)\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","\n","        train_loss = train_loss / (step + 1)\n","        train_losses.append(train_loss)\n","        print_loss(epoch, train_loss, outputs, target, is_train=True)\n","\n","\n","        # ----- validation -----\n","        model.eval()\n","        valid_loss = 0\n","        with torch.no_grad():\n","          for step, batch in enumerate(valid_dataloader):\n","              target = batch[target_idx].to(device)\n","              outputs = model(*input_fn(batch))\n","              valid_loss += loss_func(outputs, target).item()\n","        valid_loss = valid_loss / (step + 1)\n","        valid_losses.append(valid_loss)\n","        print_loss(epoch, valid_loss, outputs, target, is_train=False)\n","\n","        # timing\n","        epoch_time = time.time() - start_time\n","        epoch_times.append(epoch_time)\n","\n","        # GPU memory\n","        if use_cuda:\n","            gpu_mem_mb = torch.cuda.max_memory_allocated() / (1024 ** 2)\n","            max_gpu_mem_mb = max(max_gpu_mem_mb, gpu_mem_mb)\n","\n","        # console logging (reuse your print_loss if you want)\n","        print(\n","            f\"[{model_name or 'model'}] Epoch {epoch+1}/{epochs} \"\n","            f\"- train_loss: {train_loss:.4f}  valid_loss: {valid_loss:.4f}  \"\n","            f\"time: {epoch_time:.2f}s\"\n","        )\n","\n","        # wandb logging\n","        if log_to_wandb:\n","            wandb.log(\n","                {\n","                    \"model\": model_name or \"model\",\n","                    \"epoch\": epoch + 1,\n","                    \"train_loss\": train_loss,\n","                    \"valid_loss\": valid_loss,\n","                    \"epoch_time_sec\": epoch_time,\n","                    \"max_gpu_mem_mb_epoch\": gpu_mem_mb if use_cuda else 0.0,\n","                }\n","            )\n","\n","    return train_losses, valid_losses, epoch_times, max_gpu_mem_mb"],"metadata":{"id":"UmHRslBJGsCp","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1764273343886,"user_tz":-60,"elapsed":22,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"66d543da-ed44-40d0-cf7c-a3c15f579516"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'nn' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3470232630.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_to_wandb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mvalid_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"]}]},{"cell_type":"code","source":["def print_loss(epoch, loss, outputs, target, is_train=True, is_debug=False):\n","    loss_type = \"train loss:\" if is_train else \"valid loss:\"\n","    print(\"epoch\", str(epoch), loss_type, str(loss))\n","    if is_debug:\n","        print(\"example pred:\", format_positions(outputs[0].tolist()))\n","        print(\"example real:\", format_positions(target[0].tolist()))"],"metadata":{"id":"BOW8rdnSG1Ni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_losses(losses, title=\"Training & Validation Loss Comparison\", figsize=(10,6)):\n","    plt.figure(figsize=figsize)\n","\n","    for model_name, log in losses.items():\n","        train = log[\"train_losses\"]\n","        valid = log[\"valid_losses\"]\n","\n","        # plot train + valid with different line styles\n","        plt.plot(train, label=f\"{model_name} - train\", linewidth=2)\n","        plt.plot(valid, label=f\"{model_name} - valid\", linestyle=\"--\", linewidth=2)\n","\n","    plt.title(title, fontsize=16)\n","    plt.xlabel(\"Epochs\", fontsize=14)\n","    plt.ylabel(\"Loss\", fontsize=14)\n","    plt.grid(True, alpha=0.3)\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.show()"],"metadata":{"id":"OWej9tHY4Wjg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["img_transforms = transforms.Compose([\n","    transforms.Resize(IMG_SIZE),\n","    transforms.ToTensor(),  # Scales data into [0,1]\n","])"],"metadata":{"id":"zs0ywaXqHFbh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TODO: Load and prepare Data - or import it from Notebook 01"],"metadata":{"id":"Zf2LJnXb795e"}},{"cell_type":"markdown","source":["# Create Models\n","\n","Take the Net architecture from the workshop and turn it into an encoder that outputs an embedding instead of 9 positions."],"metadata":{"id":"tF_MGrCQ6MDd"}},{"cell_type":"code","source":["class ConvEncoder(nn.Module):\n","    \"\"\"\n","    Small CNN that turns a 4-channel image into a compact embedding vector.\n","\n","    Why:\n","    - We want to compare RGB+LiDAR representations using similarity.\n","    - For that, we need each modality mapped to a fixed-size vector (embedding).\n","    - This encoder is the \"feature extractor\" for one modality.\n","    \"\"\"\n","\n","    def __init__(self, in_ch: int, emb_dim: int = 128):\n","        \"\"\"\n","        Args:\n","            in_ch: number of input channels (4 for RGB, 4 for LiDAR)\n","            emb_dim: dimensionality of the output embedding (e.g. 128)\n","        \"\"\"\n","        super().__init__()\n","        k = 3      # kernel size\n","\n","        # Three conv layers with pooling\n","        self.conv1 = nn.Conv2d(in_ch, 50, k, padding=1)\n","        self.conv2 = nn.Conv2d(50, 100, k, padding=1)\n","        self.conv3 = nn.Conv2d(100, 200, k, padding=1)\n","        self.pool = nn.MaxPool2d(2)\n","\n","        # After 3x pooling (factor 2 each time) on 64x64 → 8x8 feature maps\n","        # channels: 200, so flattened size = 200 * 8 * 8\n","        self.fc1 = nn.Linear(200 * 8 * 8, 1000)\n","        self.fc2 = nn.Linear(1000, emb_dim)\n","\n","    def forward(self, x):\n","        # Convolution + nonlinearity + downsampling\n","        x = self.pool(F.relu(self.conv1(x)))   # (B,50,32,32)\n","        x = self.pool(F.relu(self.conv2(x)))   # (B,100,16,16)\n","        x = self.pool(F.relu(self.conv3(x)))   # (B,200,8,8)\n","\n","        # Flatten spatial dimensions\n","        x = torch.flatten(x, 1)                # (B,200*8*8)\n","\n","        # Two fully connected layers to get to embedding dimension\n","        x = F.relu(self.fc1(x))                # (B,1000)\n","        x = self.fc2(x)                        # (B,emb_dim)\n","\n","        # Normalize for cosine similarity\n","        x = F.normalize(x, dim=-1)\n","        return x\n"],"metadata":{"id":"nL0fCuzT6Pce"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["### TODO aufpassen: bei Antonio zusätzlich vorher ConvEnc trainiert und gradienten eingefroern - chatty macht das hier nicht??\n","### aber auch im nächsten lab trainiert antonio nicht vor um besser vergleichen zu können mit early fusion\n","\n","class LateFusionModel(nn.Module):\n","    \"\"\"\n","    Late fusion:\n","    - RGB and LiDAR are encoded completely separately.\n","    - Only at the very end we concatenate their embeddings and optionally project.\n","\n","    This matches the idea:\n","        \"Separate encoders → final embeddings → fusion → similarity\"\n","    \"\"\"\n","\n","    def __init__(self, emb_dim=128, hidden_dim=256, out_dim):\n","        \"\"\"\n","        Args:\n","            emb_dim: size of each individual modality embedding\n","            fused_dim: size of the final fused embedding\n","        \"\"\"\n","        super().__init__()\n","\n","        # Separate encoders for each modality\n","        self.rgb_enc = ConvEncoder(in_ch=4, emb_dim=emb_dim)      ## TODO: mit Antonio abgleichen\n","        self.lidar_enc = ConvEncoder(in_ch=4, emb_dim=emb_dim)    ## TODO: mit Antonio abgleichen\n","\n","        # Linear layer to mix and reduce concatenated embeddings\n","        # Input is [rgb_emb, lidar_emb] of size 2 * emb_dim\n","        self.fusion_fc1 = nn.Linear(2 * emb_dim, hidden_dim)\n","        self.fusion_fc2 = nn.Linear(hidden_dim, out_dim)\n","\n","    def forward(self, rgb, lidar):\n","        \"\"\"\n","        Args:\n","            rgb:   (B, 4, 64, 64)\n","            lidar: (B, 4, 64, 64)\n","\n","        Returns:\n","            fused_emb: (B, fused_dim)  # used for similarity / contrastive loss\n","            rgb_emb:   (B, emb_dim)    # optional, for analysis\n","            lidar_emb: (B, emb_dim)    # optional, for analysis\n","        \"\"\"\n","        # 1) Encode each modality with its own ConvEncoder\n","        rgb_emb = self.rgb_enc(rgb)           # (B, emb_dim)\n","        lidar_emb = self.lidar_enc(lidar)     # (B, emb_dim)\n","\n","        # 2) Late fusion: concatenate embeddings\n","        x = torch.cat((rgb_emb, lidar_emb), dim=1)  # (B, 2*emb_dim)\n","\n","        # 3) Optional projection to a joint fused space   ## TODO auch hier abweichung zu antonio, hat relu + das zweite linear\n","        x = F.relu(self.fusion_fc1(x))                 # (B, fused_dim)\n","        out = self.fusion_fc2(x)                       # (B, out_dim)\n","\n","        return out\n"],"metadata":{"id":"IpOKOUFP-GSX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class IntermediateFusionModel(nn.Module):\n","    \"\"\"\n","    Intermediate fusion:\n","    - Each modality has its own early conv layers (conv1, conv2).\n","    - Their feature maps are then concatenated.\n","    - Shared later layers (conv3 + FCs) operate on the fused feature maps.\n","\n","    This lets RGB and LiDAR interact earlier and at a more local spatial level.\n","    \"\"\"\n","\n","    def __init__(self, emb_dim=128, hidden_dim=256, out_dim):    ## TODO Antonio gibt channels als parameter rein\n","        super().__init__()\n","        k = 3\n","        # this downsampling can be done with convolutions of stride 2\n","        self.pool = nn.MaxPool2d(2)\n","\n","        # --- Modality-specific early convolutions ---\n","\n","        # RGB branch: takes 4-channel input and produces feature maps\n","        self.rgb_conv1 = nn.Conv2d(4, 50, k, padding=1)\n","        self.rgb_conv2 = nn.Conv2d(50, 100, k, padding=1)     ## TODO Antonio hat ein Conv2d more, das erste auch nur 4,25?\n","\n","        # LiDAR branch: same structure but separate weights\n","        self.lidar_conv1 = nn.Conv2d(4, 50, k, padding=1)\n","        self.lidar_conv2 = nn.Conv2d(50, 100, k, padding=1)\n","\n","        # --- Shared later layers ---        ## TODO baut das hier komplett anders auf als Antonio, außerdem hat Matmul, also nicht concatenate sondern multiplication den besten val_loss\n","\n","        # After conv2+pool in each branch:\n","        # RGB feature maps:   (B, 100, 16, 16)\n","        # LiDAR feature maps: (B, 100, 16, 16)\n","        # Concatenated:       (B, 200, 16, 16)\n","        self.shared_conv3 = nn.Conv2d(200, 200, k, padding=1)\n","\n","        # After another pool: (B, 200, 8, 8)\n","        self.fc1 = nn.Linear(200 * 8 * 8, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, out_dim)\n","\n","    def forward(self, rgb, lidar):\n","        \"\"\"\n","        Args:\n","            rgb:   (B, 4, 64, 64)\n","            lidar: (B, 4, 64, 64)\n","\n","        Returns:\n","            emb: (B, emb_dim)  # fused embedding for similarity / contrastive loss\n","        \"\"\"\n","        # --- RGB early branch ---\n","        x_rgb = self.pool(F.relu(self.rgb_conv1(rgb)))     # (B, 50, 32, 32)\n","        x_rgb = self.pool(F.relu(self.rgb_conv2(x_rgb)))   # (B, 100, 16, 16)\n","\n","        # --- LiDAR early branch ---\n","        x_lid = self.pool(F.relu(self.lidar_conv1(lidar))) # (B, 50, 32, 32)\n","        x_lid = self.pool(F.relu(self.lidar_conv2(x_lid))) # (B, 100, 16, 16)\n","\n","        # --- Intermediate fusion on feature maps ---\n","        # Concatenate along channel dimension\n","        x = torch.cat([x_rgb, x_lid], dim=1)               # (B, 200, 16, 16)\n","\n","        # Shared conv and pooling\n","        x = self.pool(F.relu(self.shared_conv3(x)))        # (B, 200, 8, 8)\n","\n","        # Flatten and project to embedding\n","        x = torch.flatten(x, 1)                            # (B, 200*8*8)\n","        x = F.relu(self.fc1(x))                            # (B, 1000)\n","        out = self.fc2(x)                                  # (B, out_dim)\n","\n","        return out\n"],"metadata":{"id":"5RLrALQ4AW3C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fusion"],"metadata":{"id":"MJLmVUfdGeXB"}},{"cell_type":"code","source":["def get_rgb_lidar_inputs(batch):\n","    \"\"\"\n","    batch: something like (rgb, lidar, label_idx)\n","    \"\"\"\n","    rgb = batch[0].to(device)\n","    lidar = batch[1].to(device)\n","    return (rgb, lidar)"],"metadata":{"id":"JnGKVhRm1_Ka"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataloader = create_deterministic_training_dataloader(\n","  #torch_train_set,             ## TODO\n","  batch_size=BATCH_SIZE,\n","  shuffle=True,\n","  num_workers=NUM_WORKERS,\n","  pin_memory=True\n",")\n","\n","valid_dataloader = torch.utils.data.DataLoader(\n","  #torch_val_set,                 ## TODO\n","  batch_size=BATCH_SIZE,\n","  shuffle=False, # No need to shuffle validation data\n","  num_workers=num_workers,\n","  pin_memory=True\n",")"],"metadata":{"id":"MmvenSlP1gwT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss_func = nn.CrossEntropyLoss()\n","\n","metrics = {}   # store losses for each model\n","\n","models_to_train = {\n","    \"late_fusion\": LateFusionClassifier().to(device),\n","    \"intermediate_fusion\": IntermediateFusionClassifier().to(device)\n","}\n","\n","for name, model in models_to_train.items():\n","  net = Net(8).to(device)\n","  opt = Adam(net.parameters(), lr=LR)\n","\n","  train_losses, valid_losses, epoch_times, max_gpu_mem_mb = train_model(\n","    model=net,\n","    optimizer=opt,\n","    input_fn=get_rgb_lidar_inputs,     ## TODO\n","    epochs=EPOCHS,\n","    loss_fn=loss_func,\n","    train_dataloader=train_dataloader,\n","    valid_dataloader=valid_dataloader,\n","    target_idx=-1,   # last element in batch is target\n","    log_to_wandb=True,\n","    model_name=name.replace(\" \", \"_\").lower(),\n","  )\n","\n","  # metrics for comparison table\n","  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","  metrics[name] = {\n","      \"train_losses\": train_losses,\n","      \"valid_losses\": valid_losses,\n","      \"epoch_times\": epoch_times,\n","      \"best_valid_loss\": min(valid_losses),\n","      \"max_gpu_mem_mb\": max_gpu_mem_mb,\n","      \"num_params\": num_params,\n","  }\n"],"metadata":{"id":"fJ9NnNfhGhN_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_losses(losses)"],"metadata":{"id":"RSfWbBea6u4x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["row_late = results[\"Late Fusion\"]\n","row_inter = results[\"Intermediate Fusion\"]\n","\n","comparison_df = pd.DataFrame(\n","    {\n","        \"Metric\": [\n","            \"Validation Loss (best)\",\n","            \"Parameters (count)\",\n","            \"Training Time (s/epoch, avg)\",\n","            \"GPU Memory (MB, peak)\",\n","        ],\n","        \"Late Fusion\": [\n","            row_late[\"best_valid_loss\"],\n","            row_late[\"num_params\"],\n","            row_late[\"avg_epoch_time\"],\n","            row_late[\"max_gpu_mem_mb\"],\n","        ],\n","        \"Intermediate Fusion\": [\n","            row_inter[\"best_valid_loss\"],\n","            row_inter[\"num_params\"],\n","            row_inter[\"avg_epoch_time\"],\n","            row_inter[\"max_gpu_mem_mb\"],\n","        ],\n","    }\n",")\n","\n","comparison_df"],"metadata":{"id":"eQkndCwi72Df"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# logs the comparison table to wandb\n","table = wandb.Table(dataframe=comparison_df)\n","wandb.log({\"fusion_comparison\": table})"],"metadata":{"id":"aL_anEO28AnC"},"execution_count":null,"outputs":[]}]}