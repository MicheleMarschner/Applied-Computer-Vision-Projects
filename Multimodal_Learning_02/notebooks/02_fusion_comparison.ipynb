{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWxsDGcq0xVI"
      },
      "source": [
        "# Overview: Fusion Architecture Comparison\n",
        "\n",
        "This notebook implements and evaluates several multimodal fusion strategies for RGB + LiDAR classification (cube vs. sphere).\n",
        "We compare early fusion, intermediate fusion (with multiple variants), and late fusion to understand their trade-offs in parameter count, performance, and training behavior.\n",
        "\n",
        "The goal of this notebook is to:\n",
        "* Build modality-specific or shared encoders\n",
        "* Implement different fusion strategies\n",
        "* Train models using identical settings\n",
        "* Log results with Weights & Biases\n",
        "* Produce a comparison table and loss curves\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cFYDTuJt5AF"
      },
      "source": [
        "**The Architecture Flow:**\n",
        "\n",
        "```\n",
        "RGB Input (4ch)       LiDAR Input (4ch)\n",
        "      │                     │\n",
        "[RGB Encoder]         [XYZ Encoder]    <-- Learn specific features independently\n",
        "      │                     │\n",
        "  RGB Features          XYZ Features   <-- (e.g. 128 channels each)\n",
        "      └──────────┬──────────┘\n",
        "                 │\n",
        "           Concatenation               <-- Fuse at the \"Feature Level\"\n",
        "                 │\n",
        "         [Regression Head]             <-- Learn relationships between features\n",
        "                 │\n",
        "           Output (x,y,z)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w79TQtD755-G"
      },
      "source": [
        "# Initial Setup\n",
        "\n",
        "The project repository is mounted from Google Drive and added to the Python path to allow clean imports from the src module. The dataset is copied to the local Colab filesystem to improve I/O performance during training. All global settings (random seed, device selection, paths, batch sizes) are defined once and reused across the notebook to ensure consistency and reproducibility.\n",
        "\n",
        "Weights & Biases is initialized for experiment tracking, and all training stages use the same precomputed dataset statistics and DataLoaders for fair comparison across models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\"\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%%capture\n",
        "%pip install --no-cache-dir -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ukkECgFX5HQT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from google.colab import userdata\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from torch.optim import Adam\n",
        "\n",
        "import wandb\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!rm -rf /content/data\n",
        "!cp -r \"$DRIVE_ROOT/data/assessment\" /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w73asd8TRGP"
      },
      "outputs": [],
      "source": [
        "from src.config import (SEED, NUM_WORKERS, BATCH_SIZE, IMG_SIZE, DRIVE_ROOT,\n",
        "                        NUM_CLASSES, RAW_DATA, CHECKPOINTS, DEVICE, VALID_BATCHES)\n",
        "from src.utility import set_seeds, init_wandb, compute_embedding_size\n",
        "from src.datasets import get_dataloaders, get_train_stats\n",
        "from src.training import get_early_inputs, get_inputs, train_model\n",
        "from src.visualization import build_fusion_comparison_df, plot_val_losses\n",
        "from src.models import EarlyFusionModel, ConcatIntermediateNet, AddIntermediateNet, MatmulIntermediateNet, HadamardIntermediateNet, LateNet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTMc18ZD5_lr"
      },
      "outputs": [],
      "source": [
        "# Fusion specific constants\n",
        "EPOCHS = 15\n",
        "LR = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "79pbllrpTRGX"
      },
      "outputs": [],
      "source": [
        "# Usage: Call this function at the beginning and before each training phase\n",
        "set_seeds(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQPywS8q7UrH",
        "outputId": "da2ff919-f787-48cb-b42b-31b8ccda0d4e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichele-marschner\u001b[0m (\u001b[33mmichele-marschner-university-of-potsdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load W&B API key from Colab Secrets and make it available as env variable\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zf2LJnXb795e"
      },
      "source": [
        "# Loading and preparation of Data\n",
        "\n",
        "This section computes normalization statistics, defines input transforms, and constructs training, validation, and test dataloaders. It ensures that all fusion models receive consistent and correctly processed input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9jcaSSpS3kd",
        "outputId": "36ee50bd-afce-4e19-974f-153931cc2f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scanning dataset in /content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data...\n",
            "cubes: 2501 RGB files found. Matching XYZA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "spheres: 9999 RGB files found. Matching XYZA...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preloading LiDAR XYZA tensors into RAM...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading XYZA:   0%|          | 38/12500 [00:28<2:35:57,  1.33it/s]"
          ]
        }
      ],
      "source": [
        "# gets calculated mean, std from file or calculates it from the rgb train data\n",
        "# for different dataset (or change in train data) recalculate mean and standard deviation\n",
        "mean, std = get_train_stats(dir=DRIVE_ROOT, img_size=IMG_SIZE, data_dir=RAW_DATA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OErXfds6EC8O"
      },
      "outputs": [],
      "source": [
        "img_transforms = transforms.Compose([\n",
        "    transforms.ToImage(),   # Scales data into [0,1]\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize(mean, std)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hiXY_6-CUYOs"
      },
      "outputs": [],
      "source": [
        "set_seeds(SEED)\n",
        "\n",
        "train_data, train_dataloader, valid_data, val_dataloader, test_data, test_dataloader = get_dataloaders(\n",
        "    str(RAW_DATA),\n",
        "    VALID_BATCHES,\n",
        "    test_frac=0.10,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    img_transforms=img_transforms,\n",
        "    num_workers=NUM_WORKERS,\n",
        "    seed=SEED\n",
        ")\n",
        "\n",
        "for i, sample in enumerate(train_data):\n",
        "    print(i, *(x.shape for x in sample))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF_MGrCQ6MDd"
      },
      "source": [
        "# Models\n",
        "\n",
        "This section introduces the different multimodal fusion strategies implemented in the experiment. It explains the conceptual differences between early fusion, intermediate fusion (with several variants), and late fusion, setting the stage for the comparative evaluation.\n",
        "\n",
        "The detailed use cases, advantages, and limitations of each fusion strategy are described in the sections below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1ST72Ch_1ev"
      },
      "source": [
        "## Early Fusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Bt4ppI4Zuv3"
      },
      "source": [
        "**Concept:** Modalities are fused before any deep processing — usually by concatenating channels or inputs.\n",
        "\n",
        "```\n",
        "[RGB , LiDAR/XYZ]  ──concat──>  x ∈ ℝ^{(C_rgb+C_lidar)×H×W}\n",
        "                                   │\n",
        "                                   ▼\n",
        "                          Shared CNN backbone\n",
        "                                   │\n",
        "                                   ▼\n",
        "                               Output\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_G-ymxeaDtf"
      },
      "source": [
        "**Advantages:**\n",
        "\n",
        "* **Captures Early Cross-Modal Interactions:** Learns joint low-level correlations directly from raw signals.\n",
        "* **Simple & Lightweight**: Easiest fusion method to implement; minimal architectural overhead.\n",
        "* **Effective with Perfect Alignment:** Works well when modalities are tightly synchronized and spatially aligned.\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "* **Noise Sensitivity:** One noisy or corrupted modality directly contaminates the shared feature space.\n",
        "* **Strict Alignment Requirement:** Modalities must have matching spatial resolution, alignment, and synchronization.\n",
        "* **Feature Space Mismatch:** Raw modalities differ in scale, units, and distribution; one modality can dominate without careful normalization.\n",
        "* **High Input Dimensionality:** Channel concatenation increases the input size and can require more data and compute to train effectively.\n",
        "* **Limited Flexibility:** Assumes combining low-level signals is beneficial; may underperform when modalities carry different types of information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QgOUeNP_44r"
      },
      "source": [
        "## Intermediate Fusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt0sT9SoasEG"
      },
      "source": [
        "**Concept:** Each modality has its own encoder / feature extractor, and fusion happens after some layers but before classification.\n",
        "\n",
        "```\n",
        "e.g. Addition:\n",
        "\n",
        "RGB image ──> RGB CNN Encoder ──> f_rgb ∈ ℝ^{C×H'×W'}\n",
        "                                     |\n",
        "LiDAR/XYZ ─> LiDAR CNN Encoder ─> f_lidar ∈ ℝ^{C×H'×W'}\n",
        "                                     |\n",
        "                                     v\n",
        "                           Fuse: f = f_rgb + f_lidar (element-wise add)\n",
        "                                     |\n",
        "                                     v\n",
        "                          Shared head / classifier / projector\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiBvpcPQa2pb"
      },
      "source": [
        "**Advantages:**\n",
        "\n",
        "* **Specialized Processing:** Each modality gets its own encoder, tailored to its characteristics.\n",
        "* **Learned Representations:** Fusion occurs on higher-level, more discriminative features rather than raw data.\n",
        "* **Flexible Design:** The fusion point can be chosen at different network depths, allowing fine-grained architectural control.\n",
        "* **Easily Extendable:** New modalities can be added by including additional modality-specific branches.\n",
        "\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "* **Architectural Complexity:** Requires designing separate modality-specific encoders and choosing an appropriate fusion point.\n",
        "* **Higher Computational Cost:** More expensive than early fusion due to duplicated feature extractors.\n",
        "* **Fusion Design Sensitivity:** Performance depends on the chosen fusion mechanism (concat, addition, multiplicative, bilinear, attention), which often requires experimentation.\n",
        "* **Depth Selection Challenge:** Deciding how much unimodal processing to perform before fusion can be non-trivial and task-dependent."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "899VHVwubGAE"
      },
      "source": [
        "Implemented 4 variants:\n",
        "\n",
        "*   Concatenation\n",
        "*   Addition\n",
        "* Hadamard Product (element-wise multiplication)\n",
        "* Matrix-Multiplication\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdnoTDCgbe8b"
      },
      "source": [
        "| Fusion Method | Advantages | Limitations |\n",
        "|---------------|------------|-------------|\n",
        "| **Concatenation** | - Very expressive and flexible<br>- Lets the network learn arbitrary cross-modal interactions<br>- Robust and widely used baseline | - Doubles channel count → more parameters & memory<br>- Computationally heavier<br>- Fusion is unguided; model must discover interactions itself |\n",
        "| **Addition** | - Lightweight (no increase in channels)<br>- Fast and parameter-efficient<br>- Enforces similar feature spaces between modalities | - Assumes features are aligned and comparable<br>- One noisy modality corrupts the other<br>- Sensitive to scale differences between modalities |\n",
        "| **Multiplicative (Hadamard Product)** | - Gating effect: highlights features important in *both* modalities<br>- More expressive than addition, cheaper than concat<br>- Natural for attention-like fusion | - Suppresses features when one modality has low magnitude<br>- Requires careful normalization<br>- Can amplify noise if both activations are high |\n",
        "| **Matrix Multiplication (Bilinear-like)** | - Captures rich pairwise correlations between modalities<br>- Most expressive among all four<br>- Enables true 2nd-order interaction learning | - Very heavy in compute & memory<br>- Requires flattening or dimensionality reduction<br>- Easily overfits; harder to train and tune |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1tl2t3s_8m9"
      },
      "source": [
        "## Late Fusion Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACKIMjiocf8-"
      },
      "source": [
        "**Concept:** Each modality is processed completely separately, and only the final predictions or high-level embeddings are fused.\n",
        "\n",
        "```\n",
        "RGB (C_rgb×H×W)  -> RGB Encoder   -> z_rgb\n",
        "                                  \\\n",
        "                                   -> Fuse at decision level -> output\n",
        "                                  /\n",
        "LiDAR/XYZ (C_l×H×W) -> LiDAR Encoder -> z_lidar\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RSADk-61csKM"
      },
      "source": [
        "**Advantages:**\n",
        "\n",
        "* **Robust to Missing Modalities:** The system can still operate if one modality is noisy, unreliable, or absent.\n",
        "* **Best for Heterogeneous Modalities:** Works well when modalities differ greatly.\n",
        "* **Modular & Simple:** Unimodal models can be trained, debugged, and replaced independently.\n",
        "* **Leverages Existing Models:** Allows the reuse of strong off-the-shelf unimodal experts without architectural changes.\n",
        "\n",
        "\n",
        "**Limitations:**\n",
        "\n",
        "* **Missed Interactions:** No joint feature learning — modalities never influence each other during representation learning.\n",
        "* **Limited Expressiveness:** Simple fusion rules (e.g., averaging, weighted sum) cannot capture complex cross-modal relationships.\n",
        "* **Information Loss:** By the time unimodal predictors output logits/embeddings, rich spatial and semantic details may already be discarded, limiting the power of fusion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJLmVUfdGeXB"
      },
      "source": [
        "# Model Training\n",
        "\n",
        "Here we train each fusion model using the same dataset, optimizer, and loss function. The training loop logs metrics to W&B, saves checkpoints, and stores results for later comparison. This is the core experimental section of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tt2AIIOA6Jj"
      },
      "outputs": [],
      "source": [
        "FEATURE_DIM = 128\n",
        "\n",
        "set_seeds(SEED)\n",
        "\n",
        "#class_weights = compute_class_weights(train_data, NUM_CLASSES).to(DEVICE)\n",
        "#loss_func = nn.CrossEntropyLoss(weight=class_weights.to(DEVICE))\n",
        "loss_func = nn.CrossEntropyLoss()\n",
        "\n",
        "metrics = {}   # store losses for each model\n",
        "\n",
        "# Define fusion models to train and compare\n",
        "models_to_train = {\n",
        "    \"early_fusion\": EarlyFusionModel(in_ch=8, output_dim=NUM_CLASSES).to(DEVICE),\n",
        "    \"intermediate_fusion_concat\": ConcatIntermediateNet(4, 4, output_dim=NUM_CLASSES, feature_dim=FEATURE_DIM).to(DEVICE),\n",
        "    \"intermediate_fusion_matmul\": MatmulIntermediateNet(4, 4, output_dim=NUM_CLASSES, feature_dim=FEATURE_DIM).to(DEVICE),\n",
        "    \"intermediate_fusion_hadamard\": HadamardIntermediateNet(4, 4, output_dim=NUM_CLASSES, feature_dim=FEATURE_DIM).to(DEVICE),\n",
        "    \"intermediate_fusion_add\": AddIntermediateNet(4, 4, output_dim=NUM_CLASSES, feature_dim=FEATURE_DIM).to(DEVICE),\n",
        "    \"late_fusion\": LateNet(4, 4, output_dim=NUM_CLASSES).to(DEVICE),\n",
        "}\n",
        "\n",
        "# === Main experiment loop over all fusion strategies ===\n",
        "for name, model in models_to_train.items():\n",
        "  model_save_path = CHECKPOINTS / f\"{name}.pt\"\n",
        "\n",
        "  # Number of trainable parameters (for the comparison table)\n",
        "  num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "  opt = Adam(model.parameters(), lr=LR)\n",
        "\n",
        "  embedding_size = compute_embedding_size(name, FEATURE_DIM, spatial=(8, 8))\n",
        "\n",
        "  # Initialize a new Weights & Biases run for this model.\n",
        "  init_wandb(\n",
        "      model=model,\n",
        "      name=name,\n",
        "      embedding_size=embedding_size,\n",
        "      fusion_name=name,\n",
        "      num_params=num_params,\n",
        "      opt_name = opt.__class__.__name__\n",
        "    )\n",
        "\n",
        "  # Choose the proper input function depending on the fusion strategy:\n",
        "  if name.startswith(\"early_fusion\"):\n",
        "    input_fn = get_early_inputs\n",
        "  else:\n",
        "    input_fn = get_inputs\n",
        "\n",
        "  results = train_model(\n",
        "    model=model,\n",
        "    optimizer=opt,\n",
        "    input_fn=input_fn,\n",
        "    epochs=EPOCHS,\n",
        "    loss_fn=loss_func,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    model_save_path=model_save_path,\n",
        "    target_idx=-1,   # last element in batch is target\n",
        "    log_to_wandb=True,\n",
        "    device=DEVICE\n",
        "  )\n",
        "\n",
        "  metrics[name] = results\n",
        "\n",
        "  # End wandb run before starting the next model\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xux2ArSO_Nnm"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "This section visualizes validation losses across all fusion strategies, builds the fusion comparison table, and logs the aggregated results to W&B.\n",
        "\n",
        "It provides the final quantitative comparison between early, intermediate, and late fusion methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx9KFu3pTRGn"
      },
      "outputs": [],
      "source": [
        "name_map = {\n",
        "    \"early_fusion\": \"Early Fusion\",\n",
        "    \"late_fusion\": \"Late Fusion\",\n",
        "    \"intermediate_fusion_concat\": \"Intermediate (Concat)\",\n",
        "    \"intermediate_fusion_matmul\": \"Intermediate (Multiplicative)\",\n",
        "    \"intermediate_fusion_hadamard\": \"Intermediate (Hadamard)\",\n",
        "    \"intermediate_fusion_add\": \"Intermediate (Add)\",\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZAhiYi7TRGz"
      },
      "outputs": [],
      "source": [
        "# Build comparison overview\n",
        "df_comparison = build_fusion_comparison_df(metrics, name_map)\n",
        "\n",
        "# Log the comparison table to wandb\n",
        "wandb.init(\n",
        "    project=\"cilp-extended-assessment\", \n",
        "    name=\"fusion_comparison_all\",\n",
        "    job_type=\"analysis\",\n",
        ")\n",
        "\n",
        "# Log comparison table and loss curves to wandb\n",
        "fusion_comparison_table = wandb.Table(dataframe=df_comparison)\n",
        "wandb.log({\"fusion_comparison\": fusion_comparison_table})\n",
        "\n",
        "loss_dict = {name: m[\"valid_losses\"] for name, m in metrics.items()}\n",
        "fig, ax = plot_val_losses(loss_dict, title=\"Validation Loss per Model\")\n",
        "plt.show()\n",
        "\n",
        "wandb.log({\"fusion/val_loss_curves\": wandb.Image(fig)})\n",
        "plt.close(fig)\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_comparison"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ad-R30zkczAj"
      },
      "source": [
        "## Evaluation of Fusion Strategies\n",
        "\n",
        "|index|Fusion Strategy|Avg Valid Loss|Best Valid Loss|Num of params|Avg time per epoch \\(min:s\\)|GPU Memory \\(MB, max\\)|\n",
        "|---|---|---|---|---|---|---|\n",
        "|0|Early Fusion|0\\.0047|1\\.2874e-06|8387990|10\\.0998|497\\.7852|\n",
        "|1|Intermediate \\(Concat\\)|0\\.0057|5\\.0663e-07|16672374|15\\.8507|672\\.7407|\n",
        "|2|Intermediate \\(Multiplicative\\)|0\\.0069|1\\.3186e-06|8480374|13\\.6595|643\\.0933|\n",
        "|3|Intermediate \\(Hadamard\\)|0\\.0023|1\\.4230e-06|8480374|13\\.2025|675\\.4458|\n",
        "|4|Intermediate \\(Add\\)|0\\.0029|1\\.0505e-07|8480374|13\\.1532|707\\.7983|\n",
        "|5|Late Fusion|0\\.0070|1\\.5497e-07|16672374|15\\.6503|833\\.4009|\n",
        "\n",
        "The evaluation of different fusion strategies indicates that intermediate fusion approaches provide the most favorable balance between performance and computational efficiency, while early and late fusion exhibit clear trade-offs. Among all tested models, Intermediate Fusion (Add) achieves the lowest and most stable best validation loss (1.05 × 10⁻⁷), indicating highly effective alignment between RGB and LiDAR representations. At the same time, Intermediate Fusion (Hadamard) yields the lowest average validation loss (0.0023), suggesting particularly stable convergence behavior across training epochs.\n",
        "\n",
        "Both additive and Hadamard intermediate fusion operate with a moderate parameter count of approximately 8.48 million, making them substantially more efficient than concatenation-based or late fusion strategies. In contrast, Intermediate Fusion (Concat) and Late Fusion roughly double the number of parameters (~16.7M) and require noticeably more GPU memory and longer training times per epoch, without delivering corresponding improvements in validation performance. This highlights that increased model capacity alone does not guarantee better cross-modal alignment.\n",
        "\n",
        "Early Fusion benefits from the lowest computational footprint, achieving the shortest average epoch time and lowest GPU memory usage. However, it converges more slowly and shows a higher average validation loss, likely because the model must learn cross-modal relationships directly from low-level features, which can be challenging when modalities differ significantly in structure or noise characteristics.\n",
        "\n",
        "Overall, taking previous experimental runs into account, Intermediate Fusion (Add) emerges as the most effective architecture. It combines excellent validation performance, stable convergence, and efficient parameter usage, making it the most robust and practical choice in this setting.\n",
        "\n",
        "**When to Use Each Fusion Strategy:**\n",
        "*Early Fusion:*\n",
        "Best suited for closely aligned, low-level modalities with similar structure and noise characteristics. While computationally efficient, it is less robust when inputs are noisy or heterogeneous.\n",
        "\n",
        "*Intermediate Fusion:*\n",
        "Ideal for modalities with different structures that benefit from separate early processing to learn modality-specific features. It provides the best overall balance between performance, flexibility, and efficiency.\n",
        "\n",
        "*Late Fusion:*\n",
        "Most appropriate when strong unimodal predictors already exist or when robustness to missing or unreliable modalities is required. It serves as a reliable fallback but comes at a higher computational cost."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
