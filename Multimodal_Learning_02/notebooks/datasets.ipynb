{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPegMDOgJFz4vSVyknQy7dW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from pathlib import Path\n","import numpy as np\n","import torch\n","from PIL import Image\n","from tqdm import tqdm"],"metadata":{"id":"1qADO5m4vG4c","executionInfo":{"status":"ok","timestamp":1764792270689,"user_tz":-60,"elapsed":7315,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["CLASSES = [\"cubes\", \"spheres\"]\n","LABEL_MAP = {\"cubes\": 0, \"spheres\": 1}"],"metadata":{"id":"71HvJx-p0dI6","executionInfo":{"status":"ok","timestamp":1764792326332,"user_tz":-60,"elapsed":6,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XcbFggPyuTJr","executionInfo":{"status":"ok","timestamp":1764792355500,"user_tz":-60,"elapsed":28248,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"0b4b8502-5af8-497f-cb58-6e55c38866bb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","STORAGE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\")"]},{"cell_type":"code","source":["DATA_SRC_PATH = STORAGE_PATH / \"multimodal_training_workshop/data\"\n","DATA_DEST_PATH = STORAGE_PATH / \"data\"\n","print(f\"Data path: {DATA_SRC_PATH}\")\n","print(f\"Data path exists: {DATA_SRC_PATH.exists()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_2rhZwvMvKIl","executionInfo":{"status":"ok","timestamp":1764792358059,"user_tz":-60,"elapsed":2561,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"8e61288d-d551-44f2-8b0f-11ff0dcb4108"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Data path: /content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/multimodal_training_workshop/data\n","Data path exists: True\n"]}]},{"cell_type":"code","source":["def get_torch_xyza(lidar_depth, azimuth, zenith):\n","    x = lidar_depth * torch.sin(-azimuth[:, None]) * torch.cos(-zenith[None, :])\n","    y = lidar_depth * torch.cos(-azimuth[:, None]) * torch.cos(-zenith[None, :])\n","    z = lidar_depth * torch.sin(-zenith[None, :])\n","    a = torch.where(lidar_depth < 50.0, torch.ones_like(lidar_depth), torch.zeros_like(lidar_depth))\n","    xyza = torch.stack((x, y, z, a))\n","    return xyza"],"metadata":{"id":"X0tZHSolvVuU","executionInfo":{"status":"ok","timestamp":1764792358074,"user_tz":-60,"elapsed":1,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from pathlib import Path\n","import numpy as np\n","import torch\n","from PIL import Image\n","from tqdm import tqdm\n","\n","def precompute_assessment_xyza(\n","    src_root,\n","    dst_root,\n","    classes,\n","    label_map,\n","):\n","    \"\"\"\n","    Convert assessment depth LiDAR (H,W) to XYZA (4,H,W) and save to dst_root.\n","    \"\"\"\n","    src_root = Path(src_root)\n","    dst_root = Path(dst_root)\n","    dst_root.mkdir(parents=True, exist_ok=True)\n","\n","    all_samples = []\n","\n","    for class_name in classes:\n","        src_class_dir = src_root / class_name\n","        rgb_dir   = src_class_dir / \"rgb\"\n","        lidar_dir = src_class_dir / \"lidar\"\n","\n","        # azimuth / zenith per class\n","        azimuth_path = src_class_dir / \"azimuth.npy\"\n","        zenith_path  = src_class_dir / \"zenith.npy\"\n","        if not azimuth_path.exists() or not zenith_path.exists():\n","            raise FileNotFoundError(f\"Missing azimuth/zenith in {src_class_dir}\")\n","\n","        azimuth = torch.from_numpy(np.load(azimuth_path)).to(torch.float32)\n","        zenith  = torch.from_numpy(np.load(zenith_path)).to(torch.float32)\n","\n","        # destination dirs\n","        dst_class_dir = dst_root / class_name\n","        dst_rgb_dir   = dst_class_dir / \"rgb\"\n","        dst_lidar_dir = dst_class_dir / \"lidar_xyza\"\n","\n","        dst_rgb_dir.mkdir(parents=True, exist_ok=True)\n","        dst_lidar_dir.mkdir(parents=True, exist_ok=True)\n","\n","        rgb_files   = sorted(rgb_dir.glob(\"*.png\"))\n","        lidar_files = sorted(lidar_dir.glob(\"*.npy\"))\n","\n","        rgb_stems   = {f.stem for f in rgb_files}\n","        lidar_stems = {f.stem for f in lidar_files}\n","        matching    = sorted(rgb_stems & lidar_stems)\n","\n","        print(f\"{class_name}: {len(matching)} paired samples\")\n","\n","        for stem in tqdm(matching, desc=f\"Converting {class_name}\"):\n","            # --- RGB: copy to dst_root (or keep src, your choice) ---\n","            rgb_src = rgb_dir / f\"{stem}.png\"\n","            rgb_dst = dst_rgb_dir / f\"{stem}.png\"\n","            if not rgb_dst.exists():\n","                Image.open(rgb_src).save(rgb_dst)\n","\n","            # --- depth → XYZA ---\n","            depth_src = lidar_dir / f\"{stem}.npy\"\n","            depth_np  = np.load(depth_src)              # (H, W)\n","            depth_t   = torch.from_numpy(depth_np).to(torch.float32)\n","\n","            xyza = get_torch_xyza(depth_t, azimuth, zenith)  # (4, H, W)\n","\n","            lidar_dst = dst_lidar_dir / f\"{stem}.npy\"\n","            np.save(lidar_dst, xyza.numpy())\n","\n","            all_samples.append(\n","                {\n","                    \"class\": class_name,\n","                    \"rgb\": rgb_dst,\n","                    \"lidar_xyza\": lidar_dst,\n","                    \"label\": label_map[class_name],\n","                }\n","            )\n","\n","    print(f\"Total converted samples: {len(all_samples)}\")\n","    return all_samples\n"],"metadata":{"id":"ywFymdb7vWM4","executionInfo":{"status":"ok","timestamp":1764792688613,"user_tz":-60,"elapsed":13,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["SRC = DATA_SRC_PATH / \"assessment\"\n","DST = DATA_DEST_PATH\n","\n","all_samples_xyza = precompute_assessment_xyza(src_root=SRC,\n","    dst_root=DST,\n","    classes=CLASSES,\n","    label_map=LABEL_MAP,)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CZSz0viBv0JI","executionInfo":{"status":"ok","timestamp":1764798774011,"user_tz":-60,"elapsed":6082509,"user":{"displayName":"Michele Marschner","userId":"00906875642059722595"}},"outputId":"4d137733-3af0-44ab-da01-ac21cfd9475c"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["cubes: 2501 paired samples\n"]},{"output_type":"stream","name":"stderr","text":["Converting cubes: 100%|██████████| 2501/2501 [20:09<00:00,  2.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["spheres: 9999 paired samples\n"]},{"output_type":"stream","name":"stderr","text":["Converting spheres: 100%|██████████| 9999/9999 [1:18:52<00:00,  2.11it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Total converted samples: 12500\n"]}]},{"cell_type":"code","source":["print(len(all_samples_xyza))"],"metadata":{"id":"p1AU5CD1y-iT"},"execution_count":null,"outputs":[]}]}