{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f7d9a6e7",
      "metadata": {
        "id": "f7d9a6e7"
      },
      "source": [
        "# Initial Setup\n",
        "\n",
        "The project repository is mounted from Google Drive and added to the Python path to allow clean imports from the src module. The dataset is copied to the local Colab filesystem to improve I/O performance during training. All global settings (random seed, device selection, paths, batch sizes) are defined once and reused across the notebook to ensure consistency and reproducibility.\n",
        "\n",
        "Weights & Biases is initialized for experiment tracking, and all training stages use the same precomputed dataset statistics and DataLoaders for fair comparison across models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b59de61",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\"\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d132903",
      "metadata": {
        "id": "3d132903"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%%capture\n",
        "%pip install --no-cache-dir -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2525061",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e2525061"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "#%pip install fiftyone==1.10.0 sympy==1.12 torch torchvision numpy open-clip-torch open3d"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "047016e7",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "047016e7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "import wandb\n",
        "import fiftyone as fo\n",
        "from PIL import Image\n",
        "\n",
        "import numpy as np\n",
        "import open3d as o3d\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms.v2 as transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51876d5b",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "51876d5b"
      },
      "outputs": [],
      "source": [
        "from src.config import (SEED, IMG_SIZE, CLASSES, TMP_TRANSFORMED_DATA_PATH, DRIVE_ROOT,\n",
        "                    RAW_DATA, DRIVE_TRANSFORMED_DATA_PATH, TMP_TRANSFORMED_DATA_PATH)\n",
        "from src.utility import set_seeds, prepare_lidar_pointclouds\n",
        "from src.datasets import find_matching_files, compute_dataset_mean_std\n",
        "from src.visualization import build_grouped_dataset, plot_class_distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3df28cad",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "3df28cad"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/data\n",
        "!cp -r \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data/assessment\" /content/data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e48ecef",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Specific constants for Dataset Visualzation\n",
        "FIFTYONE_DATASET_NAME = \"cilp_assessment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c22c45",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "b4c22c45"
      },
      "outputs": [],
      "source": [
        "# Usage: Call this function at the beginning and before each training phase\n",
        "set_seeds(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a28c8920",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "a28c8920"
      },
      "outputs": [],
      "source": [
        "# Load W&B API key from Colab Secrets and make it available as env variable\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6e735b6",
      "metadata": {
        "id": "d6e735b6"
      },
      "source": [
        "# Loading and preparation of Data\n",
        "\n",
        "The RGB–LiDAR dataset is loaded and preprocessed by normalizing RGB images and converting LiDAR depth data into aligned point cloud representations suitable for multimodal learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1LycsfEV82TV",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "1LycsfEV82TV"
      },
      "outputs": [],
      "source": [
        "pcd_status = prepare_lidar_pointclouds(\n",
        "    raw_dataset_dir=RAW_DATA,\n",
        "    local_pointcloud_dir=TMP_TRANSFORMED_DATA_PATH,\n",
        "    cache_dir=DRIVE_TRANSFORMED_DATA_PATH,\n",
        "    converter_script=DRIVE_ROOT / \"scripts\" / \"convert_lidar_to_pcd.py\",\n",
        "    classes=CLASSES,\n",
        ")\n",
        "\n",
        "print(\"LiDAR point cloud preparation:\", pcd_status)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2194cda2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculates mean and standard deviation of the rgb train data\n",
        "# for different dataset (or change in train data) recalculate mean and standard deviation\n",
        "# mean, std = compute_dataset_mean_std(root_dir=RAW_DATA, img_size=IMG_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0b42219",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "e0b42219"
      },
      "outputs": [],
      "source": [
        "img_transforms = transforms.Compose([\n",
        "    transforms.ToImage(),   # Scales data into [0,1]\n",
        "    transforms.Resize(IMG_SIZE),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize(([0.0051, 0.0052, 0.0051, 1.0000]), ([5.8023e-02, 5.8933e-02, 5.8108e-02, 2.4509e-07]))    \n",
        "    # transforms.Normalize(mean.tolist(), std.tolist())     # uncomment for different dataset (or change in train data)\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2547c21f",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2547c21f"
      },
      "outputs": [],
      "source": [
        "pairs = find_matching_files(\n",
        "    CLASSES,\n",
        "    rgb_root=RAW_DATA,\n",
        "    lidar_root=TMP_TRANSFORMED_DATA_PATH,\n",
        "    rgb_subdir=\"rgb\",\n",
        "    lidar_subdir=\"pcd\",   \n",
        "    rgb_ext=\"png\",\n",
        "    lidar_ext=\"pcd\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62bfd49d",
      "metadata": {
        "id": "62bfd49d"
      },
      "source": [
        "# Create FiftyOne Grouped Dataset\n",
        "A grouped dataset is created to explicitly link each RGB image with its corresponding LiDAR point cloud. This enables joint visualization and inspection of both modalities within a single sample, facilitating qualitative analysis of multimodal alignment and data quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3987fe1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "dataset = build_grouped_dataset(\n",
        "    name=FIFTYONE_DATASET_NAME,\n",
        "    pairs=pairs,\n",
        "    persistent=True,\n",
        "    overwrite=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2228c8a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "session = fo.launch_app(dataset, auto=False)\n",
        "\n",
        "print(f\"✅ Created dataset '{dataset.name}' with {len(dataset)} samples\")\n",
        "print(\"Group field:\", dataset.group_field)\n",
        "print(\"Group slices:\", dataset.group_slices)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd6ef4f5",
      "metadata": {},
      "source": [
        "# Visual Exploration - Evaluation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5KPit8XV4C13",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5KPit8XV4C13"
      },
      "outputs": [],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d3fc18a",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8d3fc18a"
      },
      "outputs": [],
      "source": [
        "total_per_class = {cls: len(items) for cls, items in pairs.items()}\n",
        "total_samples = sum(total_per_class.values())\n",
        "\n",
        "print(\"Total samples per class:\")\n",
        "for cls, n in total_per_class.items():\n",
        "    print(f\"  {cls}: {n}\")\n",
        "print(f\"\\nTotal samples: {total_samples}\")\n",
        "\n",
        "# picks the first class and first sample from pairs\n",
        "any_class = CLASSES[0]\n",
        "sample = pairs[any_class][0]\n",
        "\n",
        "sample_rgb_path = sample[\"rgb\"]\n",
        "sample_pcd_path = sample[\"lidar\"]\n",
        "\n",
        "# RGB image\n",
        "rgb_img = Image.open(sample_rgb_path)\n",
        "print(\"RGB image:\")\n",
        "print(\"  size (width, height):\", rgb_img.size)\n",
        "print(\"  mode:\", rgb_img.mode)\n",
        "print(\"  format:\", rgb_img.format)\n",
        "\n",
        "pcd = o3d.io.read_point_cloud(str(sample_pcd_path))\n",
        "lidar = np.asarray(pcd.points)\n",
        "color = np.asarray(pcd.colors)\n",
        "\n",
        "print(\"\\nLiDAR depth map:\")\n",
        "print(\"  shape:\", lidar.shape)\n",
        "print(\"  dtype:\", lidar.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hMfilxeWBzyU",
      "metadata": {
        "id": "hMfilxeWBzyU"
      },
      "source": [
        "## Observations about the dataset\n",
        "\n",
        "**Observation 1 — Clear shape signal in LiDAR despite low RGB resolution**\n",
        "\n",
        "What we see:\n",
        "* RGB image: very low resolution, blurry, little texture\n",
        "* LiDAR: sphere shape is clearly recognizable (smooth, rounded point distribution)\n",
        "\n",
        "Interpretation: Although the RGB images are low-resolution and provide limited texture information, the LiDAR modality captures the geometric structure of the objects very clearly. In particular, spherical objects form smooth, rounded point clouds, making shape information more salient in LiDAR than in RGB.\n",
        "\n",
        "----------\n",
        "\n",
        "**Observation 2 — Complementarity of modalities**\n",
        "\n",
        "What we see:\n",
        "* RGB alone: hard to distinguish shape confidently\n",
        "* LiDAR alone: shape (sphere vs cube) is obvious\n",
        "\n",
        "Interpretation: The two modalities provide complementary information: while RGB captures appearance cues, LiDAR provides strong geometric cues. This complementarity motivates the use of multimodal contrastive learning, as each modality compensates for weaknesses in the other.\n",
        "\n",
        "-------------\n",
        "\n",
        "**Observation 3 — Sparse but structured LiDAR point clouds**\n",
        "\n",
        "What we see:\n",
        "* LiDAR point cloud is not dense\n",
        "* Still forms a coherent spherical structure\n",
        "\n",
        "Interpretation: While the LiDAR point clouds are relatively sparse, they retain sufficient structural information to represent object shape. However, variability in point density across samples could introduce noise during training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nHr09PxjBY0w",
      "metadata": {
        "id": "nHr09PxjBY0w"
      },
      "source": [
        "## Data quality issues and observed patterns:\n",
        "\n",
        "The dataset exhibits a uniform background and consistent object centering in RGB images, introducing potential bias. RGB images are low-resolution, while LiDAR point clouds provide clearer geometric cues but vary in sparsity. Overall, RGB and LiDAR modalities are well aligned and consistently paired, indicating good dataset completeness."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a5ead5b",
      "metadata": {},
      "source": [
        "# Creating test and validation set\n",
        "The dataset is split into training, validation, and test subsets using a fixed random seed. This ensures reproducible evaluation and consistent comparisons across all models and experiments. In addition, the class distribution is analyzed to verify dataset balance and identify potential class imbalance issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce7dd1df",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ce7dd1df"
      },
      "outputs": [],
      "source": [
        "set_seeds(SEED)\n",
        "train_ratio = 0.8\n",
        "\n",
        "splits = {\n",
        "    \"train\": {},\n",
        "    \"val\": {},\n",
        "}\n",
        "\n",
        "for cls, items in pairs.items():\n",
        "    n = len(items)\n",
        "    n_train = int(n * train_ratio)\n",
        "\n",
        "    splits[\"train\"][cls] = items[:n_train]\n",
        "    splits[\"val\"][cls] = items[n_train:]\n",
        "\n",
        "train_size = sum(len(v) for v in splits[\"train\"].values())\n",
        "val_size = sum(len(v) for v in splits[\"val\"].values())\n",
        "\n",
        "print(\"Train/validation sizes:\")\n",
        "for cls in CLASSES:\n",
        "    print(\n",
        "        f\"  {cls}: train={len(splits['train'][cls])}, \"\n",
        "        f\"val={len(splits['val'][cls])}\"\n",
        "    )\n",
        "print(f\"\\nTotal train: {train_size}\")\n",
        "print(f\"Total val:   {val_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10d695d9",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "10d695d9"
      },
      "outputs": [],
      "source": [
        "fig, axes = plot_class_distributions(\n",
        "    total_per_class=total_per_class,\n",
        "    splits=splits\n",
        ")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "encoding": "# -*- coding: utf-8 -*-",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "aCV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
