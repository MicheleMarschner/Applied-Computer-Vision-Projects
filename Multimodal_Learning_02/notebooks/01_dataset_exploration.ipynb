{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7d9a6e7",
   "metadata": {},
   "source": [
    "# Setup\n",
    "This section installs required dependencies, mounts Google Drive, defines data paths, and sets global settings such as the random seed and device. It prepares the environment so that all subsequent loading, training, and evaluation steps run consistently and reproducibly, both in Colab and locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d132903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2525061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "%%capture\n",
    "%pip install fiftyone==1.10.0 sympy==1.12 torch torchvision numpy open-clip-torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26852bd",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Here we import all libraries and modules needed for the final assessment: PyTorch, torchvision transforms, W&B, dataset utilities, training utilities, and the model classes used across all three stages. Centralizing imports keeps the notebook organized and ensures that each component is available when needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047016e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from google.colab import userdata\n",
    "\n",
    "import wandb\n",
    "import fiftyone as fo\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6c6071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "STORAGE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\")\n",
    "TMP_STORAGE_PATH = \"/content\"\n",
    "\n",
    "DATA_PATH = STORAGE_PATH / \"data/assessment\"\n",
    "# DATA_PATH = TMP_STORAGE_PATH / \"data/assessment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df28cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp -r \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/data\" /content/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64203451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commented out IPython magic to ensure Python compatibility.\n",
    "%cd \"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Multimodal_Learning_02/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51876d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utility import set_seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bddac27",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "We define key configuration values such as the seed, batch size, image size, number of workers, and label mappings. These constants ensure consistent behavior across all stages and make the hyperparameters easy to adjust or reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08afb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 64\n",
    "SEED = 51\n",
    "\n",
    "FIFTYONE_DATASET_NAME = \"cilp_assessment\"\n",
    "CLASSES = [\"cubes\", \"spheres\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dedc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c22c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage: Call this function at the beginning and before each training phase\n",
    "set_seeds(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0342043",
   "metadata": {},
   "source": [
    "# Integration of Wandb\n",
    "\n",
    "This section authenticates with Weights & Biases using the API key stored in Colab Secrets. Initializing W&B enables automatic logging of losses, metrics, hyperparameters, and summary statistics for all training stages. This satisfies the experiment-tracking requirement of the assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c8920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load W&B API key from Colab Secrets and make it available as env variable\n",
    "wandb_key = userdata.get('WANDB_API_KEY')\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c202021e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def log_similarity_matrix(sim_matrix, title=\"Similarity Matrix\"):\n",
    "    # sim_matrix: (N, N) tensor or ndarray\n",
    "    sim = sim_matrix.detach().cpu().numpy() if hasattr(sim_matrix, \"detach\") else np.array(sim_matrix)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(sim, aspect=\"auto\")\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    wandb.log({\"similarity_matrix\": wandb.Image(fig)})\n",
    "    plt.close(fig)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1f2ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def log_sample_predictions(images, true_labels, pred_labels, max_samples=5):\n",
    "    samples = []\n",
    "    for img, t, p in list(zip(images, true_labels, pred_labels))[:max_samples]:\n",
    "        # assuming img is a tensor [C,H,W] in 0–1 range\n",
    "        img_np = img.detach().cpu().numpy()\n",
    "        caption = f\"true: {t}, pred: {p}\"\n",
    "        samples.append(wandb.Image(img_np, caption=caption))\n",
    "\n",
    "    wandb.log({\"sample_predictions\": samples})\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e735b6",
   "metadata": {},
   "source": [
    "# Loading and preparation of Data\n",
    "\n",
    "TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b42219",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "## Final: dynamisch\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.ToImage(),   # Scales data into [0,1]\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(([0.0051, 0.0052, 0.0051, 1.0000]), ([5.8023e-02, 5.8933e-02, 5.8108e-02, 2.4509e-07]))     ## assessment dataset\n",
    "    # transforms.Normalize(mean.tolist(), std.tolist())     ## assessment dataset\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e639c9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, root_dir, start_idx, stop_idx):\n",
    "        self.classes = [\"cubes\", \"spheres\"]\n",
    "        self.root_dir = root_dir\n",
    "        self.rgb = []\n",
    "        self.lidar = []\n",
    "        self.class_idxs = []\n",
    "\n",
    "        for class_idx, class_name in enumerate(self.classes):\n",
    "            for idx in range(start_idx, stop_idx):\n",
    "                file_number = \"{:04d}\".format(idx)\n",
    "                rbg_img = Image.open(self.root_dir + class_name + \"/rgb/\" + file_number + \".png\")\n",
    "                rbg_img = img_transforms(rbg_img).to(device)\n",
    "                self.rgb.append(rbg_img)\n",
    "\n",
    "                lidar_depth = np.load(self.root_dir + class_name + \"/lidar/\" + file_number + \".npy\")\n",
    "                lidar_depth = torch.from_numpy(lidar_depth[None, :, :]).to(torch.float32).to(device)\n",
    "                self.lidar.append(lidar_depth)\n",
    "\n",
    "                self.class_idxs.append(torch.tensor(class_idx, dtype=torch.float32)[None].to(device))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.class_idxs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        rbg_img = self.rgb[idx]\n",
    "        lidar_depth = self.lidar[idx]\n",
    "        class_idx = self.class_idxs[idx]\n",
    "        return rbg_img, lidar_depth, class_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f6f1ed",
   "metadata": {},
   "source": [
    "# Verify Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2547c21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = {}\n",
    "\n",
    "# Paths to different modalities, organized by class\n",
    "for class_name in CLASSES:\n",
    "    class_dir = DATA_PATH / class_name\n",
    "    RGB_DIR = class_dir / \"rgb\"\n",
    "    LIDAR_DIR = class_dir / \"lidar\"\n",
    "\n",
    "    # Check if directories exist\n",
    "    assert RGB_DIR.exists(), f\"RGB directory not found: {RGB_DIR}\"\n",
    "    assert LIDAR_DIR.exists(), f\"LIDAR directory not found: {LIDAR_DIR}\"\n",
    "\n",
    "    # Count files\n",
    "    rgb_files = sorted(RGB_DIR.glob(\"*.png\"))\n",
    "    npy_files = sorted(LIDAR_DIR.glob(\"*.npy\"))\n",
    "\n",
    "    print(f\"Found {len(rgb_files)} RGB images\")\n",
    "    print(f\"Found {len(npy_files)} NPY LiDAR\")\n",
    "\n",
    "    # Verify matching files\n",
    "    rgb_stems = {f.stem for f in rgb_files}\n",
    "    npy_stems = {f.stem for f in npy_files}\n",
    "    matching = rgb_stems & npy_stems\n",
    "\n",
    "    # store all matching pairs with full paths\n",
    "    pairs[class_name] = [\n",
    "        {\n",
    "            \"stem\": stem,\n",
    "            \"rgb\": RGB_DIR / f\"{stem}.png\",\n",
    "            \"lidar\": LIDAR_DIR / f\"{stem}.npy\",\n",
    "        }\n",
    "        for stem in sorted(matching)\n",
    "    ]\n",
    "\n",
    "    print(f\"Matching pairs: {len(matching)}\")\n",
    "\n",
    "    if len(matching) == 0:\n",
    "        print(\"\\n⚠️  ERROR: No matching RGB/LIDAR pairs found!\")\n",
    "    else:\n",
    "        print(f\"\\n✅ Ready to create dataset with {len(matching)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bfd49d",
   "metadata": {},
   "source": [
    "# Create FiftyOne Grouped Dataset\n",
    "A grouped dataset allows us to associate RGB images with their corresponding point clouds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df90c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete existing dataset if it exists\n",
    "if FIFTYONE_DATASET_NAME in fo.list_datasets():\n",
    "    print(f\"Deleting existing dataset: {FIFTYONE_DATASET_NAME}\")\n",
    "    fo.delete_dataset(FIFTYONE_DATASET_NAME)\n",
    "\n",
    "# Create new grouped dataset\n",
    "print(f\"Creating new dataset: {FIFTYONE_DATASET_NAME}\")\n",
    "dataset = fo.Dataset(FIFTYONE_DATASET_NAME, persistent=True)\n",
    "dataset.add_group_field(\"group\", default=\"rgb\")\n",
    "\n",
    "print(f\"✅ Created grouped dataset: {FIFTYONE_DATASET_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7c44fb",
   "metadata": {},
   "source": [
    "# Add Samples to Dataset\n",
    "\n",
    "For each matching RGB/LIDAR pair, we create a group with two slices:\n",
    "- `rgb`: The camera image\n",
    "- `lidar`: The point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1d9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "\n",
    "for class_name, class_pairs in pairs.items():\n",
    "    label_str = \"cube\" if class_name == \"cubes\" else \"sphere\"\n",
    "\n",
    "    for item in class_pairs:\n",
    "        # Get file paths\n",
    "        rgb_path = item[\"rgb\"]\n",
    "        lidar_path = item[\"lidar\"]\n",
    "\n",
    "        # Create group\n",
    "        group = fo.Group()\n",
    "\n",
    "        # Create RGB sample\n",
    "        rgb_sample = fo.Sample(\n",
    "            filepath=str(rgb_path),\n",
    "            group=group.element(\"rgb\"),\n",
    "            label=fo.Classification(label=label_str),\n",
    "        )\n",
    "\n",
    "        # Create PCD sample\n",
    "        lidar_sample = fo.Sample(\n",
    "            filepath=str(lidar_path),\n",
    "            group=group.element(\"lidar\"),\n",
    "            label=fo.Classification(label=label_str),\n",
    "        )\n",
    "\n",
    "        samples.extend([rgb_sample, lidar_sample])\n",
    "\n",
    "# Add all samples to dataset\n",
    "dataset.add_samples(samples)\n",
    "\n",
    "print(f\"✅ Created dataset '{dataset.name}' with {len(dataset)} samples\")\n",
    "print(\"Group field:\", dataset.group_field)\n",
    "print(\"Group slices:\", dataset.group_slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ad0eaf",
   "metadata": {},
   "source": [
    "# Launch FiftyOne App\n",
    "\n",
    "This will open the FiftyOne App in your browser where you can:\n",
    "- View RGB images and point clouds side-by-side\n",
    "- Use the group slices dropdown to switch between modalities\n",
    "- Filter samples by metadata (positions, colors)\n",
    "- Navigate through the dataset interactively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9ef70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "session = fo.launch_app(dataset, auto=False)\n",
    "print(session.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3ebeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_per_class = {cls: len(items) for cls, items in pairs.items()}\n",
    "total_samples = sum(total_per_class.values())\n",
    "\n",
    "print(\"Total samples per class:\")\n",
    "for cls, n in total_per_class.items():\n",
    "    print(f\"  {cls}: {n}\")\n",
    "print(f\"\\nTotal samples: {total_samples}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3fc18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# picks the first class and first sample from pairs\n",
    "any_class = CLASSES[0]\n",
    "sample = pairs[any_class][0]\n",
    "\n",
    "sample_rgb_path = sample[\"rgb\"]\n",
    "sample_lidar_path = sample[\"lidar\"]\n",
    "\n",
    "# RGB image\n",
    "rgb_img = Image.open(sample_rgb_path)\n",
    "print(\"RGB image:\")\n",
    "print(\"  size (width, height):\", rgb_img.size)\n",
    "print(\"  mode:\", rgb_img.mode)\n",
    "print(\"  format:\", rgb_img.format)\n",
    "\n",
    "# LiDAR depth map\n",
    "lidar = np.load(sample_lidar_path)\n",
    "print(\"\\nLiDAR depth map:\")\n",
    "print(\"  shape:\", lidar.shape)\n",
    "print(\"  dtype:\", lidar.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce7dd1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(SEED)\n",
    "train_ratio = 0.8\n",
    "\n",
    "splits = {\n",
    "    \"train\": {},\n",
    "    \"val\": {},\n",
    "}\n",
    "\n",
    "for cls, items in pairs.items():\n",
    "    n = len(items)\n",
    "    n_train = int(n * train_ratio)\n",
    "\n",
    "    splits[\"train\"][cls] = items[:n_train]\n",
    "    splits[\"val\"][cls] = items[n_train:]\n",
    "\n",
    "train_size = sum(len(v) for v in splits[\"train\"].values())\n",
    "val_size = sum(len(v) for v in splits[\"val\"].values())\n",
    "\n",
    "print(\"Train/validation sizes:\")\n",
    "for cls in CLASSES:\n",
    "    print(\n",
    "        f\"  {cls}: train={len(splits['train'][cls])}, \"\n",
    "        f\"val={len(splits['val'][cls])}\"\n",
    "    )\n",
    "print(f\"\\nTotal train: {train_size}\")\n",
    "print(f\"Total val:   {val_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d695d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data ---\n",
    "class_names = list(total_per_class.keys())\n",
    "counts_full = [total_per_class[c] for c in class_names]\n",
    "\n",
    "train_counts = [len(splits[\"train\"][c]) for c in class_names]\n",
    "val_counts   = [len(splits[\"val\"][c])   for c in class_names]\n",
    "\n",
    "# --- Plot ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 6))\n",
    "\n",
    "# left: full dataset distribution\n",
    "axes[0].bar(class_names, counts_full, color=\"steelblue\")\n",
    "axes[0].set_title(\"Class distribution (full dataset)\")\n",
    "axes[0].set_xlabel(\"Class\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "\n",
    "# right: train vs validation split\n",
    "x = range(len(class_names))\n",
    "width = 0.35\n",
    "\n",
    "axes[1].bar([i - width/2 for i in x], train_counts, width=width, label=\"Train\")\n",
    "axes[1].bar([i + width/2 for i in x], val_counts,   width=width, label=\"Val\")\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(class_names)\n",
    "axes[1].set_title(\"Train vs Validation\")\n",
    "axes[1].set_xlabel(\"Class\")\n",
    "axes[1].legend()\n",
    "\n",
    "# show plots\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(wspace=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a927f7",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "To delete the dataset and free up space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834ab3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to delete the dataset\n",
    "# fo.delete_dataset(DATASET_NAME)\n",
    "# print(f\"Deleted dataset: {DATASET_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "aCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
