{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8037fc0",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "In the bonus task, the MNIST digit classifier is extended with an “I Don’t Know” (IDK) option that allows the model to abstain from uncertain predictions. Confidence thresholds are applied to the classifier’s outputs to determine when a prediction should be accepted or labeled as IDK.\n",
    "\n",
    "The resulting predictions, confidence scores, and IDK decisions are stored in a FiftyOne dataset, enabling interactive analysis of model uncertainty and reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2965d53a",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "This section prepares the execution environment. Required libraries are installed and imported, project utilities are loaded, and the computation device (CPU/GPU) is selected. This ensures that the notebook runs reproducibly in Colab or locally and that all subsequent steps share a consistent configuration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace38284",
   "metadata": {},
   "source": [
    "## Installations & Imports\n",
    "\n",
    "Here we import all necessary packages. This section initializes all dependencies required for running the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98786db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install fiftyone==1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a685a9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%pip install sympy==1.12 torch==2.9.0 torchvision==0.20.0 numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b7e11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import fiftyone as fo\n",
    "import torch.nn.functional as Func\n",
    "from tqdm import tqdm\n",
    "from fiftyone import ViewField as F\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39303c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7302c256",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Here, global hyperparameters and configuration values are defined, such as image size, embedding dimensionality, and dataset names. Centralizing these constants ensures consistency across generation, evaluation, and visualization steps, and makes experiments easier to reproduce and modify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb05f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_CLASSIFIER = 64\n",
    "FIFTYONE_BONUS_DATASET_NAME = \"mnist_idk_experiment\"\n",
    "CONFIDENCE_THRESHOLD = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc63a22",
   "metadata": {},
   "source": [
    "## Data Paths\n",
    "\n",
    "This section defines the filesystem layout used throughout the notebook. Google Drive is mounted to access pretrained checkpoints and datasets, and paths for data loading and result storage are established. All model loading, image saving, and logging rely on these paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc837bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "STORAGE_PATH = Path(\"/content/drive/MyDrive/Colab Notebooks/Applied Computer Vision/Applied-Computer-Vision-Projects/Diffusion_Model_03\")\n",
    "DATA_PATH = STORAGE_PATH / \"data\"\n",
    "\n",
    "CLASSIFIER_MODEL_PATH = STORAGE_PATH / \"checkpoints/best_mnist_classifier.pth\"\n",
    "\n",
    "TEMP_IMG_DIR = Path(\"/content/mnist_temp\")\n",
    "os.makedirs(TEMP_IMG_DIR, exist_ok=True)\n",
    "\n",
    "EXPORT_MNIST_DIR = Path(\"/content/mnist_idk_export\")\n",
    "os.makedirs(EXPORT_MNIST_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462204c",
   "metadata": {},
   "source": [
    "## Load the model\n",
    "\n",
    "Here, a pretrained MNIST classifier is loaded and prepared for inference. The model is evaluated on the test set to provide a high-accuracy baseline before introducing the IDK mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mnist Model Architecture\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model\n",
    "\n",
    "# initialize mnist classifier\n",
    "mnist_classifier = MNISTClassifier().to(device)\n",
    "\n",
    "# Load model weights\n",
    "try:\n",
    "    mnist_classifier.load_state_dict(torch.load(CLASSIFIER_MODEL_PATH, map_location=device))\n",
    "    print(\"Model loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Please run the training loop in the previous notebook to generate the model file.\")\n",
    "\n",
    "mnist_classifier.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7c706",
   "metadata": {},
   "source": [
    "## Create test dataset\n",
    "\n",
    "This section prepares the MNIST test dataset with appropriate preprocessing. The dataset serves as the input for evaluating the IDK-augmented prediction function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data\n",
    "transform_classifier = transforms.Compose([\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),                     \n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "test_dataset_classifier = datasets.MNIST('.', train=False, download=True, transform=transform_classifier)\n",
    "test_loader_classifier = DataLoader(test_dataset_classifier, batch_size=BATCH_SIZE_CLASSIFIER, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af05c4",
   "metadata": {},
   "source": [
    "## Create the prediction function\n",
    "\n",
    "The prediction function applies a softmax to model outputs and compares the maximum confidence to a threshold. Predictions below the threshold are labeled as “IDK”, enabling controlled abstention when the model is uncertain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_idk(image, model, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Performs MNIST digit prediction with an explicit \"I Don't Know\" (IDK) option.\n",
    "\n",
    "    The model predicts a digit only if its confidence exceeds a given threshold;\n",
    "    otherwise, the prediction is rejected as IDK.\n",
    "\n",
    "    Args:\n",
    "        image (torch.Tensor): Input image tensor of shape (1, 28, 28) or (1, 1, 28, 28).\n",
    "        model (nn.Module): Trained MNIST classifier.\n",
    "        threshold (float): Confidence threshold for accepting a prediction.\n",
    "\n",
    "    Returns:\n",
    "        tuple[str, float]: Predicted digit (\"0\"–\"9\") or \"IDK\", and the associated confidence.\n",
    "    \"\"\"\n",
    "    # Ensure batch dimension\n",
    "    if image.dim() == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        logits = model(image)\n",
    "\n",
    "        # Convert logits to class probabilities\n",
    "        probs = Func.softmax(logits, dim=1)\n",
    "\n",
    "        # Select most confident class\n",
    "        confidence, predicted_idx = torch.max(probs, dim=1)\n",
    "        confidence = confidence.item()\n",
    "        predicted_idx = predicted_idx.item()\n",
    "\n",
    "    # Apply confidence-based abstention\n",
    "    if confidence >= threshold:\n",
    "        return str(predicted_idx), confidence\n",
    "    else:\n",
    "        return \"IDK\", confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66889341",
   "metadata": {},
   "source": [
    "## Create Fiftyone Dataset\n",
    "\n",
    "Inference results are stored in a FiftyOne dataset, including ground-truth labels, predicted labels (with IDK), and confidence scores. This enables interactive inspection of uncertain predictions and systematic analysis of coverage versus accuracy.\n",
    "\n",
    "### Choosing the confidence threshold\n",
    "Because the trained MNIST classifier achieves 99.24% accuracy, it typically assigns very high confidence to real MNIST images. Consequently, only very high thresholds (e.g. 0.99) produce noticeable IDK behavior on in-distribution data. For diffusion-generated images, confidence values are often lower due to artifacts and ambiguity, so the same threshold leads to substantially more abstentions.\n",
    "\n",
    "This illustrates how the confidence threshold directly controls the trade-off between prediction coverage and reliability, and why threshold selection must be adapted to the expected data quality and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new FiftyOne dataset for the IDK experiment\n",
    "\n",
    "# Remove existing dataset to avoid name conflicts\n",
    "if FIFTYONE_BONUS_DATASET_NAME in fo.list_datasets():\n",
    "    print(f\"Deleting existing dataset: {FIFTYONE_BONUS_DATASET_NAME}\")\n",
    "    fo.delete_dataset(FIFTYONE_BONUS_DATASET_NAME)\n",
    "\n",
    "dataset = fo.Dataset(name=FIFTYONE_BONUS_DATASET_NAME)\n",
    "samples = []\n",
    "\n",
    "print(\"Running inference...\")\n",
    "for i in tqdm(range(len(test_dataset_classifier))):\n",
    "    img_tensor, label = test_dataset_classifier[i]\n",
    "\n",
    "    # Predict digit or abstain using the IDK mechanism\n",
    "    pred_label, conf = predict_with_idk(\n",
    "        img_tensor, mnist_classifier, CONFIDENCE_THRESHOLD\n",
    "    )\n",
    "\n",
    "    # Save image to disk, as FiftyOne operates on file paths\n",
    "    filepath = os.path.join(TEMP_IMG_DIR, f\"img_{i}.png\")\n",
    "    save_image(img_tensor, filepath)\n",
    "\n",
    "    sample = fo.Sample(filepath=filepath)\n",
    "    sample[\"ground_truth\"] = fo.Classification(label=str(label))\n",
    "    sample[\"prediction_with_idk\"] = fo.Classification(\n",
    "        label=pred_label, confidence=conf\n",
    "    )\n",
    "\n",
    "    # Tag samples where the model abstains\n",
    "    if pred_label == \"IDK\":\n",
    "        sample.tags.append(\"IDK\")\n",
    "\n",
    "    samples.append(sample)\n",
    "\n",
    "    # Add samples in batches to reduce memory usage\n",
    "    if len(samples) >= 1000:\n",
    "        dataset.add_samples(samples)\n",
    "        samples = []\n",
    "\n",
    "# Add remaining samples\n",
    "if samples:\n",
    "    dataset.add_samples(samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26cab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch App\n",
    "session = fo.launch_app(dataset)\n",
    "print(f\"Dataset created with {len(dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a View of only IDK samples\n",
    "idk_view = dataset.match_tags(\"IDK\")\n",
    "print(f\"Total IDK cases found: {len(idk_view)}\")\n",
    "\n",
    "# Launch App\n",
    "session.view = idk_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coverage to test for different confidence thresholds\n",
    "\n",
    "total_samples = len(dataset)\n",
    "idk_count = len(idk_view)       # number of idk samples\n",
    "\n",
    "# Coverage = (Total - IDK) / Total\n",
    "covered_view = dataset.match_tags(\"IDK\", bool=False)\n",
    "covered_count = len(covered_view)\n",
    "coverage = covered_count / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "# Accuracy on covered samples\n",
    "correct_covered_view = covered_view.match(\n",
    "    F(\"prediction_with_idk.label\") == F(\"ground_truth.label\")\n",
    ")\n",
    "correct_covered_count = len(correct_covered_view)\n",
    "\n",
    "# Accuracy on Covered = Correct / Covered\n",
    "accuracy_on_covered = correct_covered_count / covered_count if covered_count > 0 else 0.0\n",
    "\n",
    "# Standard Accuracy (treating IDK as incorrect)\n",
    "standard_accuracy = correct_covered_count / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "# Print report\n",
    "print(f\"Total Test Images:    {total_samples}\")\n",
    "print(f\"IDK Responses:        {idk_count}\")\n",
    "print(f\"Covered Responses:    {covered_count}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"COVERAGE:             {coverage:.2%}  (Goal: Keep this high)\")\n",
    "print(f\"ACCURACY (Covered):   {accuracy_on_covered:.2%}  (Goal: Higher than standard accuracy)\")\n",
    "print(f\"ACCURACY (Standard):  {standard_accuracy:.2%}  (Baseline)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc2a2d9",
   "metadata": {},
   "source": [
    "**Observation: Effect of the Confidence Threshold**\n",
    "\n",
    "Increasing the confidence threshold leads to more frequent IDK decisions, which slightly reduces coverage but improves accuracy on the remaining predictions. At a threshold of 0.99, coverage drops to 97.88%, while accuracy on covered samples reaches 99.78%, clearly outperforming the standard accuracy of 97.66%. Lowering the threshold to 0.98 and 0.95 increases coverage to 98.35% and 98.86%, respectively, but also leads to a gradual decrease in covered accuracy (99.72% and 99.64%).\n",
    "\n",
    "Overall, these results illustrate the central trade-off controlled by the threshold: higher thresholds prioritize reliability by rejecting uncertain predictions, while lower thresholds increase coverage at the cost of slightly reduced accuracy. This demonstrates why careful threshold selection is essential for balancing prediction quality and coverage in uncertainty-aware systems.\n",
    "\n",
    "\n",
    "Here a more detailed summary of the threshold experiments:\n",
    "| Threshold | IDK Responses | Coverage (%) | Accuracy (Covered) (%) | Accuracy (Standard) (%) |\n",
    "|----------:|--------------:|-------------:|------------------------:|-------------------------:|\n",
    "| 0.99      | 212           | 97.88        | 99.78                   | 97.66                    |\n",
    "| 0.98      | 165           | 98.35        | 99.72                   | 98.07                    |\n",
    "| 0.95      | 114           | 98.86        | 99.64                   | 98.50                    |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd669f",
   "metadata": {},
   "source": [
    "## Publish Dataset on Hugging Face\n",
    "\n",
    "Finally, the FiftyOne dataset is exported and published to Hugging Face. This makes the results reproducible and shareable, allowing others to explore the dataset and the behavior of the IDK mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FiftyOne dataset (images + metadata) to disk\n",
    "print(f\"Exporting dataset to {EXPORT_MNIST_DIR}...\")\n",
    "\n",
    "dataset.export(\n",
    "    export_dir=str(EXPORT_MNIST_DIR),\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    export_media=True, # This ensures the actual .png images are included\n",
    ")\n",
    "\n",
    "print(\"Export complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish to Hugging Face\n",
    "\n",
    "# Get token\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    print(\"Uploading to Hugging Face...\")\n",
    "    api = HfApi(token=HF_TOKEN)\n",
    "    repo_id = \"mmarschn/mnist-idk-experiment\"\n",
    "\n",
    "    # Create the repo if it doesn't exist\n",
    "    try:\n",
    "        api.create_repo(repo_id=repo_id, repo_type=\"dataset\", exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Repo creation warning: {e}\")\n",
    "\n",
    "    # Upload the folder\n",
    "    api.upload_large_folder(\n",
    "        folder_path=EXPORT_MNIST_DIR,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\",\n",
    "        ignore_patterns=[\"*.ipynb_checkpoints\"],\n",
    "    )\n",
    "    print(f\"Successfully published to: https://huggingface.co/datasets/{repo_id}\")\n",
    "else:\n",
    "    print(\"Error: HF_TOKEN not found. Cannot publish to Hugging Face.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
