{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8037fc0",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "For the bonus, you will modify an MNIST-digit classifier to include an \"I don't know\" (IDK) option. This is useful in real-world applications where a model should defer to a human when it is not confident in its prediction.\n",
    "\n",
    "You will modify the MNIST generation and classification notebook for this task. Create and publish a FiftyOne dataset with your experimentsâ€™ results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c462204c",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd05d0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mnist Model Architecture\n",
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfa4a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the model\n",
    "# initialize mnist classifier\n",
    "mnist_classifier = MNISTClassifier().to(device)\n",
    "\n",
    "## TODO\n",
    "model_path = STORAGE_PATH / \"checkpoints/best_mnist_classifier.pth\"\n",
    "\n",
    "try:\n",
    "    mnist_classifier.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    print(\"Model loaded.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Please run the training loop in the previous notebook to generate the model file.\")\n",
    "\n",
    "mnist_classifier.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa7c706",
   "metadata": {},
   "source": [
    "## Create test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e4785b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE_CLASSIFIER = 64\n",
    "\n",
    "# Get the data\n",
    "transform_classifier = transforms.Compose([\n",
    "    transforms.ToTensor(),                      ## TODO: change toImage() etc. pp.\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "test_dataset_classifier = datasets.MNIST('.', train=False, download=True, transform=transform_classifier)\n",
    "test_loader_classifier = DataLoader(test_dataset_classifier, batch_size=BATCH_SIZE_CLASSIFIER, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7af05c4",
   "metadata": {},
   "source": [
    "## Create the prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4034bbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the predict_with_idk Logic\n",
    "def predict_with_idk(image, model, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        image: A single image tensor (1, 28, 28)\n",
    "        model: The trained PyTorch model\n",
    "        threshold: Float between 0 and 1\n",
    "    Returns:\n",
    "        String: Predicted class \"0\"-\"9\" or \"IDK\"\n",
    "    \"\"\"\n",
    "    # Ensure tensor has batch dimension [1, 1, 28, 28]\n",
    "    if image.dim() == 3:\n",
    "        image = image.unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image = image.to(device)\n",
    "        logits = model(image)\n",
    "\n",
    "        # Apply Softmax to get probabilities\n",
    "        probs = Func.softmax(logits, dim=1)\n",
    "\n",
    "        # Get the highest probability and its index\n",
    "        confidence, predicted_idx = torch.max(probs, dim=1)\n",
    "\n",
    "        confidence = confidence.item()\n",
    "        predicted_idx = predicted_idx.item()\n",
    "\n",
    "    # Check threshold\n",
    "    if confidence >= threshold:\n",
    "        return str(predicted_idx), confidence\n",
    "    else:\n",
    "        return \"IDK\", confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66889341",
   "metadata": {},
   "source": [
    "## Create Fiftyone Dataset\n",
    "\n",
    "\n",
    "TODO: reformulate for assignment\n",
    "Next Step for the Bonus Task\n",
    "Now that you have a model with 99.24% accuracy, you can proceed to the code I wrote for you in the previous message.\n",
    "Important Tip: Because your model is so accurate (99.24%), it is very confident. When testing your \"IDK\" function:\n",
    "On Real MNIST: You might need a very high threshold (e.g., 0.99) to see any \"IDK\" results.\n",
    "On Generated (Diffusion) Images: This is where the IDK function will shine. The generated digits are often imperfect, so the classifier will be less confident, and your function will correctly output \"IDK\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b817b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_IMG_DIR = \"mnist_temp\"\n",
    "os.makedirs(TEMP_IMG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e1077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new FiftyOne dataset\n",
    "\n",
    "# Delete existing dataset if it exists\n",
    "if FIFTYONE_BONUS_DATASET_NAME in fo.list_datasets():\n",
    "    print(f\"Deleting existing dataset: {FIFTYONE_BONUS_DATASET_NAME}\")\n",
    "    fo.delete_dataset(FIFTYONE_BONUS_DATASET_NAME)\n",
    "\n",
    "dataset = fo.Dataset(name=FIFTYONE_BONUS_DATASET_NAME)\n",
    "\n",
    "samples = []\n",
    "\n",
    "print(\"Running Inference...\")\n",
    "for i in tqdm(range(len(test_dataset_classifier))):\n",
    "    img_tensor, label = test_dataset_classifier[i]\n",
    "\n",
    "    # Run our IDK function\n",
    "    pred_label, conf = predict_with_idk(img_tensor, mnist_classifier, CONFIDENCE_THRESHOLD)\n",
    "\n",
    "    # Save image to disk for FiftyOne as it usually needs file paths\n",
    "    filepath = os.path.join(TEMP_IMG_DIR, f\"img_{i}.png\")\n",
    "    # Un-normalize for saving\n",
    "    save_image(img_tensor, filepath)\n",
    "\n",
    "    # Create Sample\n",
    "    sample = fo.Sample(filepath=filepath)\n",
    "    sample[\"ground_truth\"] = fo.Classification(label=str(label))\n",
    "    sample[\"prediction_with_idk\"] = fo.Classification(label=pred_label, confidence=conf)\n",
    "\n",
    "    # Add a simple tag to easily find IDK cases\n",
    "    if pred_label == \"IDK\":\n",
    "        sample.tags.append(\"IDK\")\n",
    "\n",
    "    samples.append(sample)\n",
    "\n",
    "    # Optimization: Add in batches to save memory\n",
    "    if len(samples) >= 1000:\n",
    "        dataset.add_samples(samples)\n",
    "        samples = []\n",
    "\n",
    "# Add remaining samples\n",
    "if samples:\n",
    "    dataset.add_samples(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26cab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch App\n",
    "session = fo.launch_app(dataset)\n",
    "print(f\"Dataset created with {len(dataset)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eff1fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a View of only IDK samples\n",
    "idk_view = dataset.match_tags(\"IDK\")\n",
    "print(f\"Total IDK cases found: {len(idk_view)}\")\n",
    "\n",
    "# Launch App\n",
    "session.view = idk_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05743d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute coverage to test for different confidence thresholds\n",
    "\n",
    "total_samples = len(dataset)\n",
    "idk_count = len(idk_view)       # number of idk samples\n",
    "\n",
    "# Coverage = (Total - IDK) / Total\n",
    "covered_view = dataset.match_tags(\"IDK\", bool=False)\n",
    "covered_count = len(covered_view)\n",
    "coverage = covered_count / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "# Calculate Accuracy on Covered Samples\n",
    "# From those covered samples, count how many match the ground truth\n",
    "correct_covered_view = covered_view.match(\n",
    "    F(\"prediction_with_idk.label\") == F(\"ground_truth.label\")\n",
    ")\n",
    "correct_covered_count = len(correct_covered_view)\n",
    "\n",
    "# Accuracy on Covered = Correct / Covered\n",
    "accuracy_on_covered = correct_covered_count / covered_count if covered_count > 0 else 0.0\n",
    "\n",
    "# Standard Accuracy (treating IDK as incorrect)\n",
    "# This helps visualize the trade-off\n",
    "standard_accuracy = correct_covered_count / total_samples if total_samples > 0 else 0.0\n",
    "\n",
    "# --- Print Report ---\n",
    "print(f\"Total Test Images:    {total_samples}\")\n",
    "print(f\"IDK Responses:        {idk_count}\")\n",
    "print(f\"Covered Responses:    {covered_count}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"COVERAGE:             {coverage:.2%}  (Goal: Keep this high)\")\n",
    "print(f\"ACCURACY (Covered):   {accuracy_on_covered:.2%}  (Goal: Higher than standard accuracy)\")\n",
    "print(f\"ACCURACY (Standard):  {standard_accuracy:.2%}  (Baseline)\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cd669f",
   "metadata": {},
   "source": [
    "## Publish Dataset on Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9b2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FiftyOne dataset (images + metadata) to disk\n",
    "export_dir = \"mnist_idk_export\"\n",
    "os.makedirs(export_dir, exist_ok=True)\n",
    "\n",
    "print(f\"Exporting dataset to {export_dir}...\")\n",
    "\n",
    "dataset.export(\n",
    "    export_dir=export_dir,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    export_media=True, # This ensures the actual .png images are included\n",
    ")\n",
    "\n",
    "print(\"Export complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4a9600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publish to Hugging Face\n",
    "\n",
    "# Get token\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if HF_TOKEN:\n",
    "    print(\"Uploading to Hugging Face...\")\n",
    "    api = HfApi(token=HF_TOKEN)\n",
    "    repo_id = \"mmarschn/mnist-idk-experiment\"\n",
    "\n",
    "    # Create the repo if it doesn't exist\n",
    "    try:\n",
    "        api.create_repo(repo_id=repo_id, repo_type=\"dataset\", exist_ok=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Repo creation warning: {e}\")\n",
    "\n",
    "    # Upload the folder\n",
    "    api.upload_large_folder(\n",
    "        folder_path=export_dir,\n",
    "        repo_id=repo_id,\n",
    "        repo_type=\"dataset\",\n",
    "        ignore_patterns=[\"*.ipynb_checkpoints\"],\n",
    "    )\n",
    "    print(f\"Successfully published to: https://huggingface.co/datasets/{repo_id}\")\n",
    "else:\n",
    "    print(\"Error: HF_TOKEN not found. Cannot publish to Hugging Face.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
