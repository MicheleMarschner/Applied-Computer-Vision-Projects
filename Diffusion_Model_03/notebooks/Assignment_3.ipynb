{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qFjE18NApEW"
   },
   "source": [
    "# Overview\n",
    "\n",
    "This assignment builds a complete evaluation pipeline for a pretrained text-to-image diffusion model. First, images of flowers are generated using classifier-free guided DDPM sampling conditioned on CLIP text embeddings, while intermediate U-Net bottleneck features are extracted via forward hooks.\n",
    "\n",
    "The generated samples are then evaluated quantitatively using CLIP Score (semantic alignment) and Fr√©chet Inception Distance (distribution realism).\n",
    "\n",
    "To analyze internal representations, the extracted U-Net embeddings are organized in a FiftyOne dataset, enabling computation of uniqueness and representativeness scores and interactive visualization.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fISkjJ_SNZ3h"
   },
   "source": [
    "**Note: Generation Cardinalities in the Sampling Pipeline**\n",
    "\n",
    "Each of the three text prompts is evaluated under multiple guidance strengths ùë§, producing several variations per prompt. With seven guidance values, this results in 21 final images. Internally, classifier-free guidance doubles the batch size during sampling to combine conditioned and unconditioned predictions, but this duplication is only used to compute the guidance signal and does not increase the number of output images. All downstream analyses therefore operate on exactly 21 images and 21 corresponding embeddings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w79TQtD755-G"
   },
   "source": [
    "# Setup\n",
    "\n",
    "The project repository is mounted from Google Drive and added to the Python path to allow clean imports from the src module. The dataset is copied to the local Colab filesystem to improve I/O performance during training. All global settings (random seed, device selection, paths, batch sizes) are defined once and reused across the notebook to ensure consistency and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FizpBjUGApEX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/MicheleMarschner/Documents/Uni/WiSe_2025_26/Applied CV/Applied-Computer-Vision-Projects/Diffusion_Model_03\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "%cd \"/content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03\"\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfyAg0fuApEY"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%%capture\n",
    "%pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-dJyKt0Y7Ka"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "import clip\n",
    "import open_clip\n",
    "\n",
    "import wandb\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w73asd8TRGP"
   },
   "outputs": [],
   "source": [
    "from utils import UNet_utils, ddpm_utils, other_utils, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTmFAsHQApEY"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/data\n",
    "!cp -r \"$config.DRIVE_ROOT/data\"* /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_MPf0Igufth"
   },
   "outputs": [],
   "source": [
    "other_utils.set_seeds(config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJmpVRWfmTlJ"
   },
   "source": [
    "# Part 1: Image Generation and Embedding Extraction\n",
    "\n",
    "In this section, you will load the pre-trained U-Net model from notebook 05_CLIP.ipynb of the corresponding NVIDIA course, generate images of flowers, and extract embeddings from the model's bottleneck.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMgyyw1ib5sh"
   },
   "source": [
    "## Recreate CLIP + DDPM + sampling\n",
    "\n",
    "In this section, the CLIP text encoder and the DDPM sampling process are reinitialized to match the training setup. The noise schedule and diffusion parameters are defined to ensure compatibility with the pretrained U-Net. This reconstruction is necessary to generate images that are consistent with the original training regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbo21wHmbEYL"
   },
   "outputs": [],
   "source": [
    "# Load CLIP for encoding the text prompts\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=config.DEVICE)\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtXC_GcibO3r"
   },
   "outputs": [],
   "source": [
    "# Re-initialize DDPM wrapper\n",
    "B_start = 0.0001\n",
    "B_end = 0.02\n",
    "B = torch.linspace(B_start, B_end, config.TIMESTEPS).to(config.DEVICE)\n",
    "\n",
    "ddpm = ddpm_utils.DDPM(B, config.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3qgBDweb0Nw"
   },
   "source": [
    "## Load the pre-trained U-Net\n",
    "\n",
    "Here, the U-Net architecture is instantiated and pretrained weights are loaded from disk. The model is switched to evaluation mode to disable training-specific behavior. This step restores the trained generative model used for all subsequent image synthesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5AFckgFZIQs"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UNet_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Define the uNet Architecture\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m uNet_model = \u001b[43mUNet_utils\u001b[49m.UNet(\n\u001b[32m      3\u001b[39m     T=config.TIMESTEPS,\n\u001b[32m      4\u001b[39m     img_ch=config.IMG_CH,\n\u001b[32m      5\u001b[39m     img_size=config.IMG_SIZE,\n\u001b[32m      6\u001b[39m     down_chs=(\u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m512\u001b[39m),\n\u001b[32m      7\u001b[39m     t_embed_dim=\u001b[32m8\u001b[39m,\n\u001b[32m      8\u001b[39m     c_embed_dim=config.CLIP_FEATURES\n\u001b[32m      9\u001b[39m ).to(config.DEVICE)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Load the model weights\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'UNet_utils' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the uNet Architecture\n",
    "uNet_model = UNet_utils.UNet(\n",
    "    T=config.TIMESTEPS,\n",
    "    img_ch=config.IMG_CH,\n",
    "    img_size=config.IMG_SIZE,\n",
    "    down_chs=(256, 256, 512),\n",
    "    t_embed_dim=8,\n",
    "    c_embed_dim=config.CLIP_FEATURES\n",
    ").to(config.DEVICE)\n",
    "\n",
    "print(\"Num params: \", sum(p.numel() for p in uNet_model.parameters()))\n",
    "\n",
    "# Load the model weights\n",
    "try:\n",
    "    uNet_model.load_state_dict(torch.load(config.UNET_MODEL_PATH))\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Model weights not found.\")\n",
    "\n",
    "uNet_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Us9Y6ZV-ZrGM"
   },
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "text_prompts = [\n",
    "    \"A photo of a red rose\",\n",
    "    \"A photo of a white daisy\",\n",
    "    \"A photo of a yellow sunflower\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVTn_TznH1OT"
   },
   "outputs": [],
   "source": [
    "# Sanity check: Calculate how many images are to be generated\n",
    "# Guidance strengths for classifier-free guidance\n",
    "P = len(text_prompts)           # number of prompts\n",
    "W = len(config.W_TESTS)         # guidance values per prompt\n",
    "n_samples = P * W               # Total images generated: one per (prompt, guidance) pair\n",
    "                                # 7 guidance weights => 7 images per prompt\n",
    "\n",
    "print(\"Expected n_samples:\", n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFhxjtKzZ3tb"
   },
   "outputs": [],
   "source": [
    "# Store intermediate feature maps extracted via forward hooks\n",
    "embeddings_storage = {}\n",
    "\n",
    "def get_embedding_hook(name):\n",
    "    \"\"\"\n",
    "    Creates a forward hook that stores the output of a given layer.\n",
    "\n",
    "    The output is detached from the computation graph to avoid\n",
    "    gradient tracking and reduce memory usage.\n",
    "    \"\"\"\n",
    "    def hook(model, input, output):\n",
    "        # We use .detach() to disconnect from the gradient graph (saves memory)\n",
    "        embeddings_storage[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register a forward hook on the U-Net bottleneck layer\n",
    "uNet_model.down2.register_forward_hook(get_embedding_hook('down2'))\n",
    "print(\"Hook registered on model.down2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZla3_4IaIeI"
   },
   "outputs": [],
   "source": [
    "def sample_flowers_with_hook(text_list, model, ddpm, input_size, T, device, w_tests):\n",
    "    \"\"\"\n",
    "    Generates images from text prompts using classifier-free guided diffusion\n",
    "    while capturing intermediate U-Net embeddings via a forward hook.\n",
    "\n",
    "    Args:\n",
    "        text_list (list[str]): Text prompts used for conditioning.\n",
    "        model (nn.Module): Pretrained U-Net diffusion model.\n",
    "        ddpm: Diffusion process wrapper.\n",
    "        input_size (tuple): Spatial size of generated images.\n",
    "        T (int): Number of diffusion timesteps.\n",
    "        device (torch.device): Computation device.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Final generated images.\n",
    "        torch.Tensor: Stored intermediate diffusion states (for visualization).\n",
    "    \"\"\"\n",
    "    # Encode text prompts using CLIP\n",
    "    text_tokens = clip.tokenize(text_list).to(device)\n",
    "    c = clip_model.encode_text(text_tokens).float()\n",
    "\n",
    "    # Run diffusion sampling with classifier-free guidance\n",
    "    x_gen, x_gen_store = ddpm_utils.sample_w(model, ddpm, input_size, T, c, device, w_tests)\n",
    "\n",
    "    return x_gen, x_gen_store\n",
    "\n",
    "# Run the generation\n",
    "other_utils.set_seeds(config.SEED)\n",
    "\n",
    "print(\"Generating images...\")\n",
    "generated_images, _ = sample_flowers_with_hook(\n",
    "    text_list=text_prompts,\n",
    "    model=uNet_model,\n",
    "    ddpm=ddpm,\n",
    "    input_size=config.INPUT_SIZE,\n",
    "    T=config.TIMESTEPS,\n",
    "    device=config.DEVICE,\n",
    "    w_tests=config.W_TESTS\n",
    ")\n",
    "\n",
    "# Retrieve the embedding captured by the bottleneck hook\n",
    "extracted_embedding_all = embeddings_storage['down2']\n",
    "\n",
    "# Forward hooks capture both conditioned and unconditioned batches (CFG);\n",
    "# retain only the conditioned samples corresponding to the final images\n",
    "assert extracted_embedding_all.shape[0] >= n_samples, (\n",
    "    f\"embedding batch {extracted_embedding_all.shape[0]} < {n_samples}\"\n",
    ")\n",
    "extracted_embeddings = extracted_embedding_all[:n_samples]\n",
    "\n",
    "print(\"Using embeddings:\", extracted_embeddings.shape)\n",
    "print(f\"Generated {len(generated_images)} images.\")\n",
    "print(f\"Extracted embedding shape: {extracted_embedding_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUxLPJ_44_z6"
   },
   "outputs": [],
   "source": [
    "# Visualize the generated samples in a grid: rows correspond to text prompts,\n",
    "# and columns show the effect of varying guidance strengths (w).\n",
    "other_utils.show_generated_images_grid(\n",
    "    generated_images,\n",
    "    prompts=text_prompts,\n",
    "    w_tests=config.W_TESTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z0kPoXt8yVy"
   },
   "outputs": [],
   "source": [
    "# Save generated images to disk for downstream evaluation\n",
    "to_pil = ToPILImage()\n",
    "\n",
    "# Track saved image paths together with their prompts and guidance values\n",
    "saved_samples = []\n",
    "\n",
    "print(\"Saving images to disk...\")\n",
    "assert len(generated_images) == n_samples, (\n",
    "    f\"generated_images={len(generated_images)} != {n_samples}\"\n",
    ")\n",
    "\n",
    "for i, img_tensor in enumerate(generated_images):\n",
    "    # Recover prompt and guidance value from the sampling order\n",
    "    prompt = text_prompts[i % P]\n",
    "    w_val = config.W_TESTS[i // P]\n",
    "\n",
    "    # Map model output from [-1, 1] to [0, 1] for image saving and clip any artifacts that fell outside the valid range\n",
    "    img_norm = ((img_tensor + 1) / 2).clamp(0, 1).detach().cpu()\n",
    "    pil_img = to_pil(img_norm)\n",
    "\n",
    "    filename = os.path.join(\n",
    "        config.SAVE_DIR, f\"flower_w{w_val:+.1f}_p{i % P}_{i}.png\"\n",
    "    )\n",
    "    pil_img.save(filename)\n",
    "\n",
    "    saved_samples.append((filename, prompt, float(w_val)))\n",
    "\n",
    "print(\"All images saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUm5iqaMmmE0"
   },
   "source": [
    "# Part 2: Evaluation with CLIP Score and FID\n",
    "In this section, the quality of the generated images is evaluated using CLIP Score and Fr√©chet Inception Distance (FID), following the definitions provided in the assignment task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZhEq6gDsyBC"
   },
   "source": [
    "## CLIP Score\n",
    "\n",
    "The CLIP score is computed as the cosine similarity between image and text embeddings produced by a pretrained CLIP model. It answers the question: \"How accurately does the generated image depict the content described in the text prompt?\"\n",
    "\n",
    "A higher score indicates stronger semantic alignment. Scores are computed for all generated images, allowing comparison across different guidance strengths and prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1G3TVZwL-GTs"
   },
   "outputs": [],
   "source": [
    "# Initialize OpenCLIP model for CLIP-score evaluation\n",
    "clip_scorer, _, clip_preprocess_val = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\"\n",
    ")\n",
    "clip_scorer.to(config.DEVICE).eval()\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TddM8_umca36"
   },
   "outputs": [],
   "source": [
    "def calculate_clip_score(image_path, text_prompt, device=None):\n",
    "    \"\"\"\n",
    "    Computes a CLIP similarity score between an image and a text prompt.\n",
    "\n",
    "    The image and text are embedded using a pretrained OpenCLIP model, L2-normalized,\n",
    "    and compared via cosine similarity (dot product of normalized embeddings).\n",
    "\n",
    "    Args:\n",
    "        image_path (str | Path): Path to the image file on disk.\n",
    "        text_prompt (str): Text prompt to compare against.\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine similarity score (higher means stronger semantic alignment).\n",
    "    \"\"\"\n",
    "    # Preprocess and move to the same device as the CLIP model\n",
    "    image = clip_preprocess_val(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    text = tokenizer([text_prompt]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_scorer.encode_image(image)\n",
    "        text_features = clip_scorer.encode_text(text)\n",
    "\n",
    "        # Normalize to turn dot product into cosine similarity\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        score = (image_features @ text_features.T).item()\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# Compute CLIP scores for all generated samples\n",
    "clip_scores = []\n",
    "\n",
    "print(\"Calculating scores...\")\n",
    "\n",
    "for i, (filepath, prompt, w_val) in enumerate(saved_samples):\n",
    "    score = calculate_clip_score(\n",
    "        image_path=filepath,\n",
    "        text_prompt=prompt,\n",
    "        device=config.DEVICE,\n",
    "    )\n",
    "    clip_scores.append(score)\n",
    "\n",
    "avg_clip_score = float(np.mean(clip_scores))\n",
    "print(f\"Average CLIP Score: {avg_clip_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoT9UPO84K9u"
   },
   "outputs": [],
   "source": [
    "# Aggregate CLIP scores by guidance strength w and compute mean score per w\n",
    "by_w = defaultdict(list)\n",
    "for (filepath, prompt, w_val), score in zip(saved_samples, clip_scores):\n",
    "    by_w[w_val].append(score)\n",
    "\n",
    "print(\"Show aggregated CLIP scores by guidance strength w\")\n",
    "print(f\"{'w':>6} | {'Mean CLIP':>10} | {'n':>3}\")\n",
    "print(\"-\" * 26)\n",
    "\n",
    "for w in sorted(by_w):\n",
    "    mean_score = np.mean(by_w[w])\n",
    "    n = len(by_w[w])\n",
    "    print(f\"{w:>6.1f} | {mean_score:>10.4f} | {n:>3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKCxRoHDwMdK"
   },
   "outputs": [],
   "source": [
    "# example image of a real red rose\n",
    "real_rose_path = (\n",
    "    Path(config.TMP_ROOT)\n",
    "    / \"data/cropped_flowers/roses/15032112248_30c5284e54_n.jpg\"\n",
    ")\n",
    "\n",
    "other_utils.compare_generated_vs_real_roses(\n",
    "    generated_images,\n",
    "    prompt_idx=0,  # \"A photo of a red rose\"\n",
    "    prompts=text_prompts,\n",
    "    w_tests=config.W_TESTS,\n",
    "    real_rose_path=real_rose_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY1X2qd4s0BT"
   },
   "source": [
    "## FID Score\n",
    "\n",
    "To measure how realistic our generated images are, we calculate the Fr√©chet Inception Distance (FID) score. We use a powerful pre-trained InceptionV3 model as a feature \"judge.\" Both our generated images and the real flower images are prepared identically‚Äîresized to 299x299 and normalized‚Äîto ensure a fair comparison.\n",
    "\n",
    "FID is computed by comparing feature statistics extracted from the model for real and generated images. It statistically compares these two collections; a lower score indicates that the generated images are more similar to the real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfvm5w54KGxw"
   },
   "outputs": [],
   "source": [
    "# Load Pretrained InceptionV3;\n",
    "inception = inception_v3(\n",
    "    weights=Inception_V3_Weights.DEFAULT,\n",
    "    transform_input=False,\n",
    ").to(config.DEVICE)\n",
    "\n",
    "# To return features (2048) - not classes as the standard Inception model does - the final\n",
    "# \"Fully Connected\" layer needs to be replaced with a \"Pass Through\" (Identity)\n",
    "inception.fc = torch.nn.Identity()\n",
    "\n",
    "inception.eval()\n",
    "\n",
    "image_net_mean = [0.485, 0.456, 0.406]\n",
    "image_net_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Inception expects 299x299 size instead of now 32x32 and specific normalization\n",
    "inception_transform = transforms.Compose([\n",
    "    transforms.Resize((config.INCEPTION_IMG_SIZE, config.INCEPTION_IMG_SIZE)), # Up-sample from 32x32\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=image_net_mean, std=image_net_std)   # from original pytorch docs\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CresJrStNhT7"
   },
   "outputs": [],
   "source": [
    "def get_inception_features_from_raw(dataset_path, batch_size, model, device=None, num_workers=0):\n",
    "    \"\"\"\n",
    "    Extracts 2048-dimensional InceptionV3 feature embeddings for a dataset of images.\n",
    "\n",
    "    Args:\n",
    "        raw_dataset): dataset of the original images\n",
    "        model (nn.Module): Pretrained InceptionV3 feature extractor (fc = Identity).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N, 2048) containing feature embeddings for all images.\n",
    "    \"\"\"\n",
    "    raw_dataset = other_utils.MyDataset(dataset_path, inception_transform, config.CLASSES)\n",
    "    raw_dataloader  = DataLoader(raw_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    features = []                         # Stores feature batches\n",
    "    with torch.no_grad():\n",
    "      for img, _ in tqdm(raw_dataloader):\n",
    "          img = img.to(device)\n",
    "          f = model(img)                # Runs Inception forward pass\n",
    "          features.append(f.cpu().numpy())  # Transform to numpy for later mathematical operations\n",
    "    return np.concatenate(features, axis=0) # Concatenate batches to one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCvQK4HUUmfh"
   },
   "outputs": [],
   "source": [
    "def get_inception_features_from_files(saved_samples, batch_size, model, transform, device=None, num_workers=0):\n",
    "    \"\"\"\n",
    "    Loads images from disk and extracts InceptionV3 feature embeddings.\n",
    "\n",
    "    Args:\n",
    "        saved_samples: List of tuples containing image filepaths.\n",
    "        model: Pretrained feature extractor.\n",
    "        transform: Image preprocessing pipeline (resize/normalize).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Feature matrix of shape (N, 2048).\n",
    "    \"\"\"\n",
    "    dataset = other_utils.GeneratedListDataset(saved_samples, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_batch in tqdm(loader, desc=\"Extracting Generated Features\"):\n",
    "            img_batch = img_batch.to(device)\n",
    "\n",
    "            # The transform handles Resize, Scale, and Normalize\n",
    "            f = model(img_batch)\n",
    "            features.append(f.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG9XOXQwgdNU"
   },
   "outputs": [],
   "source": [
    "def calculate_fid(real_embeddings, gen_embeddings):\n",
    "\n",
    "    # Calculate mean and covariance\n",
    "    mu1, sigma1 = real_embeddings.mean(axis=0), np.cov(real_embeddings, rowvar=False)\n",
    "    mu2, sigma2 = gen_embeddings.mean(axis=0), np.cov(gen_embeddings, rowvar=False)\n",
    "\n",
    "    # Sum squared difference between means\n",
    "    ssdiff = np.sum((mu1 - mu2)**2)\n",
    "\n",
    "    # Product of covariances\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "    # Numerical error handling\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    # Final FID calculation\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwuITZymtCOp"
   },
   "outputs": [],
   "source": [
    "# Extract features from real images on disk\n",
    "dataset_path = config.TMP_ROOT / \"data/cropped_flowers\"\n",
    "real_embeddings = get_inception_features_from_raw(dataset_path, config.BATCH_SIZE, inception, device=config.DEVICE, num_workers=config.NUM_WORKERS)\n",
    "\n",
    "print(\"Real Embeddings:\", real_embeddings.shape)\n",
    "\n",
    "# Extract features from generated images on disk\n",
    "gen_embeddings = get_inception_features_from_files(\n",
    "    saved_samples=saved_samples,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    model=inception,\n",
    "    transform=inception_transform,\n",
    "    device=config.DEVICE,\n",
    "    num_workers=config.NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(\"Generated Embeddings:\", gen_embeddings.shape)\n",
    "\n",
    "# Compute FID Score: checks if the images look \"real\" compared to the original dataset\n",
    "fid_score = calculate_fid(real_embeddings, gen_embeddings)\n",
    "print(f\"FID Score: {fid_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ar3mr8TKnE7s"
   },
   "source": [
    "# Part 3: Embedding Analysis with FiftyOne Brain\n",
    "This section organizes the generated images into a structured FiftyOne dataset to analyze the model's internal behavior.\n",
    "Each image is paired with its corresponding prompt, guidance weight (w), and CLIP score, while the extracted U-Net bottleneck features are stored as vector embeddings.\n",
    "Afterwards, uniqueness (identifying visually distinct samples) and representativeness (identifying the most typical examples) is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-avsvL1efvng"
   },
   "outputs": [],
   "source": [
    "# Create a new FiftyOne dataset\n",
    "\n",
    "# Delete existing dataset if it exists\n",
    "if config.FIFTYONE_DATASET_NAME in fo.list_datasets():\n",
    "    print(f\"Deleting existing dataset: {config.FIFTYONE_DATASET_NAME}\")\n",
    "    fo.delete_dataset(config.FIFTYONE_DATASET_NAME)\n",
    "\n",
    "dataset = fo.Dataset(name=config.FIFTYONE_DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mmtPSvEfLEt"
   },
   "outputs": [],
   "source": [
    "# Build a FiftyOne dataset where each image is paired with prompt, guidance w,\n",
    "# CLIP score, and a flattened U-Net embedding (used for embedding-based analysis)\n",
    "samples = []\n",
    "\n",
    "print(\"Building FiftyOne dataset...\")\n",
    "\n",
    "assert len(saved_samples) == n_samples\n",
    "assert len(clip_scores) == n_samples\n",
    "assert extracted_embeddings.shape[0] >= n_samples  # hook may capture CFG-doubled batch\n",
    "\n",
    "for i, (filepath, prompt, w_val) in enumerate(saved_samples):\n",
    "    # FiftyOne Brain expects a 1D embedding vector per sample for distance computations\n",
    "    raw_embedding = extracted_embeddings[i]                 # e.g., (512, 8, 8)\n",
    "    flat_embedding = raw_embedding.flatten().cpu().numpy() # (512*8*8,)\n",
    "\n",
    "    sample = fo.Sample(filepath=filepath)\n",
    "\n",
    "    # Store fields for filtering and analysis in the FiftyOne App\n",
    "    sample[\"ground_truth\"] = fo.Classification(label=prompt)\n",
    "    sample[\"w\"] = float(w_val)\n",
    "    sample[\"clip_score\"] = float(clip_scores[i])\n",
    "    sample[\"unet_embedding\"] = flat_embedding\n",
    "\n",
    "    samples.append(sample)\n",
    "\n",
    "# Add all samples in one call for efficiency\n",
    "dataset.add_samples(samples)\n",
    "print(f\"Added {len(samples)} samples to the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0QBzJzlf4vs"
   },
   "outputs": [],
   "source": [
    "# Compute Uniqueness (Visual diversity)\n",
    "fob.compute_uniqueness(dataset, embeddings=\"unet_embedding\")\n",
    "\n",
    "# Compute Representativeness using the extracted U-Net embeddings\n",
    "fob.compute_representativeness(dataset, embeddings=\"unet_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hM1t2zpSf51b"
   },
   "outputs": [],
   "source": [
    "# Launch the FiftyOne App to visualize your dataset and analyze the results\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzGtpcpFMDKX"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "##### 1. Consistency Between Visuals and Metrics\n",
    "The results show a clear and monotonic relationship between guidance strength ùë§ and semantic alignment as measured by the CLIP score. As ùë§ increases, the generated images become progressively more aligned with their text prompts.\n",
    "\n",
    "**Observations:**\n",
    "* w ‚â§ 0 - clip score at 0.11-0.18:\n",
    "  * Images appear weakly related or unrelated to the text prompt\n",
    "  * Flowers look blurry, abstract, or ambiguous\n",
    "* w = 0 - clip score at 0.179:\n",
    "  * Images start to show recognizable flower-like structures\n",
    "  * Prompt-specific features (such as color or shape) become more consistent\n",
    "* w ‚â• 0 - clip score at 0.24-0.28:\n",
    "  * Images more clearly match the text prompt (e.g. roses, daisies, sunflowers become visually distinct)\n",
    "  * Color and structure are more stable across samples\n",
    "\n",
    "Overall, the quantitative CLIP scores are well aligned with qualitative visual inspection, indicating that the model successfully leverages text conditioning to guide semantic generation.\n",
    "\n",
    "##### 2. Diminishing Returns at High Guidance\n",
    "\n",
    "| Guidance w | Mean CLIP Score | n |\n",
    "|-----------:|---------------:|--:|\n",
    "| -2.0 | 0.1154 | 3 |\n",
    "| -1.0 | 0.1819 | 3 |\n",
    "| -0.5 | 0.1892 | 3 |\n",
    "|  0.0 | 0.1790 | 3 |\n",
    "|  0.5 | 0.2466 | 3 |\n",
    "|  1.0 | 0.2854 | 3 |\n",
    "|  2.0 | 0.2870 | 3 |\n",
    "\n",
    "Although CLIP scores increase with guidance strength, the improvement becomes marginal at higher values. This suggests that semantic alignment begins to saturate and that w‚âà1.0 to 2.0 might be the \"sweet spot\" for this model. Beyond this range, higher guidance primarily amplifies contrast or introduces artifacts rather than adding meaningful semantic information.\n",
    "\n",
    "##### 3. Overall CLIP Performance and Experimental Context\n",
    "Across all experiments, the average CLIP score is 0.2121, indicating a moderate but meaningful level of semantic alignment between generated images and their prompts. Given the experimental constraints‚Äîmost notably the low image resolution (32√ó32) and the inclusion of weak or negative guidance values‚Äîthis score is well within the expected range.\n",
    "\n",
    "Importantly, the goal of this assignment is not to maximize image quality, but to analyze how semantic alignment and diversity evolve as a function of guidance strength. Including negative and low guidance values provides insight into the trade-off between prompt adherence and sample diversity. This trade-off is further explored through U-Net embedding analysis and visualization with FiftyOne, offering a complementary perspective beyond CLIP scores alone.\n",
    "\n",
    "##### 4. Interpretation of the High FID Score\n",
    "The Fr√©chet Inception Distance (FID) score of approximately 320 is numerically high and would normally suggest poor generative quality. However, in this experimental setup, the absolute FID value is not a reliable indicator of visual or semantic quality.\n",
    "Small sample size (n=21) and the resolution mismatch (upsampling of 32x32 to 299x299) might influence the high score rather than just poor semantic quality.\n",
    "\n",
    "\n",
    "##### 5. Future adjustments to increase the quality of the generated images\n",
    "To further enhance the generative quality and semantic fidelity of the diffusion model, the following optimizations should be implemented:\n",
    "\n",
    "1. Extended Training & Dynamic Learning Rate: Extending the training of the original U-Net model and automatically reducing the learning rate with a dedicated scheduler that monitors the validation loss will enable the model to better capture high-frequency details and intricate textures. \n",
    "2. Prompt Variation & Semantic Generalization: Text prompts should be increased to include more semantic variation, e.g. \"A red rose with layered petals\" or \"A close-up of a daisy\". This creates a more robust mapping between CLIP text embeddings and visual features and ensures that the model can generalize to diverse descriptions.\n",
    "3. Hyperparameter Search for Guidance (w): A systematic search to find the optimal guidance weight would help to identifiy the point where prompt alignment is maximized before over-saturation begins to degrade image quality again.\n",
    "4. Targeted Quality Filtering: The final output should be filtered to retain only the images generated with the highest guidance strengths. This ensures that the final evaluation is only done on high-fidelity samples that possess clear structures and definitive subject matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CpRozR3ndFQ"
   },
   "source": [
    "# Part 4: Logging with Weights & Biases\n",
    "All experiments are logged to Weights & Biases for reproducibility and comparison. Hyperparameters, generated images, guidance values, CLIP scores, and embedding-based metrics are stored in a structured table, together with aggregate evaluation metrics such as average CLIP score and FID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQPywS8q7UrH"
   },
   "outputs": [],
   "source": [
    "# Load W&B API key from Colab Secrets and make it available as env variable\n",
    "wandb_key = userdata.get('WANDB_API_KEY')\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MW8i5M4cdokX"
   },
   "outputs": [],
   "source": [
    "# Initialize Run\n",
    "timestamp = other_utils.get_timestamp()\n",
    "run = wandb.init(project=\"diffusion_model_assessment_v2\", name=f\"experiment_run_{timestamp}\")\n",
    "\n",
    "# Log Hyperparameters\n",
    "wandb.config.update({\n",
    "    \"steps_T\": config.TIMESTEPS,\n",
    "    \"image_size\": config.IMG_SIZE,\n",
    "    \"clip_features\": config.CLIP_FEATURES,\n",
    "    \"prompts\": text_prompts\n",
    "})\n",
    "\n",
    "# Create a Table for Visual Results\n",
    "columns = [\"image generated\", \"prompt\", \"guidance_w\", \"clip_score\", \"uniqueness\", \"representativeness\"]\n",
    "\n",
    "diffusion_test_table = wandb.Table(columns=columns)\n",
    "\n",
    "# Populate Table\n",
    "# Grab uniqueness and representativeness scores back from FiftyOne\n",
    "uniqueness_scores = dataset.values(\"uniqueness\")\n",
    "representativeness_scores = dataset.values(\"representativeness\")\n",
    "\n",
    "for i, (filepath, prompt, w_val) in enumerate(saved_samples):\n",
    "    wandb_img = wandb.Image(filepath)\n",
    "\n",
    "    diffusion_test_table.add_data(\n",
    "        wandb_img,\n",
    "        prompt,\n",
    "        w_val,\n",
    "        clip_scores[i],\n",
    "        uniqueness_scores[i],\n",
    "        representativeness_scores[i],\n",
    "    )\n",
    "\n",
    "# Log the Table and Metrics\n",
    "wandb.log({\n",
    "    \"generation_results\": diffusion_test_table,\n",
    "    \"evaluation/fid_score\": fid_score,\n",
    "    \"evaluation/average_clip_score\": avg_clip_score\n",
    "    })\n",
    "\n",
    "# Finish\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LwjTns8ApEh"
   },
   "source": [
    "# Publish Dataset on Hugging Face\n",
    "\n",
    "Here the FiftyOne dataset is exported and published to Hugging Face."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FCmMZnMApEi"
   },
   "outputs": [],
   "source": [
    "# Save FiftyOne dataset (images + metadata) to disk\n",
    "print(f\"Exporting dataset to {config.EXPORT_DIR}...\")\n",
    "\n",
    "dataset.export(\n",
    "    export_dir=str(config.EXPORT_DIR),\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    export_media=True, # This ensures the actual .png images are included\n",
    ")\n",
    "\n",
    "print(\"Export complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYJjK37EApEi"
   },
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"HF_TOKEN\"\n",
    "\n",
    "# Token needs to be stored in Colab Secrets\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "assert HF_TOKEN is not None, \"HF_TOKEN env var is not set!\"\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "api.upload_large_folder(\n",
    "    folder_path=f\"{config.EXPORT_DIR}\",\n",
    "    repo_id=\"mmarschn/generated_flowers_experiment\",      # ! must already exist on HF\n",
    "    repo_type=\"dataset\",\n",
    "    ignore_patterns=[\"*.ipynb_checkpoints\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
