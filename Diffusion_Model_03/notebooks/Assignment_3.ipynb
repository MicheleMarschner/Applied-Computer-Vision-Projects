{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qFjE18NApEW"
   },
   "source": [
    "# Text-to-Image Conditional Diffusion with Classifier-Free Guidance (CFG)\n",
    "\n",
    "This project trains and evaluates a text-to-image diffusion model on a cropped version of the TF Flowers dataset, with the goal of generating realistic flower images that can be steered by text prompts while still supporting unconditional generation.\n",
    "\n",
    "Our approach combines three components:\n",
    "* CLIP text encoder to convert prompts into semantic conditioning vectors\n",
    "* U-Netâ€“based DDPM to denoise and generate images through the reverse diffusion process\n",
    "* Classifier-Free Guidance (CFG) to strengthen text conditioning at sampling time without training a separate classifier\n",
    "\n",
    "\n",
    "We build an end-to-end evaluation pipeline for a pretrained text-to-image diffusion model:\n",
    "\n",
    "1. **Generation + Feature Extraction:** We generate flower images using CFG-based DDPM sampling conditioned on CLIP text embeddings, while extracting U-Net bottleneck features via forward hooks to analyze internal representations.\n",
    "\n",
    "2. **Quantitative Evaluation:** Generated samples are evaluated using:\n",
    "* CLIP Score to measure textâ€“image semantic alignment\n",
    "* FrÃ©chet Inception Distance (FID) to assess distribution-level realism\n",
    "\n",
    "3. **Representation Analysis and Visualization:** The extracted bottleneck embeddings are stored in a FiftyOne dataset to enable interactive exploration and to compute metrics such as uniqueness and representativeness, linking generation behavior to internal feature structure.\n",
    "\n",
    "4. **Experiment tracking (Weights & Biases):** For reproducibility and easy comparison across runs, we log all key hyperparameters, sampling settings, metrics, and evaluation outputs to the _diffusion-model-assessment-v2_ W&B project.\n",
    "\n",
    "**Extension (second experimental notebook)**\n",
    "Building on this evaluation pipeline, a second experimental notebook extends the sampling setup to generate higher-quality images. It explores improved sampling settings (e.g., stronger/optimized guidance, more samples per prompt, and/or refined sampling parameters) to increase visual fidelity while keeping the same conditioning and evaluation framework, enabling direct qualitative comparison to the baseline results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w79TQtD755-G"
   },
   "source": [
    "# Setup\n",
    "\n",
    "The project repository is mounted from Google Drive and added to the Python path to allow clean imports from the src module. The dataset is copied to the local Colab filesystem to improve I/O performance during training. \n",
    "\n",
    "All global settings (random seed, device selection, paths, batch sizes) are defined once and reused across the notebook to ensure consistency and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FizpBjUGApEX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/MicheleMarschner/Documents/Uni/WiSe_2025_26/Applied CV/Applied-Computer-Vision-Projects/Diffusion_Model_03\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "#  !! Change the following path if the project is located elsewhere (repeat in config.py)\n",
    "%cd \"/content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03\"    \n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfyAg0fuApEY"
   },
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%%capture\n",
    "%pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1-dJyKt0Y7Ka"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "import numpy as np\n",
    "from scipy.linalg import sqrtm\n",
    "from PIL import Image\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision.transforms.v2 as transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models import inception_v3, Inception_V3_Weights\n",
    "\n",
    "import clip\n",
    "import open_clip\n",
    "\n",
    "import wandb\n",
    "import fiftyone as fo\n",
    "import fiftyone.brain as fob\n",
    "from huggingface_hub import HfApi\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1w73asd8TRGP"
   },
   "outputs": [],
   "source": [
    "from utils import UNet_utils, ddpm_utils, other_utils, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTmFAsHQApEY"
   },
   "outputs": [],
   "source": [
    "!rm -rf /content/data\n",
    "!cp -r \"$config.DRIVE_ROOT/data\"* /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y_MPf0Igufth"
   },
   "outputs": [],
   "source": [
    "other_utils.set_seeds(config.SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJmpVRWfmTlJ"
   },
   "source": [
    "# Part 1: Generating Flower Images and Extracting U-Net Bottleneck Features\n",
    "\n",
    "This part of the project restores a pretrained CLIP-conditioned DDPM pipeline to (1) generate flower images from text prompts and (2) capture intermediate U-Net representations from the bottleneck via forward hooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMgyyw1ib5sh"
   },
   "source": [
    "## Reconstructing CLIP, DDPM and the sampling setup\n",
    "\n",
    "To ensure that sampling matches the original training regime, the CLIP text encoder and the DDPM diffusion process are reinitialized with the same diffusion hyperparameters (noise schedule and number of timesteps) as during training. \n",
    "\n",
    "Rebuilding these components is essential for producing samples that are compatible with the pretrained U-Net and therefore comparable across guidance strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vbo21wHmbEYL"
   },
   "outputs": [],
   "source": [
    "# Load CLIP for encoding the text prompts\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=config.DEVICE)\n",
    "clip_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtXC_GcibO3r"
   },
   "outputs": [],
   "source": [
    "# Re-initialize DDPM wrapper\n",
    "B_start = 0.0001\n",
    "B_end = 0.02\n",
    "B = torch.linspace(B_start, B_end, config.TIMESTEPS).to(config.DEVICE)\n",
    "\n",
    "ddpm = ddpm_utils.DDPM(B, config.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3qgBDweb0Nw"
   },
   "source": [
    "## Loading the pretrained U-Net\n",
    "\n",
    "Next, the U-Net architecture is instantiated and its trained weights are loaded from disk. The model is set to evaluation mode to disable training-time behavior (e.g., dropout-like effects) and to make generation deterministic given a fixed seed. This restored model serves as the backbone for all subsequent image synthesis and feature extraction. All images are resized to 32Ã—32 and normalized to the [-1, 1] range. \n",
    "\n",
    "No explicit train/validation split is used at this stage. Diffusion models learn a data distribution rather than a supervised mapping, so performance is best assessed through sample quality and distribution-level metrics (e.g., FID and CLIP Score).\n",
    "\n",
    "### U-Netâ€“DDPM architecture used in this project\n",
    "\n",
    "The model is a CLIP-conditioned DDPM U-Net that predicts noise at each diffusion step.\n",
    "* Encoder: residual conv stem + two downsampling blocks (Conv + GroupNorm + GELU + rearrangement pooling)\n",
    "* Bottleneck: low-res spatial feature map (e.g., 8Ã—8), optionally with self-attention.\n",
    "* Conditioning: sinusoidal timestep embeddings + projected CLIP text embeddings; injected via scaleâ€“shift modulation in the decoder; classifier-free guidance via random condition dropout (Bernoulli mask).\n",
    "* Decoder: nearest-neighbor upsampling + conv with skip connections; final conv outputs the RGB noise prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5AFckgFZIQs"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'UNet_utils' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Define the uNet Architecture\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m uNet_model = \u001b[43mUNet_utils\u001b[49m.UNet(\n\u001b[32m      3\u001b[39m     T=config.TIMESTEPS,\n\u001b[32m      4\u001b[39m     img_ch=config.IMG_CH,\n\u001b[32m      5\u001b[39m     img_size=config.IMG_SIZE,\n\u001b[32m      6\u001b[39m     down_chs=(\u001b[32m256\u001b[39m, \u001b[32m256\u001b[39m, \u001b[32m512\u001b[39m),\n\u001b[32m      7\u001b[39m     t_embed_dim=\u001b[32m8\u001b[39m,\n\u001b[32m      8\u001b[39m     c_embed_dim=config.CLIP_FEATURES\n\u001b[32m      9\u001b[39m ).to(config.DEVICE)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Load the model weights\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mNameError\u001b[39m: name 'UNet_utils' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the uNet Architecture\n",
    "uNet_model = UNet_utils.UNet(\n",
    "    T=config.TIMESTEPS,\n",
    "    img_ch=config.IMG_CH,\n",
    "    img_size=config.IMG_SIZE,\n",
    "    down_chs=(256, 256, 512),\n",
    "    t_embed_dim=8,\n",
    "    c_embed_dim=config.CLIP_FEATURES\n",
    ").to(config.DEVICE)\n",
    "\n",
    "print(\"Num params: \", sum(p.numel() for p in uNet_model.parameters()))\n",
    "\n",
    "# Load the model weights\n",
    "try:\n",
    "    uNet_model.load_state_dict(torch.load(config.UNET_MODEL_PATH))\n",
    "    print(\"Model weights loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Model weights not found.\")\n",
    "\n",
    "uNet_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation\n",
    "\n",
    "In this part of the project, we sample flower images from a pretrained CLIP-conditioned UNet-DDPM using classifier-free guidance (CFG). \n",
    "\n",
    "**Image generation (CFG sampling)**\n",
    "Given a small set of text prompts and a list of guidance weights ð‘¤, images are generated via a wrapper function (`sample_flowers_with_hook`) that:\n",
    "1. encodes prompts using CLIP, and\n",
    "2. runs CFG sampling through ddpm_utils.sample_w(...). \n",
    "\n",
    "The sampling setup produces one final image per (prompt, ð‘¤) pair (in our configuration: 3 prompts Ã— 7 guidance values = 21 images). \n",
    "\n",
    "**Bottleneck embedding extraction (forward hook)**\n",
    "\n",
    "To analyze internal representations, we register a forward hook on the U-Netâ€™s down2 module and store its activations during sampling. Because CFG internally doubles the batch (conditioned + unconditioned), the captured tensor may contain more entries than final outputs; we therefore keep only the first n_samples embeddings to align 1 embedding with 1 generated image. \n",
    "\n",
    "\n",
    "**Where outputs are stored**\n",
    "Generated images are mapped from [âˆ’1,1] to [0,1] and saved as PNG files to `config.SAVE_DIR` using the naming scheme: `flower_w{w:+.1f}_p{prompt_idx}_{i}.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Us9Y6ZV-ZrGM"
   },
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "text_prompts = [\n",
    "    \"A photo of a red rose\",\n",
    "    \"A photo of a white daisy\",\n",
    "    \"A photo of a yellow sunflower\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVTn_TznH1OT"
   },
   "outputs": [],
   "source": [
    "# Sanity check: Calculate how many images are to be generated\n",
    "# Guidance strengths for classifier-free guidance\n",
    "P = len(text_prompts)           # number of prompts\n",
    "W = len(config.W_TESTS)         # guidance values per prompt\n",
    "n_samples = P * W               # Total images generated: one per (prompt, guidance) pair\n",
    "                                # 7 guidance weights => 7 images per prompt\n",
    "\n",
    "print(\"Expected n_samples:\", n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bFhxjtKzZ3tb"
   },
   "outputs": [],
   "source": [
    "# Store intermediate feature maps extracted via forward hooks\n",
    "embeddings_storage = {}\n",
    "\n",
    "def get_embedding_hook(name):\n",
    "    \"\"\"\n",
    "    Creates a forward hook that stores the output of a given layer.\n",
    "\n",
    "    The output is detached from the computation graph to avoid\n",
    "    gradient tracking and reduce memory usage.\n",
    "    \"\"\"\n",
    "    def hook(model, input, output):\n",
    "        # We use .detach() to disconnect from the gradient graph (saves memory)\n",
    "        embeddings_storage[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "# Register a forward hook on the U-Net bottleneck layer\n",
    "uNet_model.down2.register_forward_hook(get_embedding_hook('down2'))\n",
    "print(\"Hook registered on model.down2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZla3_4IaIeI"
   },
   "outputs": [],
   "source": [
    "def sample_flowers_with_hook(text_list, model, ddpm, input_size, T, device, w_tests):\n",
    "    \"\"\"\n",
    "    Generates images from text prompts using classifier-free guided diffusion\n",
    "    while capturing intermediate U-Net embeddings via a forward hook.\n",
    "\n",
    "    Args:\n",
    "        text_list (list[str]): Text prompts used for conditioning.\n",
    "        model (nn.Module): Pretrained U-Net diffusion model.\n",
    "        ddpm: Diffusion process wrapper.\n",
    "        input_size (tuple): Spatial size of generated images.\n",
    "        T (int): Number of diffusion timesteps.\n",
    "        device (torch.device): Computation device.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Final generated images.\n",
    "        torch.Tensor: Stored intermediate diffusion states (for visualization).\n",
    "    \"\"\"\n",
    "    # Encode text prompts using CLIP\n",
    "    text_tokens = clip.tokenize(text_list).to(device)\n",
    "    c = clip_model.encode_text(text_tokens).float()\n",
    "\n",
    "    # Run diffusion sampling with classifier-free guidance\n",
    "    x_gen, x_gen_store = ddpm_utils.sample_w(model, ddpm, input_size, T, c, device, w_tests)\n",
    "\n",
    "    return x_gen, x_gen_store\n",
    "\n",
    "# Run the generation\n",
    "other_utils.set_seeds(config.SEED)\n",
    "\n",
    "print(\"Generating images...\")\n",
    "generated_images, _ = sample_flowers_with_hook(\n",
    "    text_list=text_prompts,\n",
    "    model=uNet_model,\n",
    "    ddpm=ddpm,\n",
    "    input_size=config.INPUT_SIZE,\n",
    "    T=config.TIMESTEPS,\n",
    "    device=config.DEVICE,\n",
    "    w_tests=config.W_TESTS\n",
    ")\n",
    "\n",
    "# Retrieve the embedding captured by the bottleneck hook\n",
    "extracted_embedding_all = embeddings_storage['down2']\n",
    "\n",
    "# Forward hooks capture both conditioned and unconditioned batches (CFG);\n",
    "# retain only the conditioned samples corresponding to the final images\n",
    "assert extracted_embedding_all.shape[0] >= n_samples, (\n",
    "    f\"embedding batch {extracted_embedding_all.shape[0]} < {n_samples}\"\n",
    ")\n",
    "extracted_embeddings = extracted_embedding_all[:n_samples]\n",
    "\n",
    "print(\"Using embeddings:\", extracted_embeddings.shape)\n",
    "print(f\"Generated {len(generated_images)} images.\")\n",
    "print(f\"Extracted embedding shape: {extracted_embedding_all.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DUxLPJ_44_z6"
   },
   "outputs": [],
   "source": [
    "# Visualize the generated samples in a grid: rows correspond to text prompts,\n",
    "# and columns show the effect of varying guidance strengths (w).\n",
    "other_utils.show_generated_images_grid(\n",
    "    generated_images,\n",
    "    prompts=text_prompts,\n",
    "    w_tests=config.W_TESTS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z0kPoXt8yVy"
   },
   "outputs": [],
   "source": [
    "# Save generated images to disk for downstream evaluation\n",
    "to_pil = ToPILImage()\n",
    "\n",
    "# Track saved image paths together with their prompts and guidance values\n",
    "saved_samples = []\n",
    "\n",
    "print(\"Saving images to disk...\")\n",
    "assert len(generated_images) == n_samples, (\n",
    "    f\"generated_images={len(generated_images)} != {n_samples}\"\n",
    ")\n",
    "\n",
    "for i, img_tensor in enumerate(generated_images):\n",
    "    # Recover prompt and guidance value from the sampling order\n",
    "    prompt = text_prompts[i % P]\n",
    "    w_val = config.W_TESTS[i // P]\n",
    "\n",
    "    # Map model output from [-1, 1] to [0, 1] for image saving and clip any artifacts that fell outside the valid range\n",
    "    img_norm = ((img_tensor + 1) / 2).clamp(0, 1).detach().cpu()\n",
    "    pil_img = to_pil(img_norm)\n",
    "\n",
    "    filename = os.path.join(\n",
    "        config.SAVE_DIR, f\"flower_w{w_val:+.1f}_p{i % P}_{i}.png\"\n",
    "    )\n",
    "    pil_img.save(filename)\n",
    "\n",
    "    saved_samples.append((filename, prompt, float(w_val)))\n",
    "\n",
    "print(\"All images saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bUm5iqaMmmE0"
   },
   "source": [
    "# Part 2: Evaluation with CLIP Score and FID\n",
    "In this section, we evaluate the generated flower images using CLIP Score and FrÃ©chet Inception Distance (FID), following the metrics defined in the assignment. Together, these measures capture (1) semantic alignment with the text prompt and (2) distribution-level realism compared to real flower images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ZhEq6gDsyBC"
   },
   "source": [
    "## CLIP Score (semantic alignment)\n",
    "\n",
    "CLIP Score measures how well a generated image matches its conditioning prompt. It answers the question: \"How accurately does the generated image depict the content described in the text prompt?\"\n",
    "\n",
    "We compute it as the cosine similarity between text and image embeddings produced by a pretrained OpenCLIP ViT-B-32 model. Before computing similarity, both embeddings are L2-normalized, so the score is a dot product in normalized embedding space. Higher CLIP scores indicate stronger promptâ€“image correspondence. Scores are computed for all generated images, enabling comparisons across prompts and different guidance strengths ð‘¤.\n",
    "\n",
    "A higher score indicates stronger semantic alignment. Scores are computed for all generated images, allowing comparison across different guidance strengths and prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1G3TVZwL-GTs"
   },
   "outputs": [],
   "source": [
    "# Initialize OpenCLIP model for CLIP-score evaluation\n",
    "clip_scorer, _, clip_preprocess_val = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\"\n",
    ")\n",
    "clip_scorer.to(config.DEVICE).eval()\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TddM8_umca36"
   },
   "outputs": [],
   "source": [
    "def calculate_clip_score(image_path, text_prompt, device=None):\n",
    "    \"\"\"\n",
    "    Computes a CLIP similarity score between an image and a text prompt.\n",
    "\n",
    "    The image and text are embedded using a pretrained OpenCLIP model, L2-normalized,\n",
    "    and compared via cosine similarity (dot product of normalized embeddings).\n",
    "\n",
    "    Args:\n",
    "        image_path (str | Path): Path to the image file on disk.\n",
    "        text_prompt (str): Text prompt to compare against.\n",
    "\n",
    "    Returns:\n",
    "        float: Cosine similarity score (higher means stronger semantic alignment).\n",
    "    \"\"\"\n",
    "    # Preprocess and move to the same device as the CLIP model\n",
    "    image = clip_preprocess_val(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "    text = tokenizer([text_prompt]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = clip_scorer.encode_image(image)\n",
    "        text_features = clip_scorer.encode_text(text)\n",
    "\n",
    "        # Normalize to turn dot product into cosine similarity\n",
    "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        score = (image_features @ text_features.T).item()\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# Compute CLIP scores for all generated samples\n",
    "clip_scores = []\n",
    "\n",
    "print(\"Calculating scores...\")\n",
    "\n",
    "for i, (filepath, prompt, w_val) in enumerate(saved_samples):\n",
    "    score = calculate_clip_score(\n",
    "        image_path=filepath,\n",
    "        text_prompt=prompt,\n",
    "        device=config.DEVICE,\n",
    "    )\n",
    "    clip_scores.append(score)\n",
    "\n",
    "avg_clip_score = float(np.mean(clip_scores))\n",
    "print(f\"Average CLIP Score: {avg_clip_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoT9UPO84K9u"
   },
   "outputs": [],
   "source": [
    "# Aggregate CLIP scores by guidance strength w and compute mean score per w\n",
    "by_w = defaultdict(list)\n",
    "for (filepath, prompt, w_val), score in zip(saved_samples, clip_scores):\n",
    "    by_w[w_val].append(score)\n",
    "\n",
    "print(\"Show aggregated CLIP scores by guidance strength w\")\n",
    "print(f\"{'w':>6} | {'Mean CLIP':>10} | {'n':>3}\")\n",
    "print(\"-\" * 26)\n",
    "\n",
    "for w in sorted(by_w):\n",
    "    mean_score = np.mean(by_w[w])\n",
    "    n = len(by_w[w])\n",
    "    print(f\"{w:>6.1f} | {mean_score:>10.4f} | {n:>3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hKCxRoHDwMdK"
   },
   "outputs": [],
   "source": [
    "# example image of a real red rose\n",
    "real_rose_path = (\n",
    "    Path(config.TMP_ROOT)\n",
    "    / \"data/cropped_flowers/roses/15032112248_30c5284e54_n.jpg\"\n",
    ")\n",
    "\n",
    "other_utils.compare_generated_vs_real_roses(\n",
    "    generated_images,\n",
    "    prompt_idx=0,  # \"A photo of a red rose\"\n",
    "    prompts=text_prompts,\n",
    "    w_tests=config.W_TESTS,\n",
    "    real_rose_path=real_rose_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IY1X2qd4s0BT"
   },
   "source": [
    "## FrÃ©chet Inception Distance (FID) (distribution realism)\n",
    "\n",
    "FID measures how close the distribution of generated images is to the distribution of real images. \n",
    "\n",
    "We compute FID using 2048-dimensional feature vectors extracted from a pretrained InceptionV3 network, where the classification head is replaced by an identity layer to access pooled features. Real images are loaded from disk, while generated samples are read from the saved PNG outputs.\n",
    "\n",
    "For a fair comparison, both real and generated images are processed identically: they are resized to 299Ã—299 and normalized with ImageNet mean and standard deviation (from [0,1]) before being passed through InceptionV3. FID is then computed by comparing the mean and covariance of Inception features for real vs. generated samples; lower values indicate more realistic generations.\n",
    "\n",
    "**Practical note:** since the evaluation uses a small number of generated samples (here 21), FID estimates can be noisy and should be interpreted primarily for relative comparisons within the same experimental setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qfvm5w54KGxw"
   },
   "outputs": [],
   "source": [
    "# Load Pretrained InceptionV3;\n",
    "inception = inception_v3(\n",
    "    weights=Inception_V3_Weights.DEFAULT,\n",
    "    transform_input=False,\n",
    ").to(config.DEVICE)\n",
    "\n",
    "# To return features (2048) - not classes as the standard Inception model does - the final\n",
    "# \"Fully Connected\" layer needs to be replaced with a \"Pass Through\" (Identity)\n",
    "inception.fc = torch.nn.Identity()\n",
    "\n",
    "inception.eval()\n",
    "\n",
    "image_net_mean = [0.485, 0.456, 0.406]\n",
    "image_net_std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Inception expects 299x299 size instead of now 32x32 and specific normalization\n",
    "inception_transform = transforms.Compose([\n",
    "    transforms.Resize((config.INCEPTION_IMG_SIZE, config.INCEPTION_IMG_SIZE)), # Up-sample from 32x32\n",
    "    transforms.ToImage(),\n",
    "    transforms.ToDtype(torch.float32, scale=True),\n",
    "    transforms.Normalize(mean=image_net_mean, std=image_net_std)   # from original pytorch docs\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CresJrStNhT7"
   },
   "outputs": [],
   "source": [
    "def get_inception_features_from_raw(dataset_path, batch_size, model, device=None, num_workers=0):\n",
    "    \"\"\"\n",
    "    Extracts 2048-dimensional InceptionV3 feature embeddings for a dataset of images.\n",
    "\n",
    "    Args:\n",
    "        raw_dataset): dataset of the original images\n",
    "        model (nn.Module): Pretrained InceptionV3 feature extractor (fc = Identity).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (N, 2048) containing feature embeddings for all images.\n",
    "    \"\"\"\n",
    "    raw_dataset = other_utils.MyDataset(dataset_path, inception_transform, config.CLASSES)\n",
    "    raw_dataloader  = DataLoader(raw_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    features = []                         # Stores feature batches\n",
    "    with torch.no_grad():\n",
    "      for img, _ in tqdm(raw_dataloader):\n",
    "          img = img.to(device)\n",
    "          f = model(img)                # Runs Inception forward pass\n",
    "          features.append(f.cpu().numpy())  # Transform to numpy for later mathematical operations\n",
    "    return np.concatenate(features, axis=0) # Concatenate batches to one array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCvQK4HUUmfh"
   },
   "outputs": [],
   "source": [
    "def get_inception_features_from_files(saved_samples, batch_size, model, transform, device=None, num_workers=0):\n",
    "    \"\"\"\n",
    "    Loads images from disk and extracts InceptionV3 feature embeddings.\n",
    "\n",
    "    Args:\n",
    "        saved_samples: List of tuples containing image filepaths.\n",
    "        model: Pretrained feature extractor.\n",
    "        transform: Image preprocessing pipeline (resize/normalize).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Feature matrix of shape (N, 2048).\n",
    "    \"\"\"\n",
    "    dataset = other_utils.GeneratedListDataset(saved_samples, transform=transform)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    features = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img_batch in tqdm(loader, desc=\"Extracting Generated Features\"):\n",
    "            img_batch = img_batch.to(device)\n",
    "\n",
    "            # The transform handles Resize, Scale, and Normalize\n",
    "            f = model(img_batch)\n",
    "            features.append(f.cpu().numpy())\n",
    "\n",
    "    return np.concatenate(features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG9XOXQwgdNU"
   },
   "outputs": [],
   "source": [
    "def calculate_fid(real_embeddings, gen_embeddings):\n",
    "\n",
    "    # Calculate mean and covariance\n",
    "    mu1, sigma1 = real_embeddings.mean(axis=0), np.cov(real_embeddings, rowvar=False)\n",
    "    mu2, sigma2 = gen_embeddings.mean(axis=0), np.cov(gen_embeddings, rowvar=False)\n",
    "\n",
    "    # Sum squared difference between means\n",
    "    ssdiff = np.sum((mu1 - mu2)**2)\n",
    "\n",
    "    # Product of covariances\n",
    "    covmean = sqrtm(sigma1.dot(sigma2))\n",
    "\n",
    "    # Numerical error handling\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    # Final FID calculation\n",
    "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "\n",
    "    return fid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwuITZymtCOp"
   },
   "outputs": [],
   "source": [
    "# Extract features from real images on disk\n",
    "dataset_path = config.TMP_ROOT / \"data/cropped_flowers\"\n",
    "real_embeddings = get_inception_features_from_raw(dataset_path, config.BATCH_SIZE, inception, device=config.DEVICE, num_workers=config.NUM_WORKERS)\n",
    "\n",
    "print(\"Real Embeddings:\", real_embeddings.shape)\n",
    "\n",
    "# Extract features from generated images on disk\n",
    "gen_embeddings = get_inception_features_from_files(\n",
    "    saved_samples=saved_samples,\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    model=inception,\n",
    "    transform=inception_transform,\n",
    "    device=config.DEVICE,\n",
    "    num_workers=config.NUM_WORKERS\n",
    ")\n",
    "\n",
    "print(\"Generated Embeddings:\", gen_embeddings.shape)\n",
    "\n",
    "# Compute FID Score: checks if the images look \"real\" compared to the original dataset\n",
    "fid_score = calculate_fid(real_embeddings, gen_embeddings)\n",
    "print(f\"FID Score: {fid_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rzGtpcpFMDKX"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "##### 1. Consistency Between Visuals and Metrics\n",
    "The results show a clear and monotonic relationship between guidance strength ð‘¤ and semantic alignment as measured by the CLIP score. As ð‘¤ increases, the generated images become progressively more aligned with their text prompts.\n",
    "\n",
    "**Observations:**\n",
    "* w â‰¤ 0 - clip score at 0.11-0.18:\n",
    "  * Images appear weakly related or unrelated to the text prompt\n",
    "  * Flowers look blurry, abstract, or ambiguous\n",
    "* w = 0 - clip score at 0.179:\n",
    "  * Images start to show recognizable flower-like structures\n",
    "  * Prompt-specific features (such as color or shape) become more consistent\n",
    "* w â‰¥ 0 - clip score at 0.24-0.28:\n",
    "  * Images more clearly match the text prompt (e.g. roses, daisies, sunflowers become visually distinct)\n",
    "  * Color and structure are more stable across samples\n",
    "\n",
    "Overall, the quantitative CLIP scores are well aligned with qualitative visual inspection, indicating that the model successfully leverages text conditioning to guide semantic generation.\n",
    "\n",
    "##### 2. Diminishing Returns at High Guidance\n",
    "\n",
    "| Guidance w | Mean CLIP Score | n |\n",
    "|-----------:|---------------:|--:|\n",
    "| -2.0 | 0.1154 | 3 |\n",
    "| -1.0 | 0.1819 | 3 |\n",
    "| -0.5 | 0.1892 | 3 |\n",
    "|  0.0 | 0.1790 | 3 |\n",
    "|  0.5 | 0.2466 | 3 |\n",
    "|  1.0 | 0.2854 | 3 |\n",
    "|  2.0 | 0.2870 | 3 |\n",
    "\n",
    "Although CLIP scores increase with guidance strength, the improvement becomes marginal at higher values. This suggests that semantic alignment begins to saturate and that wâ‰ˆ1.0 to 2.0 might be the \"sweet spot\" for this model. Beyond this range, higher guidance primarily amplifies contrast or introduces artifacts rather than adding meaningful semantic information.\n",
    "\n",
    "##### 3. Overall CLIP Performance and Experimental Context\n",
    "Across all experiments, the average CLIP score is 0.2121, indicating a **moderate but meaningful level of semantic alignment** between generated images and their prompts. Given the experimental constraintsâ€”most notably the low image resolution (32Ã—32) and the inclusion of weak or negative guidance valuesâ€”this score is well within the expected range.\n",
    "\n",
    "Importantly, the goal of this assignment is not to maximize image quality, but to analyze how semantic alignment and diversity evolve as a function of guidance strength. Including negative and low guidance values provides insight into the trade-off between prompt adherence and sample diversity. This trade-off is further explored through U-Net embedding analysis and visualization with FiftyOne, offering a complementary perspective. \n",
    "\n",
    "##### 4. Interpretation of the High FID Score\n",
    "The FrÃ©chet Inception Distance (FID) score of approximately 320 is numerically high and would normally suggest poor generative quality. However, in this experimental setup, the absolute FID value is not a reliable indicator of visual or semantic quality.\n",
    "Small **sample size** (n=21) and the **resolution mismatch** (upsampling of 32x32 to 299x299) influence the high score rather than just poor semantic quality.\n",
    "\n",
    "\n",
    "##### 5. Future adjustments to increase the quality of the generated images\n",
    "To further enhance the generative quality and semantic fidelity of the diffusion model, the following optimizations should be implemented:\n",
    "\n",
    "1. Extended Training & Dynamic Learning Rate: Extending the training of the original U-Net model and automatically reducing the learning rate with a dedicated scheduler that monitors the validation loss will enable the model to better capture high-frequency details and intricate textures. \n",
    "2. Prompt Variation & Semantic Generalization: Text prompts should be increased to include more semantic variation, e.g. \"A red rose with layered petals\" or \"A close-up of a daisy\". This creates a more robust mapping between CLIP text embeddings and visual features and ensures that the model can generalize to diverse descriptions.\n",
    "3. Hyperparameter Search for Guidance (w): A systematic search to find the optimal guidance weight would help to identifiy the point where prompt alignment is maximized before over-saturation begins to degrade image quality again.\n",
    "4. Targeted Quality Filtering: The final output should be filtered to retain only the images generated with the highest guidance strengths. This ensures that the final evaluation is only done on high-fidelity samples that possess clear structures and definitive subject matter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ar3mr8TKnE7s"
   },
   "source": [
    "# Part 3: Embedding Analysis with FiftyOne Brain\n",
    "\n",
    "In this section, we analyze the internal representations of our pretrained UNet-DDPM using FiftyOne Brain. We build a FiftyOne dataset from the generated samples, and attach the relevant metadata to each sample:\n",
    "* **Prompt:** the text used for conditioning\n",
    "* **Guidance weight ð‘¤:** the CFG strength used during sampling\n",
    "* **CLIP Score:** semantic alignment between the generated image and its prompt\n",
    "* **U-Net bottleneck embedding:** intermediate features captured via a forward hook (from the UNetâ€™s down2 block)\n",
    "\n",
    "The extracted embeddings are stored directly as vector fields in the FiftyOne dataset (one embedding per image), enabling representation-based analysis.\n",
    "\n",
    "We then compute two FiftyOne Brain metrics on these embeddings:\n",
    "* **Uniqueness:** identifies samples that are most distinct relative to the rest (useful for spotting diverse or outlier generations)\n",
    "* **Representativeness:** identifies samples that best summarize the set (i.e., typical examples in embedding space)\n",
    "\n",
    "Finally, we launch the FiftyOne App for interactive explorationâ€”allowing us to visually inspect images alongside their prompts, ð‘¤, CLIP scores, and embedding-driven uniqueness/representativeness rankings, giving a qualitative view into how the modelâ€™s bottleneck features structure the generated set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-avsvL1efvng"
   },
   "outputs": [],
   "source": [
    "# Create a new FiftyOne dataset\n",
    "\n",
    "# Delete existing dataset if it exists\n",
    "if config.FIFTYONE_DATASET_NAME in fo.list_datasets():\n",
    "    print(f\"Deleting existing dataset: {config.FIFTYONE_DATASET_NAME}\")\n",
    "    fo.delete_dataset(config.FIFTYONE_DATASET_NAME)\n",
    "\n",
    "dataset = fo.Dataset(name=config.FIFTYONE_DATASET_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mmtPSvEfLEt"
   },
   "outputs": [],
   "source": [
    "# Build a FiftyOne dataset where each image is paired with prompt, guidance w,\n",
    "# CLIP score, and a flattened U-Net embedding (used for embedding-based analysis)\n",
    "samples = []\n",
    "\n",
    "print(\"Building FiftyOne dataset...\")\n",
    "\n",
    "assert len(saved_samples) == n_samples\n",
    "assert len(clip_scores) == n_samples\n",
    "assert extracted_embeddings.shape[0] >= n_samples  # hook may capture CFG-doubled batch\n",
    "\n",
    "for i, (filepath, prompt, w_val) in enumerate(saved_samples):\n",
    "    # FiftyOne Brain expects a 1D embedding vector per sample for distance computations\n",
    "    raw_embedding = extracted_embeddings[i]                 # e.g., (512, 8, 8)\n",
    "    flat_embedding = raw_embedding.flatten().cpu().numpy() # (512*8*8,)\n",
    "\n",
    "    sample = fo.Sample(filepath=filepath)\n",
    "\n",
    "    # Store fields for filtering and analysis in the FiftyOne App\n",
    "    sample[\"ground_truth\"] = fo.Classification(label=prompt)\n",
    "    sample[\"w\"] = float(w_val)\n",
    "    sample[\"clip_score\"] = float(clip_scores[i])\n",
    "    sample[\"unet_embedding\"] = flat_embedding\n",
    "\n",
    "    samples.append(sample)\n",
    "\n",
    "# Add all samples in one call for efficiency\n",
    "dataset.add_samples(samples)\n",
    "print(f\"Added {len(samples)} samples to the dataset.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j0QBzJzlf4vs"
   },
   "outputs": [],
   "source": [
    "# Compute Uniqueness (Visual diversity)\n",
    "fob.compute_uniqueness(dataset, embeddings=\"unet_embedding\")\n",
    "\n",
    "# Compute Representativeness using the extracted U-Net embeddings\n",
    "fob.compute_representativeness(dataset, embeddings=\"unet_embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hM1t2zpSf51b"
   },
   "outputs": [],
   "source": [
    "# Launch the FiftyOne App to visualize your dataset and analyze the results\n",
    "session = fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FiftyOne Screenshot of generated flowers](../results/assignment_3/overview_FiftyOne.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6CpRozR3ndFQ"
   },
   "source": [
    "# Part 4: Logging with Weights & Biases\n",
    "All experiments are logged to Weights & Biases (`diffusion-model-assessment-v2`) under the run name `experiment_run_TIMESTAMP` for reproducibility and comparison. Hyperparameters, generated images, guidance values, CLIP scores, and embedding-based metrics are stored in a structured table, together with aggregate evaluation metrics such as average CLIP score and FID.\n",
    "\n",
    "**Detailed Overview:**\n",
    "* Number of diffusion timesteps (T)\n",
    "* Image size (IMG_SIZE)\n",
    "* Clip Features\n",
    "* Prompts used for generation\n",
    "\n",
    "**Aggregate Metrics:**\n",
    "We log the final FID score and the average CLIP score across the FiftyOne dataset.\n",
    "\n",
    "**Per-sample results table:**\n",
    "We also create a W&B table for detailed inspection, with one row per generated image containing:\n",
    "* Generated image (with preview)\n",
    "* Text prompt\n",
    "* Guidance weight w\n",
    "* CLIP score\n",
    "* FiftyOne Brain uniqueness\n",
    "* FiftyOne Brain representativeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LQPywS8q7UrH"
   },
   "outputs": [],
   "source": [
    "# Load W&B API key from Colab Secrets and make it available as env variable\n",
    "wandb_key = userdata.get('WANDB_API_KEY')\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MW8i5M4cdokX"
   },
   "outputs": [],
   "source": [
    "# Initialize Run\n",
    "timestamp = other_utils.get_timestamp()\n",
    "run = wandb.init(project=\"diffusion_model_assessment_v2\", name=f\"experiment_run_{timestamp}\")\n",
    "\n",
    "# Log Hyperparameters\n",
    "wandb.config.update({\n",
    "    \"steps_T\": config.TIMESTEPS,\n",
    "    \"image_size\": config.IMG_SIZE,\n",
    "    \"clip_features\": config.CLIP_FEATURES,\n",
    "    \"prompts\": text_prompts\n",
    "})\n",
    "\n",
    "# Create a Table for Visual Results\n",
    "columns = [\"image generated\", \"prompt\", \"guidance_w\", \"clip_score\", \"uniqueness\", \"representativeness\"]\n",
    "\n",
    "diffusion_test_table = wandb.Table(columns=columns)\n",
    "\n",
    "# Populate Table\n",
    "# Grab uniqueness and representativeness scores back from FiftyOne\n",
    "uniqueness_scores = dataset.values(\"uniqueness\")\n",
    "representativeness_scores = dataset.values(\"representativeness\")\n",
    "\n",
    "for i, (filepath, prompt, w_val) in enumerate(saved_samples):\n",
    "    wandb_img = wandb.Image(filepath)\n",
    "\n",
    "    diffusion_test_table.add_data(\n",
    "        wandb_img,\n",
    "        prompt,\n",
    "        w_val,\n",
    "        clip_scores[i],\n",
    "        uniqueness_scores[i],\n",
    "        representativeness_scores[i],\n",
    "    )\n",
    "\n",
    "# Log the Table and Metrics\n",
    "wandb.log({\n",
    "    \"generation_results\": diffusion_test_table,\n",
    "    \"evaluation/fid_score\": fid_score,\n",
    "    \"evaluation/average_clip_score\": avg_clip_score\n",
    "    })\n",
    "\n",
    "# Finish\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3LwjTns8ApEh"
   },
   "source": [
    "# Part 5: Publish Dataset on Hugging Face\n",
    "\n",
    "The dataset is exported in FiftyOneâ€™s native format to a local directory (set via export_dir). This export produces a folder that includes the media files (data/) and the dataset metadata (samples.json, metadata.json), where any stored fields (e.g., CLIP score, uniqueness, representativeness, and embeddings) are preserved for later restoration.\n",
    "\n",
    "To use this in the project, the export path must be adapted to a valid location (e.g., under /content/ or config.DRIVE_ROOT). In addition, the desired metrics must already be present as sample fields in the FiftyOne dataset prior to export; otherwise they will not appear in the exported samples.json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-FCmMZnMApEi"
   },
   "outputs": [],
   "source": [
    "# Save FiftyOne dataset (images + metadata) to disk\n",
    "print(f\"Exporting dataset to {config.EXPORT_DIR}...\")\n",
    "\n",
    "dataset.export(\n",
    "    export_dir=str(config.EXPORT_DIR),\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    export_media=True, # This ensures the actual .png images are included\n",
    ")\n",
    "\n",
    "print(\"Export complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gYJjK37EApEi"
   },
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"HF_TOKEN\"\n",
    "\n",
    "# Token needs to be stored in Colab Secrets\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "assert HF_TOKEN is not None, \"HF_TOKEN env var is not set!\"\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "api.upload_large_folder(\n",
    "    folder_path=f\"{config.EXPORT_DIR}\",\n",
    "    repo_id=config.HF_BASE_REPO_ID,      # ! must already exist on HF\n",
    "    repo_type=\"dataset\",\n",
    "    ignore_patterns=[\"*.ipynb_checkpoints\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the HF dataset repo snapshot to a local cache directory\n",
    "local_dir = snapshot_download(\n",
    "    repo_id=config.HF_BASE_REPO_ID,\n",
    "    repo_type=\"dataset\",\n",
    ")\n",
    "\n",
    "# Name under which the dataset will be registered in FiftyOne\n",
    "dataset_name = config.FIFTYONE_DATASET_NAME\n",
    "if dataset_name in fo.list_datasets():\n",
    "    fo.delete_dataset(dataset_name)\n",
    "\n",
    "# Import the exported FiftyOneDataset from disk (expects samples.json, etc.)\n",
    "restored_dataset = fo.Dataset.from_dir(\n",
    "    dataset_dir=local_dir,\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    name=dataset_name,\n",
    ")\n",
    "\n",
    "# Launch the FiftyOne App\n",
    "print(restored_dataset)\n",
    "fo.launch_app(restored_dataset)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "aCV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
