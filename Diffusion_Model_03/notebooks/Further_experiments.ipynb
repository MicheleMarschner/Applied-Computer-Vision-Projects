{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qFjE18NApEW"
      },
      "source": [
        "# Experiment Goal: Optimizing Generative Evaluation via Sample Size Expansion and Semantic Refinement\n",
        "\n",
        "In this experiment, I focus on proving that even minimal adjustments ‚Äî such as increasing the sample size, refining text prompts, and filtering for high-quality results ‚Äî can significantly improve evaluation metrics.\n",
        "\n",
        "**Key Improvements:**\n",
        "* **Increased Sample Size:** Expanding the generated dataset by using 20 repetitions for each prompt stabilizes the covariance matrix calculations required for the FID score. This reduces the statistical noise caused by small sample sizes.\n",
        "* **High-Guidance Generation:** To reduce low-guidance artifacts, we directly generate samples at a higher guidance setting (w ‚â• 1.0) and evaluate metrics on the complete set of generated images.\n",
        "* **Prompt Engineering:** I extended the text prompts to include more detailed descriptions. Providing richer semantic information helps the CLIP model better align pixels with concepts, thereby increasing the CLIP score.\n",
        "\n",
        "**Expected Outcome:**\n",
        "Overall, these adjustments should lead to a significant reduction in the FID score (indicating better realism) and a higher, more stable average CLIP score (indicating better prompt adherence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w79TQtD755-G"
      },
      "source": [
        "# Setup\n",
        "\n",
        "The project repository is mounted from Google Drive and added to the Python path to allow clean imports from the src module. The dataset is copied to the local Colab filesystem to improve I/O performance during training. \n",
        "\n",
        "All global settings (random seed, device selection, paths, batch sizes) are defined once and reused across the notebook to ensure consistency and reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FizpBjUGApEX",
        "outputId": "60d35d1b-c3e5-4fbb-86b1-8524cef6e3de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    #  !! Change the following path if the project is located elsewhere (repeat in config.py)\n",
        "    %cd \"/content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03\"                    \n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TfyAg0fuApEY"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%%capture\n",
        "%pip install --no-cache-dir -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-dJyKt0Y7Ka",
        "outputId": "7ae6ee69-8172-46e6-e499-f1df321178aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/glob2/fnmatch.py:141: SyntaxWarning: invalid escape sequence '\\Z'\n",
            "  return '(?ms)' + res + '\\Z'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import clip\n",
        "import open_clip\n",
        "\n",
        "import wandb\n",
        "import fiftyone as fo\n",
        "import fiftyone.brain as fob\n",
        "\n",
        "from huggingface_hub import snapshot_download\n",
        "from huggingface_hub import HfApi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1w73asd8TRGP"
      },
      "outputs": [],
      "source": [
        "from utils import UNet_utils, ddpm_utils, other_utils, config, metrics, visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qTmFAsHQApEY"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/data\n",
        "!cp -r \"$config.DRIVE_ROOT/data\"* /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_MPf0Igufth",
        "outputId": "d33e5926-e820-4b0b-8521-490d82c783a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All random seeds set to 51 for reproducibility\n"
          ]
        }
      ],
      "source": [
        "other_utils.set_seeds(config.SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJmpVRWfmTlJ"
      },
      "source": [
        "# Part 1: Generating Flower Images and Extracting U-Net Bottleneck Features\n",
        "\n",
        "This part of the project restores a pretrained CLIP-conditioned DDPM pipeline to (1) generate flower images from text prompts and (2) capture intermediate U-Net representations from the bottleneck via forward hooks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMgyyw1ib5sh"
      },
      "source": [
        "## Reconstructing CLIP, DDPM and the sampling setup\n",
        "\n",
        "To ensure that sampling matches the original training regime, the CLIP text encoder and the DDPM diffusion process are reinitialized with the same diffusion hyperparameters (noise schedule and number of timesteps) as during training. \n",
        "\n",
        "Rebuilding these components is essential for producing samples that are compatible with the pretrained U-Net and therefore comparable across guidance strengths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vbo21wHmbEYL",
        "outputId": "71b85cef-e7fa-4054-bdb5-2727733261b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338M/338M [00:02<00:00, 120MiB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CLIP(\n",
              "  (visual): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (resblocks): Sequential(\n",
              "      (0): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (1): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (2): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (3): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (4): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (5): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (6): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (7): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (8): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (9): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (10): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (11): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (token_embedding): Embedding(49408, 512)\n",
              "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load CLIP for encoding the text prompts\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=config.DEVICE)\n",
        "clip_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jtXC_GcibO3r"
      },
      "outputs": [],
      "source": [
        "# Re-initialize DDPM wrapper\n",
        "B_start = 0.0001\n",
        "B_end = 0.02\n",
        "B = torch.linspace(B_start, B_end, config.TIMESTEPS).to(config.DEVICE)\n",
        "\n",
        "ddpm = ddpm_utils.DDPM(B, config.DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3qgBDweb0Nw"
      },
      "source": [
        "## Loading the pretrained U-Net\n",
        "\n",
        "Next, the U-Net architecture is instantiated and its trained weights are loaded from disk. The model is set to evaluation mode to disable training-time behavior (e.g., dropout-like effects) and to make generation deterministic given a fixed seed. This restored model serves as the backbone for all subsequent image synthesis and feature extraction. All images are resized to 32√ó32 and normalized to the [-1, 1] range. \n",
        "\n",
        "No explicit train/validation split is used at this stage. Diffusion models learn a data distribution rather than a supervised mapping, so performance is best assessed through sample quality and distribution-level metrics (e.g., FID and CLIP Score).\n",
        "\n",
        "### U-Net‚ÄìDDPM architecture used in this project\n",
        "\n",
        "The model is a CLIP-conditioned DDPM U-Net that predicts noise at each diffusion step.\n",
        "* Encoder: residual conv stem + two downsampling blocks (Conv + GroupNorm + GELU + rearrangement pooling)\n",
        "* Bottleneck: low-res spatial feature map (e.g., 8√ó8), optionally with self-attention.\n",
        "* Conditioning: sinusoidal timestep embeddings + projected CLIP text embeddings; injected via scale‚Äìshift modulation in the decoder; classifier-free guidance via random condition dropout (Bernoulli mask).\n",
        "* Decoder: nearest-neighbor upsampling + conv with skip connections; final conv outputs the RGB noise prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5AFckgFZIQs",
        "outputId": "4ab9f170-958b-4a6b-9bc5-9eef3ba7b7a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model weights loaded successfully.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (down0): ResidualConvBlock(\n",
              "    (conv1): GELUConvBlock(\n",
              "      (model): Sequential(\n",
              "        (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "        (2): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (conv2): GELUConvBlock(\n",
              "      (model): Sequential(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "        (2): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (down1): DownBlock(\n",
              "    (model): Sequential(\n",
              "      (0): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (2): RearrangePoolBlock(\n",
              "        (rearrange): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
              "        (conv): GELUConvBlock(\n",
              "          (model): Sequential(\n",
              "            (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (down2): DownBlock(\n",
              "    (model): Sequential(\n",
              "      (0): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (2): RearrangePoolBlock(\n",
              "        (rearrange): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
              "        (conv): GELUConvBlock(\n",
              "          (model): Sequential(\n",
              "            (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (to_vec): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): GELU(approximate='none')\n",
              "  )\n",
              "  (dense_emb): Sequential(\n",
              "    (0): Linear(in_features=32768, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=256, out_features=32768, bias=True)\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (sinusoidaltime): SinusoidalPositionEmbedBlock()\n",
              "  (t_emb1): EmbedBlock(\n",
              "    (model): Sequential(\n",
              "      (0): Linear(in_features=8, out_features=512, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (3): Unflatten(dim=1, unflattened_size=(512, 1, 1))\n",
              "    )\n",
              "  )\n",
              "  (t_emb2): EmbedBlock(\n",
              "    (model): Sequential(\n",
              "      (0): Linear(in_features=8, out_features=256, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): Unflatten(dim=1, unflattened_size=(256, 1, 1))\n",
              "    )\n",
              "  )\n",
              "  (c_embed1): EmbedBlock(\n",
              "    (model): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (3): Unflatten(dim=1, unflattened_size=(512, 1, 1))\n",
              "    )\n",
              "  )\n",
              "  (c_embed2): EmbedBlock(\n",
              "    (model): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): Unflatten(dim=1, unflattened_size=(256, 1, 1))\n",
              "    )\n",
              "  )\n",
              "  (up0): Sequential(\n",
              "    (0): Unflatten(dim=1, unflattened_size=(512, 8, 8))\n",
              "    (1): GELUConvBlock(\n",
              "      (model): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "        (2): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up1): UpBlock(\n",
              "    (model): Sequential(\n",
              "      (0): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (2): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (3): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (4): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up2): UpBlock(\n",
              "    (model): Sequential(\n",
              "      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (2): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (3): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (4): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (out): Sequential(\n",
              "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "    (2): ReLU()\n",
              "    (3): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the uNet Architecture\n",
        "uNet_model = UNet_utils.UNet(\n",
        "    T=config.TIMESTEPS,\n",
        "    img_ch=config.IMG_CH,\n",
        "    img_size=config.IMG_SIZE,\n",
        "    down_chs=(256, 256, 512),\n",
        "    t_embed_dim=8,\n",
        "    c_embed_dim=config.CLIP_FEATURES\n",
        ").to(config.DEVICE)\n",
        "\n",
        "\n",
        "# Load the model weights\n",
        "try:\n",
        "    uNet_model.load_state_dict(torch.load(config.UNET_MODEL_PATH))\n",
        "    print(\"Model weights loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Model weights not found.\")\n",
        "\n",
        "uNet_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Us9Y6ZV-ZrGM"
      },
      "outputs": [],
      "source": [
        "# Define prompts\n",
        "text_prompts = [\n",
        "    \"A photo of a red rose\",\n",
        "    \"A high-quality photo of a vibrant red rose\",\n",
        "    \"A close-up shot of a red rose with many layered petals\",\n",
        "    \"Macro photography of a deep red rose with velvet-like texture\",\n",
        "    \"A single red rose blooming in a lush green garden\",\n",
        "    \"A detailed photo of a red rose flower with morning dew drops\",\n",
        "    \"A vibrant red rose captured in bright, natural sunlight\",\n",
        "    \"A professional studio photograph of a single red rose against a soft background\",\n",
        "\n",
        "    \"A photo of a white daisy\",\n",
        "    \"A high-resolution photo of a white daisy with a yellow center\",\n",
        "    \"A round white daisy with crisp, clean white petals\",\n",
        "    \"A white daisy flower growing in a sunny green meadow\",\n",
        "    \"A macro photo of a daisy showing the texture of the yellow pollen center\",\n",
        "    \"A delicate white daisy captured in soft, diffused natural light\",\n",
        "    \"A top-down view of a symmetrical white daisy flower\",\n",
        "    \"A professional photograph of a simple white daisy with sharp focus\",\n",
        "\n",
        "    \"A photo of a yellow sunflower\",\n",
        "    \"A vibrant yellow sunflower with a large brown center\",\n",
        "    \"A detailed photo of a sunflower with bright, radiant yellow petals\",\n",
        "    \"A tall yellow sunflower standing in a vast sunflower field\",\n",
        "    \"A macro shot of a sunflower head showing the pattern of the seeds\",\n",
        "    \"A bright yellow sunflower facing the sun during golden hour\",\n",
        "    \"A large, blooming yellow sunflower with a thick green stem\",\n",
        "    \"A professional high-quality photo of a yellow sunflower with vivid colors\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVTn_TznH1OT",
        "outputId": "a9ebf992-a4c2-4df0-9921-e37152c3deac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected n_samples: 1680\n"
          ]
        }
      ],
      "source": [
        "# Sanity check: Calculate how many images are to be generated\n",
        "# Guidance strengths for classifier-free guidance\n",
        "P = len(text_prompts)           # number of prompts\n",
        "W = len(config.W_OPT)         # guidance values per prompt\n",
        "NUM_REPETITIONS = 20            # Number of copies of every prompt/guidance pair\n",
        "n_samples = P * W * NUM_REPETITIONS   # Total images generated: one per (prompt, guidance) pair\n",
        "\n",
        "print(\"Expected n_samples:\", n_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Generation\n",
        "\n",
        "In this part of the project, we sample flower images from a pretrained CLIP-conditioned UNet-DDPM using classifier-free guidance (CFG). \n",
        "\n",
        "**Image generation (CFG sampling)**\n",
        "Given a small set of text prompts and a list of guidance weights ùë§, images are generated via a wrapper function (`sample_flowers_with_hook`) that:\n",
        "1. encodes prompts using CLIP, and\n",
        "2. runs CFG sampling through ddpm_utils.sample_w(...). \n",
        "\n",
        "The sampling setup produces one final image per (prompt, ùë§) pair (in our configuration: 24 prompts √ó 3 guidance values * 20 repetitions = 1440 images). \n",
        "\n",
        "**Bottleneck embedding extraction (forward hook)**\n",
        "\n",
        "To analyze internal representations, we register a forward hook on the U-Net‚Äôs down2 module and store its activations during sampling. Because CFG internally doubles the batch (conditioned + unconditioned), the captured tensor may contain more entries than final outputs; we therefore keep only the first n_samples embeddings to align 1 embedding with 1 generated image. \n",
        "\n",
        "\n",
        "**Where outputs are stored**\n",
        "Generated images are mapped from [‚àí1,1] to [0,1] and saved as PNG files to `config.SAVE_DIR` using the naming scheme: `flower_w{w:+.1f}_p{prompt_idx}_{i}.png`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZla3_4IaIeI",
        "outputId": "fe785378-ab7a-4513-ff9b-2c79cd3a8ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All random seeds set to 51 for reproducibility\n",
            "Generating 1680 total images...\n",
            "Generating images...\n",
            "  Running repetition 1/10...\n",
            "  Running repetition 2/10...\n",
            "  Running repetition 3/10...\n",
            "  Running repetition 4/10...\n",
            "  Running repetition 5/10...\n",
            "  Running repetition 6/10...\n",
            "  Running repetition 7/10...\n",
            "  Running repetition 8/10...\n",
            "  Running repetition 9/10...\n",
            "  Running repetition 10/10...\n",
            "Generation Complete.\n",
            "Final Images Shape: torch.Size([1680, 3, 32, 32])\n",
            "Final Embeddings Shape: torch.Size([1680, 512, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "# Register a forward hook on the U-Net bottleneck layer\n",
        "uNet_model.down2.register_forward_hook(UNet_utils.get_embedding_hook('down2'))\n",
        "print(\"Hook registered on model.down2\")\n",
        "\n",
        "# Run the generation\n",
        "other_utils.set_seeds(config.SEED)\n",
        "\n",
        "print(f\"Generating {len(text_prompts) * len(config.W_OPT) * NUM_REPETITIONS} total images...\")\n",
        "\n",
        "print(\"Generating images...\")\n",
        "generated_images, extracted_embeddings = ddpm_utils.sample_flowers_with_hook(\n",
        "    text_list=text_prompts,\n",
        "    model=uNet_model,\n",
        "    clip_model=clip_model,\n",
        "    ddpm=ddpm,\n",
        "    input_size=config.INPUT_SIZE,\n",
        "    T=config.TIMESTEPS,\n",
        "    device=config.DEVICE,\n",
        "    w=config.W_OPT,\n",
        "    num_repetitions=NUM_REPETITIONS\n",
        ")\n",
        "\n",
        "print(f\"Generation Complete.\")\n",
        "print(f\"Final Images Shape: {generated_images.shape}\")\n",
        "print(f\"Final Embeddings Shape: {extracted_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z0kPoXt8yVy",
        "outputId": "f74e00f4-e419-4eb1-c69d-036b8cda9098"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving images to disk...\n",
            "All images saved.\n"
          ]
        }
      ],
      "source": [
        "# save generated flower images to disk\n",
        "saved_samples = other_utils.save_samples_to_disk(generated_images, text_prompts, config.W_OPT, config.SAVE_DIR, P, n_samples)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUm5iqaMmmE0"
      },
      "source": [
        "# Part 2: Evaluation with CLIP Score and FID\n",
        "In this section, we evaluate the generated flower images using CLIP Score and Fr√©chet Inception Distance (FID), following the metrics defined in the assignment. Together, these measures capture (1) semantic alignment with the text prompt and (2) distribution-level realism compared to real flower images.\n",
        "\n",
        "\n",
        "## CLIP Score (semantic alignment)\n",
        "\n",
        "CLIP Score measures how well a generated image matches its conditioning prompt. It answers the question: \"How accurately does the generated image depict the content described in the text prompt?\"\n",
        "\n",
        "We compute it as the cosine similarity between text and image embeddings produced by a pretrained OpenCLIP ViT-B-32 model. Before computing similarity, both embeddings are L2-normalized, so the score is a dot product in normalized embedding space. Higher CLIP scores indicate stronger prompt‚Äìimage correspondence. Scores are computed for all generated images, enabling comparisons across prompts and different guidance strengths ùë§.\n",
        "\n",
        "A higher score indicates stronger semantic alignment. Scores are computed for all generated images, allowing comparison across different guidance strengths and prompts.\n",
        "\n",
        "## Fr√©chet Inception Distance (FID) (distribution realism)\n",
        "\n",
        "FID measures how close the distribution of generated images is to the distribution of real images. \n",
        "\n",
        "We compute FID using 2048-dimensional feature vectors extracted from a pretrained InceptionV3 network, where the classification head is replaced by an identity layer to access pooled features. Real images are loaded from disk, while generated samples are read from the saved PNG outputs.\n",
        "\n",
        "For a fair comparison, both real and generated images are processed identically: they are resized to 299√ó299 and normalized with ImageNet mean and standard deviation (from [0,1]) before being passed through InceptionV3. FID is then computed by comparing the mean and covariance of Inception features for real vs. generated samples; lower values indicate more realistic generations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TddM8_umca36",
        "outputId": "b8535d8d-69ee-4ba4-d74d-b108b2688527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores...\n",
            "Average CLIP Score: 0.2594\n"
          ]
        }
      ],
      "source": [
        "# Initialize OpenCLIP model for CLIP-score evaluation\n",
        "clip_scorer, _, clip_preprocess_val = open_clip.create_model_and_transforms(\n",
        "    \"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\"\n",
        ")\n",
        "clip_scorer.to(config.DEVICE).eval()\n",
        "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
        "\n",
        "# Compute CLIP scores for all generated samples\n",
        "clip_scores = []\n",
        "\n",
        "print(\"Calculating scores...\")\n",
        "\n",
        "for i, (filepath, prompt, w_val) in enumerate(saved_samples):\n",
        "    score = metrics.calculate_clip_score(\n",
        "        clip_preprocess_val=clip_preprocess_val,\n",
        "        tokenizer=tokenizer,\n",
        "        clip_scorer=clip_scorer,\n",
        "        image_path=filepath,\n",
        "        text_prompt=prompt,\n",
        "        device=config.DEVICE,\n",
        "    )\n",
        "    clip_scores.append(score)\n",
        "\n",
        "avg_clip_score = float(np.mean(clip_scores))\n",
        "print(f\"Average CLIP Score: {avg_clip_score:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwuITZymtCOp",
        "outputId": "539ceecc-89ca-4806-f18f-593b3eace720"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 37/37 [02:57<00:00,  4.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real Embeddings: (1166, 2048)\n",
            "Total generated: 1680 | Realistic samples for FID: 480\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Generated Features: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [00:02<00:00,  5.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Embeddings: (480, 2048)\n",
            "FID Score: 239.2461\n"
          ]
        }
      ],
      "source": [
        "# Extract features from real images on disk\n",
        "dataset_path = config.TMP_ROOT / \"data/cropped_flowers\"\n",
        "\n",
        "# Compute FID Score: checks if the images look \"real\" compared to the original dataset\n",
        "fid_score = metrics.calculate_fid_score(saved_samples, dataset_path)\n",
        "print(f\"FID Score: {fid_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Evaluation\n",
        "\n",
        "To reduce statistical noise observed in earlier experiments, the evaluation pipeline was expanded from a small-scale snapshot of only 21 samples to 1,440 images (3 guidance weights, 24 prompts and 20 repetitions), enabling a more stable and reliable assessment (but overall still limited).\n",
        "\n",
        "| Metric          | Baseline (N = 21) | Optimized (N=1440) | Improvement |\n",
        "|-----------------|------------------:|-------------------:|------------:|\n",
        "| Avg. CLIP score | 0.212             | 0.259              | +22.1%      |\n",
        "| FID             | 320.5             | 230.2              | ‚àí28.2%      |\n",
        "\n",
        "**Analysis of Improvements**\n",
        "* The FID score dropped from 320.5 to 230.2. This improvement is primarily attributed to the increased sample mass (10 repetitions across 21 unique prompts). \n",
        "* Semantic Alignment (CLIP): The CLIP score rose from 0.212 to 0.259, indicating a stronger correlation between the pixels and text prompts. This jump is a direct result of replacing generic labels with descriptive prompts and exclude abstract and noisy outputs (where w<1.0). \n",
        "* Dataset Diversity: Utilizing 24 distinct prompts provided a wider feature set and allowed for a more comprehensive comparison against the real flower dataset.\n",
        "\n",
        "**Conclusion**\n",
        "The results prove that already minimal adjustments lead to a noticeable improvement in both scores. \n",
        "Low resolution and limited data are the main bottlenecks: the model captures the key, repeated cues of each flower type but misses fine textures and petal/leaf detail, and colors appear slightly oversaturated."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---------------------------------------------------------------------------------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar3mr8TKnE7s"
      },
      "source": [
        "# Part 3: Embedding Analysis with FiftyOne Brain\n",
        "\n",
        "In this section, we analyze the internal representations of our pretrained UNet-DDPM using FiftyOne Brain. We build a FiftyOne dataset from the generated samples, and attach the relevant metadata to each sample:\n",
        "* **Prompt:** the text used for conditioning\n",
        "* **Guidance weight ùë§:** the CFG strength used during sampling\n",
        "* **CLIP Score:** semantic alignment between the generated image and its prompt\n",
        "* **U-Net bottleneck embedding:** intermediate features captured via a forward hook (from the UNet‚Äôs down2 block)\n",
        "\n",
        "The extracted embeddings are stored directly as vector fields in the FiftyOne dataset (one embedding per image), enabling representation-based analysis.\n",
        "\n",
        "We then compute two FiftyOne Brain metrics on these embeddings:\n",
        "* **Uniqueness:** identifies samples that are most distinct relative to the rest (useful for spotting diverse or outlier generations)\n",
        "* **Representativeness:** identifies samples that best summarize the set (i.e., typical examples in embedding space)\n",
        "\n",
        "Finally, we launch the FiftyOne App for interactive exploration‚Äîallowing us to visually inspect images alongside their prompts, ùë§, CLIP scores, and embedding-driven uniqueness/representativeness rankings, giving a qualitative view into how the model‚Äôs bottleneck features structure the generated set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0QBzJzlf4vs",
        "outputId": "5be89cfb-2dec-4ecc-882a-89661460b53f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing uniqueness...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.uniqueness:Computing uniqueness...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uniqueness computation complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.uniqueness:Uniqueness computation complete\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing representativeness...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.representativeness:Computing representativeness...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing clusters for 480 embeddings; this may take awhile...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.representativeness:Computing clusters for 480 embeddings; this may take awhile...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Representativeness computation complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.representativeness:Representativeness computation complete\n"
          ]
        }
      ],
      "source": [
        "dataset = other_utils.create_FiftyOne_dataset(saved_samples, extracted_embeddings, clip_scores)\n",
        "\n",
        "# Compute Uniqueness (Visual diversity)\n",
        "fob.compute_uniqueness(dataset, embeddings=\"unet_embedding\")\n",
        "\n",
        "# Compute Representativeness using the extracted U-Net embeddings\n",
        "fob.compute_representativeness(dataset, embeddings=\"unet_embedding\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "hM1t2zpSf51b",
        "outputId": "6d3d0832-afc5-4539-81ad-b3633f43c39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r\n",
            "Could not connect session, trying again in 10 seconds\r\n",
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Client is not connected",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3995292868.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Launch the FiftyOne App to visualize your dataset and analyze the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fiftyone/core/session/session.py\u001b[0m in \u001b[0;36mlaunch_app\u001b[0;34m(dataset, view, sample_id, group_id, spaces, color_scheme, plots, port, address, remote, browser, height, auto, config)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \"\"\"\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_session\u001b[0m  \u001b[0;31m# pylint: disable=global-statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     _session = Session(\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fiftyone/core/session/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, view, sample_id, group_id, spaces, color_scheme, plots, port, address, remote, browser, height, auto, config, view_name)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_notebook_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fiftyone/core/session/session.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(session, *args, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mauto_show\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_notebook_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStateUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mauto_show\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_notebook_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fiftyone/core/session/session.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, height)\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fiftyone/core/session/session.py\u001b[0m in \u001b[0;36mfreeze\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeactivateNotebookCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fiftyone/core/session/client.py\u001b[0m in \u001b[0;36msend_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Client is not connected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Client is not connected"
          ]
        }
      ],
      "source": [
        "# Launch the FiftyOne App to visualize your dataset and analyze the results\n",
        "session = fo.launch_app(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rep_high_view = dataset.sort_by(\"representativeness\", reverse=True).limit(12)\n",
        "session.view = rep_high_view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rep_low_view = dataset.sort_by(\"representativeness\").limit(12)\n",
        "session.view = rep_low_view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "low_unique_view = dataset.sort_by(\"uniqueness\").limit(12)\n",
        "session.view = low_unique_view"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "high_unique_view = dataset.sort_by(\"uniqueness\", reverse=True).limit(12)\n",
        "session.view = high_unique_view"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Qualitative Analysis of FiftyOne Embedding Metrics\n",
        "\n",
        "With 1,440 generated samples, the FiftyOne Brain metrics provide a statistically meaningful view of how the U-Net bottleneck embeddings structure the generated image distribution.\n",
        "\n",
        "We eavluate the images by their representativeness (how central a sample lies within the embedding space) and uniqueness (how much samples deviate from nearby generations)\n",
        "\n",
        "When sorting the images by representativeness, clear patterns become visible.\n",
        "The most representative samples mainly show white daisies and yellow sunflowers.\n",
        "These flowers are centered in the image, have a clear circular shape, and look\n",
        "very similar across different generations. This indicates that the model can\n",
        "produce these flower types in a stable and consistent way.\n",
        "\n",
        "![High representativeness samples](../results/further_experiments/high_representativeness.png)\n",
        "\n",
        "The least representative samples are mostly red roses. While their color matches\n",
        "the text prompt well, the rose images vary strongly in shape and texture and often\n",
        "appear overly saturated or blurry. This higher variation places them further away\n",
        "from the center of the learned embedding space.\n",
        "\n",
        "![Low representativeness samples](../results/further_experiments/low_representativeness.png)\n",
        "\n",
        "The uniqueness views show a similar effect. Images with low uniqueness are almost\n",
        "identical red roses with the same size, position, and background, indicating\n",
        "very little variation. \n",
        "\n",
        "![Low uniqueness samples](../results/further_experiments/low_uniqueness.png)\n",
        "\n",
        "In contrast, highly unique samples show different image\n",
        "layouts, including changes in zoom level and images containing multiple flowers.\n",
        "In particular, images with two flowers are clearly more unique than those with\n",
        "only a single flower.\n",
        "\n",
        "![High uniqueness samples](../results/further_experiments/high_uniqueness.png)\n",
        "\n",
        "In addition, more detailed text prompts tend to produce images with higher\n",
        "uniqueness and higher representativeness than simple prompts. Richer descriptions\n",
        "encourage more consistent structure while still allowing visual variation,\n",
        "whereas short prompts often lead to repetitive compositions.\n",
        "\n",
        "Overall, the screenshots show that the model performs best on simple, centered,\n",
        "and symmetric flower shapes, while increased scene complexity and richer prompts\n",
        "introduce greater diversity in the generated results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CpRozR3ndFQ"
      },
      "source": [
        "# Part 4: Logging with Weights & Biases\n",
        "All experiments are logged to Weights & Biases (`diffusion-model-assessment-v2`) under the run name `experiment_run_TIMESTAMP` for reproducibility and comparison. Hyperparameters, generated images, guidance values, CLIP scores, and embedding-based metrics are stored in a structured table, together with aggregate evaluation metrics such as average CLIP score and FID.\n",
        "\n",
        "**Detailed Overview:**\n",
        "* Number of diffusion timesteps (T)\n",
        "* Image size (IMG_SIZE)\n",
        "* Clip Features\n",
        "* Prompts used for generation\n",
        "\n",
        "**Aggregate Metrics:**\n",
        "We log the final FID score and the average CLIP score across the FiftyOne dataset.\n",
        "\n",
        "**Per-sample results table:**\n",
        "We also create a W&B table for detailed inspection, with one row per generated image containing:\n",
        "* Generated image (with preview)\n",
        "* Text prompt\n",
        "* Guidance weight w\n",
        "* CLIP score\n",
        "* FiftyOne Brain uniqueness\n",
        "* FiftyOne Brain representativeness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQPywS8q7UrH",
        "outputId": "e6d07f64-ce3a-498b-8ccc-040afe053752"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichele-marschner\u001b[0m (\u001b[33mmichele-marschner-university-of-potsdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load W&B API key from Colab Secrets and make it available as env variable\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "MW8i5M4cdokX",
        "outputId": "8c5a365c-fd9c-4ee1-a85c-cbee3d02d2bb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03/wandb/run-20260106_173236-jt9b2tn3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2/runs/jt9b2tn3' target=\"_blank\">experiment_run_further_experiments_2026-01-06_17-32</a></strong> to <a href='https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2/runs/jt9b2tn3' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2/runs/jt9b2tn3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/average_clip_score</td><td>‚ñÅ</td></tr><tr><td>evaluation/fid_score</td><td>‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/average_clip_score</td><td>0.25942</td></tr><tr><td>evaluation/fid_score</td><td>239.24612</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_run_further_experiments_2026-01-06_17-32</strong> at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2/runs/jt9b2tn3' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2/runs/jt9b2tn3</a><br> View project at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2</a><br>Synced 4 W&B file(s), 1 media file(s), 482 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20260106_173236-jt9b2tn3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize Run\n",
        "timestamp = other_utils.get_timestamp()\n",
        "run = wandb.init(project=\"diffusion_model_assessment_v2\", name=f\"experiment_run_further_experiments_{timestamp}\")\n",
        "\n",
        "# Log Hyperparameters\n",
        "wandb.config.update({\n",
        "    \"steps_T\": config.TIMESTEPS,\n",
        "    \"image_size\": config.IMG_SIZE,\n",
        "    \"clip_features\": config.CLIP_FEATURES,\n",
        "    \"prompts\": text_prompts\n",
        "})\n",
        "\n",
        "# Create a Table for Visual Results\n",
        "columns = [\"image generated\", \"prompt\", \"guidance_w\", \"clip_score\", \"uniqueness\", \"representativeness\"]\n",
        "\n",
        "diffusion_test_table = wandb.Table(columns=columns)\n",
        "\n",
        "# Populate Table\n",
        "# Grab uniqueness and representativeness scores back from FiftyOne\n",
        "uniqueness_scores = dataset.values(\"uniqueness\")\n",
        "representativeness_scores = dataset.values(\"representativeness\")\n",
        "\n",
        "for i, (filepath, prompt, w_val) in enumerate(saved_samples):\n",
        "    wandb_img = wandb.Image(filepath)\n",
        "\n",
        "    diffusion_test_table.add_data(\n",
        "        wandb_img,\n",
        "        prompt,\n",
        "        w_val,\n",
        "        clip_scores[i],\n",
        "        uniqueness_scores[i],\n",
        "        representativeness_scores[i],\n",
        "    )\n",
        "\n",
        "# Log the Table and Metrics\n",
        "wandb.log({\n",
        "    \"generation_results\": diffusion_test_table,\n",
        "    \"evaluation/fid_score\": fid_score,\n",
        "    \"evaluation/average_clip_score\": avg_clip_score\n",
        "    })\n",
        "\n",
        "# Finish\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Part 5: Publish Dataset on Hugging Face\n",
        "\n",
        "The dataset is exported in FiftyOne‚Äôs native format to a local directory (set via export_dir). This export produces a folder that includes the media files (data/) and the dataset metadata (samples.json, metadata.json), where any stored fields (e.g., CLIP score, uniqueness, representativeness, and embeddings) are preserved for later restoration.\n",
        "\n",
        "To use this in the project, the export path must be adapted to a valid location (e.g., under /content/ or config.DRIVE_ROOT). In addition, the desired metrics must already be present as sample fields in the FiftyOne dataset prior to export; otherwise they will not appear in the exported samples.json."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save FiftyOne dataset (images + metadata) to disk\n",
        "print(f\"Exporting dataset to {config.EXPORT_DIR}...\")\n",
        "\n",
        "dataset.export(\n",
        "    export_dir=str(config.EXPORT_DIR),\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    export_media=True, # This ensures the actual .png images are included\n",
        ")\n",
        "\n",
        "print(\"Export complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ[\"HF_TOKEN\"] = \"HF_TOKEN\"\n",
        "\n",
        "# Token needs to be stored in Colab Secrets\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "assert HF_TOKEN is not None, \"HF_TOKEN env var is not set!\"\n",
        "\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "\n",
        "api.upload_large_folder(\n",
        "    folder_path=f\"{config.EXPORT_DIR}\",\n",
        "    repo_id=config.HF_EXPERIMENT_REPO_ID,      # ! must already exist on HF\n",
        "    repo_type=\"dataset\",\n",
        "    ignore_patterns=[\"*.ipynb_checkpoints\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download the HF dataset repo snapshot to a local cache directory\n",
        "local_dir = snapshot_download(\n",
        "    repo_id=config.HF_EXPERIMENT_REPO_ID,\n",
        "    repo_type=\"dataset\",\n",
        ")\n",
        "\n",
        "# Name under which the dataset will be registered in FiftyOne\n",
        "dataset_name = config.FIFTYONE_DATASET_EXPERIMENTS_NAME\n",
        "if dataset_name in fo.list_datasets():\n",
        "    fo.delete_dataset(dataset_name)\n",
        "\n",
        "# Import the exported FiftyOneDataset from disk (expects samples.json, etc.)\n",
        "restored_dataset = fo.Dataset.from_dir(\n",
        "    dataset_dir=local_dir,\n",
        "    dataset_type=fo.types.FiftyOneDataset,\n",
        "    name=dataset_name,\n",
        ")\n",
        "\n",
        "# Launch the FiftyOne App\n",
        "print(restored_dataset)\n",
        "fo.launch_app(restored_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aCV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
