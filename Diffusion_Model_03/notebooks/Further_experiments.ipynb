{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qFjE18NApEW"
      },
      "source": [
        "# Experiment Goal: Optimizing Generative Evaluation via Sample Size Expansion and Semantic Refinement\n",
        "\n",
        "In this experiment, I focus on proving that even minimal adjustments — such as increasing the sample size, refining text prompts, and filtering for high-quality results — can significantly improve evaluation metrics.\n",
        "\n",
        "**Key Improvements:**\n",
        "* Increased Sample Size: Expanding the generated dataset by using 10 repetitions for each prompt stabilizes the covariance matrix calculations required for the FID score. This reduces the statistical noise caused by small sample sizes.\n",
        "* Strategic Filtering: Metrics are now calculated only on a subset of images with a guidance weight of w ≥ 1.0. This excludes the abstract noise generated by low or negative guidance weights.\n",
        "* Prompt Engineering: I extended the text prompts to include more detailed descriptions. Providing richer semantic information helps the CLIP model better align pixels with concepts, thereby increasing the CLIP score.\n",
        "\n",
        "**Expected Outcome:**\n",
        "Overall, these adjustments should lead to a significant reduction in the FID score (indicating better realism) and a higher, more stable average CLIP score (indicating better prompt adherence)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w79TQtD755-G"
      },
      "source": [
        "# Setup\n",
        "\n",
        "The project repository is mounted from Google Drive and added to the Python path to allow clean imports from the src module. The dataset is copied to the local Colab filesystem to improve I/O performance during training. All global settings (random seed, device selection, paths, batch sizes) are defined once and reused across the notebook to ensure consistency and reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FizpBjUGApEX",
        "outputId": "60d35d1b-c3e5-4fbb-86b1-8524cef6e3de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    %cd \"/content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03\"\n",
        "\n",
        "PROJECT_ROOT = Path.cwd()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.append(str(PROJECT_ROOT))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TfyAg0fuApEY"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "%%capture\n",
        "%pip install --no-cache-dir -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1-dJyKt0Y7Ka",
        "outputId": "7ae6ee69-8172-46e6-e499-f1df321178aa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
            "/usr/local/lib/python3.12/dist-packages/glob2/fnmatch.py:141: SyntaxWarning: invalid escape sequence '\\Z'\n",
            "  return '(?ms)' + res + '\\Z'\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torchvision.transforms.v2 as transforms\n",
        "from torchvision.transforms import ToPILImage\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import inception_v3, Inception_V3_Weights\n",
        "\n",
        "import clip\n",
        "import open_clip\n",
        "\n",
        "import wandb\n",
        "import fiftyone as fo\n",
        "import fiftyone.brain as fob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1w73asd8TRGP"
      },
      "outputs": [],
      "source": [
        "from utils import UNet_utils, ddpm_utils, other_utils, config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "qTmFAsHQApEY"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/data\n",
        "!cp -r \"$config.DRIVE_ROOT/data\"* /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y_MPf0Igufth",
        "outputId": "d33e5926-e820-4b0b-8521-490d82c783a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All random seeds set to 51 for reproducibility\n"
          ]
        }
      ],
      "source": [
        "other_utils.set_seeds(config.SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJmpVRWfmTlJ"
      },
      "source": [
        "# Part 1: Image Generation and Embedding Extraction\n",
        "\n",
        "In this section, you will load the pre-trained U-Net model from notebook 05_CLIP.ipynb of the corresponding NVIDIA course, generate images of flowers, and extract embeddings from the model's bottleneck."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMgyyw1ib5sh"
      },
      "source": [
        "## Recreate CLIP + DDPM + sampling\n",
        "\n",
        "In this section, the CLIP text encoder and the DDPM sampling process are reinitialized to match the training setup. The noise schedule and diffusion parameters are defined to ensure compatibility with the pretrained U-Net. This reconstruction is necessary to generate images that are consistent with the original training regime."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vbo21wHmbEYL",
        "outputId": "71b85cef-e7fa-4054-bdb5-2727733261b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 120MiB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "CLIP(\n",
              "  (visual): VisionTransformer(\n",
              "    (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
              "    (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "    (transformer): Transformer(\n",
              "      (resblocks): Sequential(\n",
              "        (0): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): ResidualAttentionBlock(\n",
              "          (attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (mlp): Sequential(\n",
              "            (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (gelu): QuickGELU()\n",
              "            (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          )\n",
              "          (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (resblocks): Sequential(\n",
              "      (0): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (1): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (2): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (3): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (4): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (5): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (6): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (7): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (8): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (9): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (10): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "      (11): ResidualAttentionBlock(\n",
              "        (attn): MultiheadAttention(\n",
              "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): Sequential(\n",
              "          (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
              "          (gelu): QuickGELU()\n",
              "          (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        )\n",
              "        (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (token_embedding): Embedding(49408, 512)\n",
              "  (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              ")"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load CLIP for encoding the text prompts\n",
        "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=config.DEVICE)\n",
        "clip_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "jtXC_GcibO3r"
      },
      "outputs": [],
      "source": [
        "# Re-initialize DDPM wrapper\n",
        "B_start = 0.0001\n",
        "B_end = 0.02\n",
        "B = torch.linspace(B_start, B_end, config.TIMESTEPS).to(config.DEVICE)\n",
        "\n",
        "ddpm = ddpm_utils.DDPM(B, config.DEVICE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3qgBDweb0Nw"
      },
      "source": [
        "## Load the pre-trained U-Net\n",
        "\n",
        "Here, the U-Net architecture is instantiated and pretrained weights are loaded from disk. The model is switched to evaluation mode to disable training-specific behavior. This step restores the trained generative model used for all subsequent image synthesis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5AFckgFZIQs",
        "outputId": "4ab9f170-958b-4a6b-9bc5-9eef3ba7b7a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model weights loaded successfully.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "UNet(\n",
              "  (down0): ResidualConvBlock(\n",
              "    (conv1): GELUConvBlock(\n",
              "      (model): Sequential(\n",
              "        (0): Conv2d(3, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "        (2): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "    (conv2): GELUConvBlock(\n",
              "      (model): Sequential(\n",
              "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "        (2): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (down1): DownBlock(\n",
              "    (model): Sequential(\n",
              "      (0): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (2): RearrangePoolBlock(\n",
              "        (rearrange): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
              "        (conv): GELUConvBlock(\n",
              "          (model): Sequential(\n",
              "            (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (down2): DownBlock(\n",
              "    (model): Sequential(\n",
              "      (0): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (1): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (2): RearrangePoolBlock(\n",
              "        (rearrange): Rearrange('b c (h p1) (w p2) -> b (c p1 p2) h w', p1=2, p2=2)\n",
              "        (conv): GELUConvBlock(\n",
              "          (model): Sequential(\n",
              "            (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "            (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "            (2): GELU(approximate='none')\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (to_vec): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): GELU(approximate='none')\n",
              "  )\n",
              "  (dense_emb): Sequential(\n",
              "    (0): Linear(in_features=32768, out_features=256, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=256, out_features=32768, bias=True)\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (sinusoidaltime): SinusoidalPositionEmbedBlock()\n",
              "  (t_emb1): EmbedBlock(\n",
              "    (model): Sequential(\n",
              "      (0): Linear(in_features=8, out_features=512, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (3): Unflatten(dim=1, unflattened_size=(512, 1, 1))\n",
              "    )\n",
              "  )\n",
              "  (t_emb2): EmbedBlock(\n",
              "    (model): Sequential(\n",
              "      (0): Linear(in_features=8, out_features=256, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): Unflatten(dim=1, unflattened_size=(256, 1, 1))\n",
              "    )\n",
              "  )\n",
              "  (c_embed1): EmbedBlock(\n",
              "    (model): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
              "      (3): Unflatten(dim=1, unflattened_size=(512, 1, 1))\n",
              "    )\n",
              "  )\n",
              "  (c_embed2): EmbedBlock(\n",
              "    (model): Sequential(\n",
              "      (0): Linear(in_features=512, out_features=256, bias=True)\n",
              "      (1): GELU(approximate='none')\n",
              "      (2): Linear(in_features=256, out_features=256, bias=True)\n",
              "      (3): Unflatten(dim=1, unflattened_size=(256, 1, 1))\n",
              "    )\n",
              "  )\n",
              "  (up0): Sequential(\n",
              "    (0): Unflatten(dim=1, unflattened_size=(512, 8, 8))\n",
              "    (1): GELUConvBlock(\n",
              "      (model): Sequential(\n",
              "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "        (1): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
              "        (2): GELU(approximate='none')\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up1): UpBlock(\n",
              "    (model): Sequential(\n",
              "      (0): ConvTranspose2d(1024, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (2): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (3): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (4): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (up2): UpBlock(\n",
              "    (model): Sequential(\n",
              "      (0): ConvTranspose2d(512, 256, kernel_size=(2, 2), stride=(2, 2))\n",
              "      (1): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (2): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (3): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "      (4): GELUConvBlock(\n",
              "        (model): Sequential(\n",
              "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
              "          (2): GELU(approximate='none')\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (out): Sequential(\n",
              "    (0): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): GroupNorm(8, 256, eps=1e-05, affine=True)\n",
              "    (2): ReLU()\n",
              "    (3): Conv2d(256, 3, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Define the uNet Architecture\n",
        "uNet_model = UNet_utils.UNet(\n",
        "    T=config.TIMESTEPS,\n",
        "    img_ch=config.IMG_CH,\n",
        "    img_size=config.IMG_SIZE,\n",
        "    down_chs=(256, 256, 512),\n",
        "    t_embed_dim=8,\n",
        "    c_embed_dim=config.CLIP_FEATURES\n",
        ").to(config.DEVICE)\n",
        "\n",
        "\n",
        "# Load the model weights\n",
        "try:\n",
        "    uNet_model.load_state_dict(torch.load(config.UNET_MODEL_PATH))\n",
        "    print(\"Model weights loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Model weights not found.\")\n",
        "\n",
        "uNet_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Us9Y6ZV-ZrGM"
      },
      "outputs": [],
      "source": [
        "# Define prompts\n",
        "text_prompts = [\n",
        "    \"A photo of a red rose\",\n",
        "    \"A high-quality photo of a vibrant red rose\",\n",
        "    \"A close-up shot of a red rose with many layered petals\",\n",
        "    \"Macro photography of a deep red rose with velvet-like texture\",\n",
        "    \"A single red rose blooming in a lush green garden\",\n",
        "    \"A detailed photo of a red rose flower with morning dew drops\",\n",
        "    \"A vibrant red rose captured in bright, natural sunlight\",\n",
        "    \"A professional studio photograph of a single red rose against a soft background\",\n",
        "\n",
        "    \"A photo of a white daisy\",\n",
        "    \"A high-resolution photo of a white daisy with a yellow center\",\n",
        "    \"A round white daisy with crisp, clean white petals\",\n",
        "    \"A white daisy flower growing in a sunny green meadow\",\n",
        "    \"A macro photo of a daisy showing the texture of the yellow pollen center\",\n",
        "    \"A delicate white daisy captured in soft, diffused natural light\",\n",
        "    \"A top-down view of a symmetrical white daisy flower\",\n",
        "    \"A professional photograph of a simple white daisy with sharp focus\",\n",
        "\n",
        "    \"A photo of a yellow sunflower\",\n",
        "    \"A vibrant yellow sunflower with a large brown center\",\n",
        "    \"A detailed photo of a sunflower with bright, radiant yellow petals\",\n",
        "    \"A tall yellow sunflower standing in a vast sunflower field\",\n",
        "    \"A macro shot of a sunflower head showing the pattern of the seeds\",\n",
        "    \"A bright yellow sunflower facing the sun during golden hour\",\n",
        "    \"A large, blooming yellow sunflower with a thick green stem\",\n",
        "    \"A professional high-quality photo of a yellow sunflower with vivid colors\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVTn_TznH1OT",
        "outputId": "a9ebf992-a4c2-4df0-9921-e37152c3deac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected n_samples: 1680\n"
          ]
        }
      ],
      "source": [
        "# Sanity check: Calculate how many images are to be generated\n",
        "# Guidance strengths for classifier-free guidance\n",
        "P = len(text_prompts)           # number of prompts\n",
        "W = len(config.W_TESTS)         # guidance values per prompt\n",
        "NUM_REPETITIONS = 10            # Number of copies of every prompt/guidance pair\n",
        "n_samples = P * W * NUM_REPETITIONS   # Total images generated: one per (prompt, guidance) pair\n",
        "\n",
        "print(\"Expected n_samples:\", n_samples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bFhxjtKzZ3tb",
        "outputId": "c0d601ad-38a1-4df1-a922-c8d9c4e7567a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hook registered on model.down2\n"
          ]
        }
      ],
      "source": [
        "# Store intermediate feature maps extracted via forward hooks\n",
        "embeddings_storage = {}\n",
        "\n",
        "def get_embedding_hook(name):\n",
        "    \"\"\"\n",
        "    Creates a forward hook that stores the output of a given layer.\n",
        "\n",
        "    The output is detached from the computation graph to avoid\n",
        "    gradient tracking and reduce memory usage.\n",
        "    \"\"\"\n",
        "    def hook(model, input, output):\n",
        "        # We use .detach() to disconnect from the gradient graph (saves memory)\n",
        "        embeddings_storage[name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "# Register a forward hook on the U-Net bottleneck layer\n",
        "uNet_model.down2.register_forward_hook(get_embedding_hook('down2'))\n",
        "print(\"Hook registered on model.down2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZla3_4IaIeI",
        "outputId": "fe785378-ab7a-4513-ff9b-2c79cd3a8ef6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All random seeds set to 51 for reproducibility\n",
            "Generating 1680 total images...\n",
            "Generating images...\n",
            "  Running repetition 1/10...\n",
            "  Running repetition 2/10...\n",
            "  Running repetition 3/10...\n",
            "  Running repetition 4/10...\n",
            "  Running repetition 5/10...\n",
            "  Running repetition 6/10...\n",
            "  Running repetition 7/10...\n",
            "  Running repetition 8/10...\n",
            "  Running repetition 9/10...\n",
            "  Running repetition 10/10...\n",
            "Generation Complete.\n",
            "Final Images Shape: torch.Size([1680, 3, 32, 32])\n",
            "Final Embeddings Shape: torch.Size([1680, 512, 8, 8])\n"
          ]
        }
      ],
      "source": [
        "def sample_flowers_with_hook(text_list, model, ddpm, input_size, T, device, w_tests, num_repetitions=10):\n",
        "    \"\"\"\n",
        "    Generates images from text prompts using classifier-free guided diffusion\n",
        "    while capturing intermediate U-Net embeddings via a forward hook.\n",
        "\n",
        "    Args:\n",
        "        text_list (list[str]): Text prompts used for conditioning.\n",
        "        model (nn.Module): Pretrained U-Net diffusion model.\n",
        "        ddpm: Diffusion process wrapper.\n",
        "        input_size (tuple): Spatial size of generated images.\n",
        "        T (int): Number of diffusion timesteps.\n",
        "        device (torch.device): Computation device.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Final generated images.\n",
        "        torch.Tensor: Stored intermediate diffusion states (for visualization).\n",
        "    \"\"\"\n",
        "    all_generated_images = []\n",
        "    all_extracted_embeddings = []\n",
        "\n",
        "    # Encode text prompts using CLIP\n",
        "    text_tokens = clip.tokenize(text_list).to(device)\n",
        "    with torch.no_grad():\n",
        "      c = clip_model.encode_text(text_tokens).float()\n",
        "\n",
        "    for rep in range(num_repetitions):\n",
        "      print(f\"  Running repetition {rep+1}/{num_repetitions}...\")\n",
        "\n",
        "      # Run diffusion sampling with classifier-free guidance\n",
        "      x_gen, _ = ddpm_utils.sample_w(model, ddpm, input_size, T, c, device, w_tests)\n",
        "\n",
        "      # Grabs the embedding from the hook storage before it gets overwritten\n",
        "      # As the images are doubled, we keep only the conditioned ones\n",
        "      current_batch_embs = embeddings_storage['down2'][:x_gen.shape[0]].detach().cpu()\n",
        "\n",
        "      # Store both images and embeddings\n",
        "      all_generated_images.append(x_gen.detach().cpu())\n",
        "      all_extracted_embeddings.append(current_batch_embs)\n",
        "\n",
        "    # Concatenate all repetitions into single large tensors\n",
        "    final_images = torch.cat(all_generated_images, dim=0)\n",
        "    final_embeddings = torch.cat(all_extracted_embeddings, dim=0)\n",
        "\n",
        "    return final_images, final_embeddings\n",
        "\n",
        "# Run the generation\n",
        "other_utils.set_seeds(config.SEED)\n",
        "\n",
        "print(f\"Generating {len(text_prompts) * len(config.W_TESTS) * NUM_REPETITIONS} total images...\")\n",
        "\n",
        "print(\"Generating images...\")\n",
        "generated_images, extracted_embeddings = sample_flowers_with_hook(\n",
        "    text_list=text_prompts,\n",
        "    model=uNet_model,\n",
        "    ddpm=ddpm,\n",
        "    input_size=config.INPUT_SIZE,\n",
        "    T=config.TIMESTEPS,\n",
        "    device=config.DEVICE,\n",
        "    w_tests=config.W_TESTS,\n",
        "    num_repetitions=NUM_REPETITIONS\n",
        ")\n",
        "\n",
        "print(f\"Generation Complete.\")\n",
        "print(f\"Final Images Shape: {generated_images.shape}\")\n",
        "print(f\"Final Embeddings Shape: {extracted_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z0kPoXt8yVy",
        "outputId": "f74e00f4-e419-4eb1-c69d-036b8cda9098"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving images to disk...\n",
            "All images saved.\n"
          ]
        }
      ],
      "source": [
        "# Save generated images to disk for downstream evaluation\n",
        "to_pil = ToPILImage()\n",
        "\n",
        "# Track saved image paths together with their prompts and guidance values\n",
        "saved_samples = []\n",
        "\n",
        "print(\"Saving images to disk...\")\n",
        "assert len(generated_images) == n_samples, (\n",
        "    f\"generated_images={len(generated_images)} != {n_samples}\"\n",
        ")\n",
        "\n",
        "for i, img_tensor in enumerate(generated_images):\n",
        "    # Recover prompt and guidance value from the sampling order\n",
        "    idx_within_rep = i % (P * W)\n",
        "    prompt = text_prompts[idx_within_rep % P]\n",
        "    w_val = config.W_TESTS[idx_within_rep // P]\n",
        "\n",
        "    # Map model output from [-1, 1] to [0, 1] for image saving and clip any artifacts that fell outside the valid range\n",
        "    img_norm = ((img_tensor + 1) / 2).clamp(0, 1).detach().cpu()\n",
        "    pil_img = to_pil(img_norm)\n",
        "\n",
        "    filename = os.path.join(\n",
        "        config.SAVE_DIR, f\"flower_w{w_val:+.1f}_p{idx_within_rep % P}_{i}.png\"\n",
        "    )\n",
        "    pil_img.save(filename)\n",
        "\n",
        "    saved_samples.append((filename, prompt, float(w_val)))\n",
        "\n",
        "print(\"All images saved.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bUm5iqaMmmE0"
      },
      "source": [
        "# Part 2: Evaluation with CLIP Score and FID\n",
        "In this section, the quality of the generated images is evaluated using CLIP Score and Fréchet Inception Distance (FID), following the definitions provided in the assignment task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-KKudDw7z7G",
        "outputId": "ef3171e1-9c4d-43b0-e7da-f22fc9d2382c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Samples: 480\n"
          ]
        }
      ],
      "source": [
        "# Filters the generated samples before extraction to only only include samples where guidance (w) is 1.0 or higher.\n",
        "filtered_samples = [s for s in saved_samples if s[2] >= 1.0]\n",
        "\n",
        "print(f\"Filtered Samples: {len(filtered_samples)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZhEq6gDsyBC"
      },
      "source": [
        "## CLIP Score\n",
        "\n",
        "The CLIP score is computed as the cosine similarity between image and text embeddings produced by a pretrained CLIP model. It answers the question: \"How accurately does the generated image depict the content described in the text prompt?\"\n",
        "\n",
        "A higher score indicates stronger semantic alignment. Scores are computed for all generated images, allowing comparison across different guidance strengths and prompts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "1G3TVZwL-GTs"
      },
      "outputs": [],
      "source": [
        "# Initialize OpenCLIP model for CLIP-score evaluation\n",
        "clip_scorer, _, clip_preprocess_val = open_clip.create_model_and_transforms(\n",
        "    \"ViT-B-32\", pretrained=\"laion2b_s34b_b79k\"\n",
        ")\n",
        "clip_scorer.to(config.DEVICE).eval()\n",
        "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TddM8_umca36",
        "outputId": "b8535d8d-69ee-4ba4-d74d-b108b2688527"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculating scores...\n",
            "Average CLIP Score: 0.2594\n"
          ]
        }
      ],
      "source": [
        "def calculate_clip_score(image_path, text_prompt, device=None):\n",
        "    \"\"\"\n",
        "    Computes a CLIP similarity score between an image and a text prompt.\n",
        "\n",
        "    The image and text are embedded using a pretrained OpenCLIP model, L2-normalized,\n",
        "    and compared via cosine similarity (dot product of normalized embeddings).\n",
        "\n",
        "    Args:\n",
        "        image_path (str | Path): Path to the image file on disk.\n",
        "        text_prompt (str): Text prompt to compare against.\n",
        "\n",
        "    Returns:\n",
        "        float: Cosine similarity score (higher means stronger semantic alignment).\n",
        "    \"\"\"\n",
        "    # Preprocess and move to the same device as the CLIP model\n",
        "    image = clip_preprocess_val(Image.open(image_path)).unsqueeze(0).to(device)\n",
        "    text = tokenizer([text_prompt]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_scorer.encode_image(image)\n",
        "        text_features = clip_scorer.encode_text(text)\n",
        "\n",
        "        # Normalize to turn dot product into cosine similarity\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        score = (image_features @ text_features.T).item()\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "# Compute CLIP scores for all generated samples\n",
        "clip_scores = []\n",
        "\n",
        "print(\"Calculating scores...\")\n",
        "\n",
        "for i, (filepath, prompt, w_val) in enumerate(filtered_samples):\n",
        "    score = calculate_clip_score(\n",
        "        image_path=filepath,\n",
        "        text_prompt=prompt,\n",
        "        device=config.DEVICE,\n",
        "    )\n",
        "    clip_scores.append(score)\n",
        "\n",
        "avg_clip_score = float(np.mean(clip_scores))\n",
        "print(f\"Average CLIP Score: {avg_clip_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IY1X2qd4s0BT"
      },
      "source": [
        "## FID Score\n",
        "\n",
        "To measure how realistic our generated images are, we calculate the Fréchet Inception Distance (FID) score. We use a powerful pre-trained InceptionV3 model as a feature \"judge.\" Both our generated images and the real flower images are prepared identically—resized to 299x299 and normalized—to ensure a fair comparison.\n",
        "\n",
        "FID is computed by comparing feature statistics extracted from the model for real and generated images. It statistically compares these two collections; a lower score indicates that the generated images are more similar to the real data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfvm5w54KGxw",
        "outputId": "ba06ea0d-7b9f-4b3f-d93f-3179dcb4240e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-0cc3c7bd.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-0cc3c7bd.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 104M/104M [00:00<00:00, 186MB/s] \n"
          ]
        }
      ],
      "source": [
        "# Load Pretrained InceptionV3;\n",
        "inception = inception_v3(\n",
        "    weights=Inception_V3_Weights.DEFAULT,\n",
        "    transform_input=False,\n",
        ").to(config.DEVICE)\n",
        "\n",
        "# To return features (2048) - not classes as the standard Inception model does - the final\n",
        "# \"Fully Connected\" layer needs to be replaced with a \"Pass Through\" (Identity)\n",
        "inception.fc = torch.nn.Identity()\n",
        "\n",
        "inception.eval()\n",
        "\n",
        "image_net_mean = [0.485, 0.456, 0.406]\n",
        "image_net_std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "# Inception expects 299x299 size instead of now 32x32 and specific normalization\n",
        "inception_transform = transforms.Compose([\n",
        "    transforms.Resize((config.INCEPTION_IMG_SIZE, config.INCEPTION_IMG_SIZE)), # Up-sample from 32x32\n",
        "    transforms.ToImage(),\n",
        "    transforms.ToDtype(torch.float32, scale=True),\n",
        "    transforms.Normalize(mean=image_net_mean, std=image_net_std)   # from original pytorch docs\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "CresJrStNhT7"
      },
      "outputs": [],
      "source": [
        "def get_inception_features_from_raw(dataset_path, batch_size, model, device=None, num_workers=0):\n",
        "    \"\"\"\n",
        "    Extracts 2048-dimensional InceptionV3 feature embeddings for a dataset of images.\n",
        "\n",
        "    Args:\n",
        "        raw_dataset): dataset of the original images\n",
        "        model (nn.Module): Pretrained InceptionV3 feature extractor (fc = Identity).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Array of shape (N, 2048) containing feature embeddings for all images.\n",
        "    \"\"\"\n",
        "    raw_dataset = other_utils.MyDataset(dataset_path, inception_transform, config.CLASSES)\n",
        "    raw_dataloader  = DataLoader(raw_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    features = []                         # Stores feature batches\n",
        "    with torch.no_grad():\n",
        "      for img, _ in tqdm(raw_dataloader):\n",
        "          img = img.to(device)\n",
        "          f = model(img)                # Runs Inception forward pass\n",
        "          features.append(f.cpu().numpy())  # Transform to numpy for later mathematical operations\n",
        "    return np.concatenate(features, axis=0) # Concatenate batches to one array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "cCvQK4HUUmfh"
      },
      "outputs": [],
      "source": [
        "def get_inception_features_from_files(saved_samples, batch_size, model, transform, device=None, num_workers=0):\n",
        "    \"\"\"\n",
        "    Loads images from disk and extracts InceptionV3 feature embeddings.\n",
        "\n",
        "    Args:\n",
        "        saved_samples: List of tuples containing image filepaths.\n",
        "        model: Pretrained feature extractor.\n",
        "        transform: Image preprocessing pipeline (resize/normalize).\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Feature matrix of shape (N, 2048).\n",
        "    \"\"\"\n",
        "    dataset = other_utils.GeneratedListDataset(saved_samples, transform=transform)\n",
        "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "\n",
        "    features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for img_batch in tqdm(loader, desc=\"Extracting Generated Features\"):\n",
        "            img_batch = img_batch.to(device)\n",
        "\n",
        "            # The transform handles Resize, Scale, and Normalize\n",
        "            f = model(img_batch)\n",
        "            features.append(f.cpu().numpy())\n",
        "\n",
        "    return np.concatenate(features, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sG9XOXQwgdNU"
      },
      "outputs": [],
      "source": [
        "def calculate_fid(real_embeddings, gen_embeddings):\n",
        "\n",
        "    # Calculate mean and covariance\n",
        "    mu1, sigma1 = real_embeddings.mean(axis=0), np.cov(real_embeddings, rowvar=False)\n",
        "    mu2, sigma2 = gen_embeddings.mean(axis=0), np.cov(gen_embeddings, rowvar=False)\n",
        "\n",
        "    # Sum squared difference between means\n",
        "    ssdiff = np.sum((mu1 - mu2)**2)\n",
        "\n",
        "    # Product of covariances\n",
        "    covmean = sqrtm(sigma1.dot(sigma2))\n",
        "\n",
        "    # Numerical error handling\n",
        "    if np.iscomplexobj(covmean):\n",
        "        covmean = covmean.real\n",
        "\n",
        "    # Final FID calculation\n",
        "    fid = ssdiff + np.trace(sigma1 + sigma2 - 2.0 * covmean)\n",
        "\n",
        "    return fid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZwuITZymtCOp",
        "outputId": "539ceecc-89ca-4806-f18f-593b3eace720"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 37/37 [02:57<00:00,  4.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Real Embeddings: (1166, 2048)\n",
            "Total generated: 1680 | Realistic samples for FID: 480\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting Generated Features: 100%|██████████| 15/15 [00:02<00:00,  5.41it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Embeddings: (480, 2048)\n",
            "FID Score: 239.2461\n"
          ]
        }
      ],
      "source": [
        "# Extract features from real images on disk\n",
        "dataset_path = config.TMP_ROOT / \"data/cropped_flowers\"\n",
        "\n",
        "# Extract features from real images on disk\n",
        "real_embeddings = get_inception_features_from_raw(dataset_path, config.BATCH_SIZE, inception, device=config.DEVICE, num_workers=config.NUM_WORKERS)\n",
        "\n",
        "print(\"Real Embeddings:\", real_embeddings.shape)\n",
        "\n",
        "print(f\"Total generated: {len(saved_samples)} | Realistic samples for FID: {len(filtered_samples)}\")\n",
        "\n",
        "# Extract features from generated images on disk\n",
        "gen_embeddings = get_inception_features_from_files(\n",
        "    saved_samples=filtered_samples,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    model=inception,\n",
        "    transform=inception_transform,\n",
        "    device=config.DEVICE,\n",
        "    num_workers=config.NUM_WORKERS\n",
        ")\n",
        "\n",
        "print(\"Generated Embeddings:\", gen_embeddings.shape)\n",
        "\n",
        "# Compute FID Score: checks if the images look \"real\" compared to the original dataset\n",
        "fid_score = calculate_fid(real_embeddings, gen_embeddings)\n",
        "print(f\"FID Score: {fid_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ar3mr8TKnE7s"
      },
      "source": [
        "# Part 3: Embedding Analysis with FiftyOne Brain\n",
        "This section organizes the generated images into a structured FiftyOne dataset to analyze the model's internal behavior.\n",
        "Each image is paired with its corresponding prompt, guidance weight (w), and CLIP score, while the extracted U-Net bottleneck features are stored as vector embeddings.\n",
        "Afterwards, uniqueness (identifying visually distinct samples) and representativeness (identifying the most typical examples) is computed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "-avsvL1efvng"
      },
      "outputs": [],
      "source": [
        "# Create a new FiftyOne dataset\n",
        "\n",
        "# Delete existing dataset if it exists\n",
        "if config.FIFTYONE_DATASET_EXPERIMENTS_NAME in fo.list_datasets():\n",
        "    print(f\"Deleting existing dataset: {config.FIFTYONE_DATASET_EXPERIMENTS_NAME}\")\n",
        "    fo.delete_dataset(config.FIFTYONE_DATASET_EXPERIMENTS_NAME)\n",
        "\n",
        "dataset = fo.Dataset(name=config.FIFTYONE_DATASET_EXPERIMENTS_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mmtPSvEfLEt",
        "outputId": "1538708a-5e1b-489c-f0c8-18f067a30e2d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building FiftyOne dataset...\n",
            " 100% |█████████████████| 480/480 [11.8s elapsed, 0s remaining, 30.1 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 480/480 [11.8s elapsed, 0s remaining, 30.1 samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 480 samples to the dataset.\n"
          ]
        }
      ],
      "source": [
        "# Build a FiftyOne dataset where each image is paired with prompt, guidance w,\n",
        "# CLIP score, and a flattened U-Net embedding (used for embedding-based analysis)\n",
        "samples = []\n",
        "\n",
        "print(\"Building FiftyOne dataset...\")\n",
        "\n",
        "for i, (filepath, prompt, w_val) in enumerate(filtered_samples):\n",
        "    # FiftyOne Brain expects a 1D embedding vector per sample for distance computations\n",
        "    raw_embedding = extracted_embeddings[i]                 # e.g., (512, 8, 8)\n",
        "    flat_embedding = raw_embedding.flatten().cpu().numpy() # (512*8*8,)\n",
        "\n",
        "    sample = fo.Sample(filepath=filepath)\n",
        "\n",
        "    # Store fields for filtering and analysis in the FiftyOne App\n",
        "    sample[\"ground_truth\"] = fo.Classification(label=prompt)\n",
        "    sample[\"w\"] = float(w_val)\n",
        "    sample[\"clip_score\"] = float(clip_scores[i])\n",
        "    sample[\"unet_embedding\"] = flat_embedding\n",
        "\n",
        "    samples.append(sample)\n",
        "\n",
        "# Add all samples in one call for efficiency\n",
        "dataset.add_samples(samples)\n",
        "print(f\"Added {len(samples)} samples to the dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0QBzJzlf4vs",
        "outputId": "5be89cfb-2dec-4ecc-882a-89661460b53f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing uniqueness...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.uniqueness:Computing uniqueness...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Uniqueness computation complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.uniqueness:Uniqueness computation complete\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing representativeness...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.representativeness:Computing representativeness...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing clusters for 480 embeddings; this may take awhile...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.representativeness:Computing clusters for 480 embeddings; this may take awhile...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Representativeness computation complete\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.brain.internal.core.representativeness:Representativeness computation complete\n"
          ]
        }
      ],
      "source": [
        "# Compute Uniqueness (Visual diversity)\n",
        "fob.compute_uniqueness(dataset, embeddings=\"unet_embedding\")\n",
        "\n",
        "# Compute Representativeness using the extracted U-Net embeddings\n",
        "fob.compute_representativeness(dataset, embeddings=\"unet_embedding\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "hM1t2zpSf51b",
        "outputId": "6d3d0832-afc5-4539-81ad-b3633f43c39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\r\n",
            "Could not connect session, trying again in 10 seconds\r\n",
            "\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Client is not connected",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3995292868.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Launch the FiftyOne App to visualize your dataset and analyze the results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlaunch_app\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fiftyone/core/session/session.py\u001b[0m in \u001b[0;36mlaunch_app\u001b[0;34m(dataset, view, sample_id, group_id, spaces, color_scheme, plots, port, address, remote, browser, height, auto, config)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \"\"\"\n\u001b[1;32m    191\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0m_session\u001b[0m  \u001b[0;31m# pylint: disable=global-statement\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m     _session = Session(\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0mview\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fiftyone/core/session/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, view, sample_id, group_id, spaces, color_scheme, plots, port, address, remote, browser, height, auto, config, view_name)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_notebook_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook_height\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbrowser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbrowser\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fiftyone/core/session/session.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(session, *args, **kwargs)\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mauto_show\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_notebook_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m                 \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m             \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStateUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mauto_show\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfocx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_notebook_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fiftyone/core/session/session.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, height)\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1066\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1067\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1068\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fiftyone/core/session/session.py\u001b[0m in \u001b[0;36mfreeze\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1157\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1159\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeactivateNotebookCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1160\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfreeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/fiftyone/core/session/client.py\u001b[0m in \u001b[0;36msend_event\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Client is not connected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_post_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Client is not connected"
          ]
        }
      ],
      "source": [
        "# Launch the FiftyOne App to visualize your dataset and analyze the results\n",
        "session = fo.launch_app(dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3EOrhi-RW7vG"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "To reduce statistical noise observed in earlier experiments, the evaluation pipeline was expanded from a small-scale snapshot of only 21 samples to 1,680 images (7 guidance weights, 21 prompts and 10 repetitions), enabling a more stable and reliable assessment.\n",
        "\n",
        "To ensure meaningful evaluation, a quality-based filtering strategy was applied. Only samples with guidance weight w ≥ 1.0 were retained, resulting in a final evaluation set of 480 images.\n",
        "\n",
        "| Metric          | Baseline (N = 21) | Filtered (N = 480) | Improvement |\n",
        "|-----------------|------------------:|-------------------:|------------:|\n",
        "| Avg. CLIP score | 0.212             | 0.259              | +22.1%      |\n",
        "| FID             | 320.5             | 239.2              | −25.3%      |\n",
        "\n",
        "**Analysis of Improvements**\n",
        "* The FID score dropped from 320.5 to 239.2. This improvement is primarily attributed to the increased sample mass (10 repetitions across 21 unique prompts). \n",
        "* Semantic Alignment (CLIP): The CLIP score rose from 0.212 to 0.259, indicating a stronger correlation between the pixels and text prompts. This jump is a direct result of replacing generic labels with descriptive prompts and programmatically filtering the evaluation to exclude abstract and noisy outputs (where w<1.0). \n",
        "* Dataset Diversity: Utilizing 21 distinct prompts provided a wider feature set and allowed for a more comprehensive comparison against the real flower dataset.\n",
        "\n",
        "**Conclusion**\n",
        "The results prove that already minimal adjustments lead to an improvement in both scores. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CpRozR3ndFQ"
      },
      "source": [
        "# Part 4: Logging with Weights & Biases\n",
        "All experiments are logged to Weights & Biases for reproducibility and comparison. Hyperparameters, generated images, guidance values, CLIP scores, and embedding-based metrics are stored in a structured table, together with aggregate evaluation metrics such as average CLIP score and FID."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LQPywS8q7UrH",
        "outputId": "e6d07f64-ce3a-498b-8ccc-040afe053752"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmichele-marschner\u001b[0m (\u001b[33mmichele-marschner-university-of-potsdam\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load W&B API key from Colab Secrets and make it available as env variable\n",
        "wandb_key = userdata.get('WANDB_API_KEY')\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_key\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "MW8i5M4cdokX",
        "outputId": "8c5a365c-fd9c-4ee1-a85c-cbee3d02d2bb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.23.1"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03/wandb/run-20260106_173236-jt9b2tn3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2/runs/jt9b2tn3' target=\"_blank\">experiment_run_further_experiments_2026-01-06_17-32</a></strong> to <a href='https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2/runs/jt9b2tn3' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2/runs/jt9b2tn3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>evaluation/average_clip_score</td><td>▁</td></tr><tr><td>evaluation/fid_score</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>evaluation/average_clip_score</td><td>0.25942</td></tr><tr><td>evaluation/fid_score</td><td>239.24612</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">experiment_run_further_experiments_2026-01-06_17-32</strong> at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2/runs/jt9b2tn3' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2/runs/jt9b2tn3</a><br> View project at: <a href='https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2' target=\"_blank\">https://wandb.ai/michele-marschner-university-of-potsdam/diffusion_model_assessment_v2</a><br>Synced 4 W&B file(s), 1 media file(s), 482 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20260106_173236-jt9b2tn3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Initialize Run\n",
        "timestamp = other_utils.get_timestamp()\n",
        "run = wandb.init(project=\"diffusion_model_assessment_v2\", name=f\"experiment_run_further_experiments_{timestamp}\")\n",
        "\n",
        "# Log Hyperparameters\n",
        "wandb.config.update({\n",
        "    \"steps_T\": config.TIMESTEPS,\n",
        "    \"image_size\": config.IMG_SIZE,\n",
        "    \"clip_features\": config.CLIP_FEATURES,\n",
        "    \"prompts\": text_prompts\n",
        "})\n",
        "\n",
        "# Create a Table for Visual Results\n",
        "columns = [\"image generated\", \"prompt\", \"guidance_w\", \"clip_score\", \"uniqueness\", \"representativeness\"]\n",
        "\n",
        "diffusion_test_table = wandb.Table(columns=columns)\n",
        "\n",
        "# Populate Table\n",
        "# Grab uniqueness and representativeness scores back from FiftyOne\n",
        "uniqueness_scores = dataset.values(\"uniqueness\")\n",
        "representativeness_scores = dataset.values(\"representativeness\")\n",
        "\n",
        "for i, (filepath, prompt, w_val) in enumerate(filtered_samples):\n",
        "    wandb_img = wandb.Image(filepath)\n",
        "\n",
        "    diffusion_test_table.add_data(\n",
        "        wandb_img,\n",
        "        prompt,\n",
        "        w_val,\n",
        "        clip_scores[i],\n",
        "        uniqueness_scores[i],\n",
        "        representativeness_scores[i],\n",
        "    )\n",
        "\n",
        "# Log the Table and Metrics\n",
        "wandb.log({\n",
        "    \"generation_results\": diffusion_test_table,\n",
        "    \"evaluation/fid_score\": fid_score,\n",
        "    \"evaluation/average_clip_score\": avg_clip_score\n",
        "    })\n",
        "\n",
        "# Finish\n",
        "run.finish()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aCV",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
