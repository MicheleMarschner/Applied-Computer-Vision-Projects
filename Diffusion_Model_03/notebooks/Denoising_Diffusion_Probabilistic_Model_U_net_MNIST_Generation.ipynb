{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-Aware Classification with an “IDK” Label\n",
    "\n",
    "In this notebook, we extend an MNIST digit classifier originally used to evaluate images generated by the UNet-DDPM classifier-free guidance model from the NVIDIA Generative AI with Diffusion Models course.\n",
    "\n",
    "The classifier is augmented with an “IDK” (I Don’t Know) prediction option. When the model’s confidence falls below a chosen threshold, it outputs IDK instead of forcing a potentially unreliable digit prediction. This allows uncertain samples to be deferred for human review and provides insight into the model’s aleatoric uncertainty.\n",
    "\n",
    "We first study how different confidence thresholds affect accuracy and coverage. The resulting predictions are then examined qualitatively using the FiftyOne app. Finally, the test set, together with predictions and metadata, is exported as a FiftyOne dataset for structured evaluation and further analysis.\n",
    "\n",
    "In the next cell, we import the required modules, set random seeds for reproducibility, and define the configuration parameters and file paths used throughout the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Feagam036AYl"
   },
   "source": [
    "Original code by [Antonio Rueda-Toicen](antonio@kineto.ai)\n",
    "Parts of the implementation are reused and extended in this notebook.\n",
    "\n",
    "[![Creative Commons License](https://i.creativecommons.org/l/by/4.0/88x31.png)](http://creativecommons.org/licenses/by/4.0/)\n",
    "\n",
    "This work is licensed under a [Creative Commons Attribution 4.0 International License](http://creativecommons.org/licenses/by/4.0/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7izynWdRGH1x"
   },
   "source": [
    "![](https://raw.githubusercontent.com/andandandand/practical-computer-vision/refs/heads/main/images/generated_mnist_digits_FO.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r5GqxAMl7ndl"
   },
   "source": [
    "\n",
    "\n",
    "* U-Net: For the noise prediction network in the Denoising Diffusion Probabilisit Model (DDPM)\n",
    "* Diffusion Process: Defines how noise is added (forward) and removed (reverse).\n",
    "* MNIST Classifier: A standard convolutional neural network for classifying digits. It doesn't share gradients or information with the U-net, we use it just to evaluate the quality of predictions.\n",
    "* Training Loops: One for the DDPM, one for the classifier.\n",
    "* Sampling & Evaluation: Generating digits from the DDPM and classifying them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtdeQlc7KPO3"
   },
   "source": [
    "\n",
    "\n",
    "# Benefits of U-Net's Downsampling-Upsampling Architecture in Diffusion Models\n",
    "\n",
    "## Multi-scale Feature Processing\n",
    "- U-Net captures hierarchical image features through progressive downsampling\n",
    "- Lower resolutions encode global context and structure\n",
    "- Higher resolutions preserve fine details and textures\n",
    "- This multi-scale approach matches diffusion's noise characteristics at different timesteps\n",
    "\n",
    "## Efficient Long-range Dependencies\n",
    "- Downsampling increases effective receptive field without computational explosion\n",
    "- Later timesteps (more noise) require global context to reconstruct structure\n",
    "- Earlier timesteps (less noise) need local precision for detail refinement\n",
    "- U-Net addresses both through its pyramidal architecture\n",
    "\n",
    "## Skip Connections Preserve Information\n",
    "- Skip connections between corresponding encoding/decoding layers maintain critical information\n",
    "- These connections combat vanishing gradients in deep networks\n",
    "- They create residual pathways that help preserve spatial details lost during downsampling\n",
    "- Particularly valuable in diffusion models where preserving underlying image structure is essential\n",
    "\n",
    "## Parameter Efficiency\n",
    "- Downsampling reduces spatial dimensions, allowing deeper networks with manageable parameters\n",
    "- Deeper networks capture more complex noise patterns across diffusion timesteps\n",
    "- Upsampling gradually reconstructs spatial resolution while maintaining semantic understanding\n",
    "- This efficiency enables handling the complex mapping from noisy to clean images\n",
    "\n",
    "## Resolution-dependent Processing\n",
    "- Different stages in diffusion process benefit from different resolution processing:\n",
    "  - Early denoising (high noise): Benefits from low-resolution processing focused on structural elements\n",
    "  - Late denoising (low noise): Requires high-resolution processing for detail refinement\n",
    "- U-Net's architecture naturally aligns with this progression\n",
    "\n",
    "## Consistent with Diffusion Physics\n",
    "- Noise diffusion follows a coarse-to-fine pattern in physical systems\n",
    "- U-Net's architecture mimics this natural process:\n",
    "  - Downsampling path: Identifies global structures in highly noisy images\n",
    "  - Upsampling path: Progressively refines details as noise levels decrease\n",
    "- This alignment improves model convergence and generation quality\n",
    "\n",
    "## Attention Integration\n",
    "- Modern diffusion U-Nets incorporate self-attention at lower resolutions\n",
    "- This combines convolutional inductive bias with transformer-like global reasoning\n",
    "- Particularly valuable for coherent structure generation in highly degraded images\n",
    "\n",
    "The U-Net architecture  balances global context and local detail processing, making it suited for the progressive denoising task of diffusion models across their full timestep spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q_2ZnhI5ScNJ"
   },
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!uv pip install fiftyone==1.5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%%capture\n",
    "%pip install --no-cache-dir -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependency from prev assignment\n",
    "!pip -q install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ShaVNKtpSfwz"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14298,
     "status": "ok",
     "timestamp": 1768864339712,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "yBcsmn_npcSN",
    "outputId": "cddbb231-2abe-4182-b91f-292ca9b738cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    #  !! Change the following path if the project is located elsewhere (repeat in config.py)\n",
    "    %cd \"/content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03\"\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 17000,
     "status": "ok",
     "timestamp": 1768864356716,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "lTXppAKy0c-b",
    "outputId": "e3202e9c-0f7b-4e5c-d343-ae530134b85d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/glob2/fnmatch.py:141: SyntaxWarning: invalid escape sequence '\\Z'\n",
      "  return '(?ms)' + res + '\\Z'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "import shutil\n",
    "import fiftyone as fo\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# --- Configuration ---\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "IMG_SIZE = 28 # MNIST image size\n",
    "BATCH_SIZE_DDPM = 128\n",
    "BATCH_SIZE_CLASSIFIER = 64\n",
    "# Learning rates\n",
    "LR_DDPM = 1e-3\n",
    "LR_CLASSIFIER = 1e-3\n",
    "EPOCHS_DDPM = 50 # Increase for better results (e.g., 100-200)\n",
    "EPOCHS_CLASSIFIER = 10 # Usually converges faster\n",
    "T = 300 # Number of diffusion timesteps (can be 200-1000)\n",
    "SAVE_INTERVAL_DDPM = 10 # Save generated images every N epochs\n",
    "OUTPUT_DIR = \"ddpm_mnist_output\"\n",
    "CLASSIFIER_MODEL_PATH = os.path.join(OUTPUT_DIR, \"mnist_classifier.pth\")\n",
    "DDPM_MODEL_PATH = os.path.join(OUTPUT_DIR, \"ddpm_unet.pth\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768864356719,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "DVe_nG1Opp8V"
   },
   "outputs": [],
   "source": [
    "# Set this to False in case that you don't want to use Google Drive to store models and generated images\n",
    "DRIVE_AVAILABLE = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ha87423ksgai"
   },
   "source": [
    "## Define Model Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1768864356726,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "mqmXYmVNpTp0",
    "outputId": "ddc86764-a759-45cf-ccf5-704c4d1ba15c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model Paths ---\n",
      "Local output directory: ddpm_mnist_output\n",
      "Google Drive model directory: /content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03\n",
      "Local Classifier (Final): ddpm_mnist_output/mnist_classifier.pth\n",
      "Local Classifier (Best): ddpm_mnist_output/best_mnist_classifier.pth\n",
      "Local DDPM (Final): ddpm_mnist_output/ddpm_unet.pth\n",
      "Local DDPM (Best): ddpm_mnist_output/best_ddpm_unet.pth\n",
      "Drive Classifier (Final): /content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03/mnist_classifier.pth\n",
      "Drive Classifier (Best): /content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03/best_mnist_classifier.pth\n",
      "Drive DDPM (Final): /content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03/ddpm_unet.pth\n",
      "Drive DDPM (Best): /content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03/best_ddpm_unet.pth\n"
     ]
    }
   ],
   "source": [
    "# Base directory for models on Google Drive\n",
    "DRIVE_MODEL_ROOT_DIR = \"/content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03\"\n",
    "if DRIVE_AVAILABLE:\n",
    "    os.makedirs(DRIVE_MODEL_ROOT_DIR, exist_ok=True)\n",
    "\n",
    "# Local output directory (as before)\n",
    "LOCAL_OUTPUT_DIR = \"ddpm_mnist_output\"\n",
    "os.makedirs(LOCAL_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Model file names\n",
    "CLASSIFIER_MODEL_FILENAME = \"mnist_classifier.pth\"\n",
    "BEST_CLASSIFIER_MODEL_FILENAME = \"best_mnist_classifier.pth\"\n",
    "DDPM_MODEL_FILENAME = \"ddpm_unet.pth\"\n",
    "BEST_DDPM_MODEL_FILENAME = \"best_ddpm_unet.pth\"\n",
    "\n",
    "# Local Paths\n",
    "CLASSIFIER_MODEL_PATH_LOCAL = os.path.join(LOCAL_OUTPUT_DIR, CLASSIFIER_MODEL_FILENAME)\n",
    "BEST_CLASSIFIER_MODEL_PATH_LOCAL = os.path.join(LOCAL_OUTPUT_DIR, BEST_CLASSIFIER_MODEL_FILENAME)\n",
    "DDPM_MODEL_PATH_LOCAL = os.path.join(LOCAL_OUTPUT_DIR, DDPM_MODEL_FILENAME)\n",
    "BEST_DDPM_MODEL_PATH_LOCAL = os.path.join(LOCAL_OUTPUT_DIR, BEST_DDPM_MODEL_FILENAME)\n",
    "\n",
    "# Google Drive Paths (None if Drive not available)\n",
    "CLASSIFIER_MODEL_PATH_DRIVE = os.path.join(DRIVE_MODEL_ROOT_DIR, LOCAL_OUTPUT_DIR, CLASSIFIER_MODEL_FILENAME) if DRIVE_AVAILABLE else None\n",
    "BEST_CLASSIFIER_MODEL_PATH_DRIVE = os.path.join(DRIVE_MODEL_ROOT_DIR, LOCAL_OUTPUT_DIR, BEST_CLASSIFIER_MODEL_FILENAME) if DRIVE_AVAILABLE else None\n",
    "DDPM_MODEL_PATH_DRIVE = os.path.join(DRIVE_MODEL_ROOT_DIR, LOCAL_OUTPUT_DIR, DDPM_MODEL_FILENAME) if DRIVE_AVAILABLE else None\n",
    "BEST_DDPM_MODEL_PATH_DRIVE = os.path.join(DRIVE_MODEL_ROOT_DIR, LOCAL_OUTPUT_DIR, BEST_DDPM_MODEL_FILENAME) if DRIVE_AVAILABLE else None\n",
    "\n",
    "print(\"--- Model Paths ---\")\n",
    "print(f\"Local output directory: {LOCAL_OUTPUT_DIR}\")\n",
    "if DRIVE_AVAILABLE:\n",
    "    print(f\"Google Drive model directory: {DRIVE_MODEL_ROOT_DIR}\")\n",
    "else:\n",
    "    print(\"Google Drive not available. Models will be local only.\")\n",
    "\n",
    "print(f\"Local Classifier (Final): {CLASSIFIER_MODEL_PATH_LOCAL}\")\n",
    "print(f\"Local Classifier (Best): {BEST_CLASSIFIER_MODEL_PATH_LOCAL}\")\n",
    "print(f\"Local DDPM (Final): {DDPM_MODEL_PATH_LOCAL}\")\n",
    "print(f\"Local DDPM (Best): {BEST_DDPM_MODEL_PATH_LOCAL}\")\n",
    "\n",
    "if DRIVE_AVAILABLE:\n",
    "    print(f\"Drive Classifier (Final): {CLASSIFIER_MODEL_PATH_DRIVE}\")\n",
    "    print(f\"Drive Classifier (Best): {BEST_CLASSIFIER_MODEL_PATH_DRIVE}\")\n",
    "    print(f\"Drive DDPM (Final): {DDPM_MODEL_PATH_DRIVE}\")\n",
    "    print(f\"Drive DDPM (Best): {BEST_DDPM_MODEL_PATH_DRIVE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1768864356730,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "zEMAb_FMmjTZ",
    "outputId": "b348c3b7-db05-427d-a2b7-16ee220ca5b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Configuration\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "IMG_SIZE = 28 # MNIST image size\n",
    "BATCH_SIZE_DDPM = 128\n",
    "BATCH_SIZE_CLASSIFIER = 64\n",
    "# Learning rates\n",
    "LR_DDPM = 1e-3\n",
    "LR_CLASSIFIER = 1e-3\n",
    "\n",
    "# SET THESE CAREFULLY:\n",
    "# 0 means only load, >0 means train (or continue training if model loaded)\n",
    "EPOCHS_DDPM = 0       # e.g., 50 for training, 0 for loading only\n",
    "EPOCHS_CLASSIFIER = 0 # e.g., 10 for training, 0 for loading only\n",
    "\n",
    "T = 300 # Number of diffusion timesteps (can be 200-1000)\n",
    "SAVE_INTERVAL_DDPM = 10 # Save generated images every N epochs during DDPM training\n",
    "# LOCAL_OUTPUT_DIR is used for non-model artifacts like sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import UNet_utils, ddpm_utils, other_utils, config, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9JcGDqTG43o"
   },
   "source": [
    "# Why We Use Positional Embeddings for Timesteps\n",
    "\n",
    "Sinusoidal positional embeddings for timesteps `t` in Denoising Diffusion Probabilistic Models serve several critical functions:\n",
    "\n",
    "## Purpose\n",
    "- The U-Net predicts noise `ε` added to image `x_0` to produce `x_t`\n",
    "- Noise characteristics depend on timestep `t` - early timesteps have minimal noise while late timesteps contain mostly noise\n",
    "- The U-Net requires timestep information to predict the appropriate noise to remove\n",
    "\n",
    "## Problems with Raw Integer Timesteps\n",
    "- **Scale Issues:** Raw integers (0-1000) have disparate magnitudes that complicate neural network learning\n",
    "- **Interpretation Gaps:** Networks may treat each integer as distinct, missing the relationship between adjacent timesteps\n",
    "- **Limited Range:** Raw integers provide no framework for understanding timesteps outside training range\n",
    "- **Insufficient Information:** A single scalar value lacks expressiveness for timestep representation\n",
    "\n",
    "## Benefits of Sinusoidal Positional Embeddings\n",
    "- **Unique Vectors:** Each timestep receives a distinct vector representation\n",
    "- **Fixed Calculation:** Embeddings derive from sine/cosine functions rather than learning, providing consistency\n",
    "- **Continuity:** Smooth functions create proximity between adjacent timesteps in embedding space, enabling interpolation\n",
    "- **Structured Relationships:** For offset `k`, embedding of `t+k` can be expressed as linear function of embedding `t`\n",
    "- **Frequency Spectrum:** Multiple frequencies capture both coarse and fine-grained timestep relationships:\n",
    "  ```\n",
    "  PE(t, 2i)   = sin(t / 10000^(2i / d_model))\n",
    "  PE(t, 2i+1) = cos(t / 10000^(2i / d_model))\n",
    "  ```\n",
    "  - Low frequencies (large wavelengths) capture broad noise patterns\n",
    "  - High frequencies (small wavelengths) differentiate between adjacent timesteps\n",
    "- **Value Constraints:** Sine/cosine outputs bound between -1 and 1, stabilizing network training\n",
    "- **Dimensional Richness:** Vector representation (256-512 dimensions) provides depth beyond scalar values\n",
    "\n",
    "## Implementation in U-Net Architecture\n",
    "- Integer timestep `t` converts to sinusoidal positional embedding vector\n",
    "- This vector passes through a multi-layer perceptron for transformation\n",
    "- The resulting embedding integrates with U-Net feature maps via addition or concatenation, conditioning the network on the noise level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1768864356750,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "O5BuZaWx8DXx"
   },
   "outputs": [],
   "source": [
    "#  Helper: Sinusoidal Positional Embeddings for Timesteps\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, time):\n",
    "        device = time.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = time[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B3fB8gLasTYF"
   },
   "source": [
    "## U-net Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1768864356780,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "KBqy3SQH8GCF"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, time_emb_dim, up=False):\n",
    "        super().__init__()\n",
    "        self.time_mlp = nn.Linear(time_emb_dim, out_ch)\n",
    "        if up:\n",
    "            # For upsampling, conv1 takes concatenated input (skip + previous upsample)\n",
    "            # The input channels to conv1 will be in_ch (from skip) + in_ch_prev_upsample\n",
    "            # Here, in_ch is specified as the total input channels after concatenation.\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1) # Adjusted for explicit concat in UNet forward\n",
    "            self.transform = nn.ConvTranspose2d(out_ch, out_ch, 4, 2, 1)\n",
    "        else: # Downsampling\n",
    "            self.conv1 = nn.Conv2d(in_ch, out_ch, 3, padding=1)\n",
    "            self.transform = nn.Conv2d(out_ch, out_ch, 4, 2, 1)\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, 3, padding=1)\n",
    "        self.bnorm1 = nn.BatchNorm2d(out_ch)\n",
    "        self.bnorm2 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        h = self.bnorm1(self.relu(self.conv1(x)))\n",
    "        time_emb = self.relu(self.time_mlp(t))\n",
    "        time_emb = time_emb[(...,) + (None,) * 2] # Expand to match spatial dims\n",
    "        h = h + time_emb\n",
    "        h = self.bnorm2(self.relu(self.conv2(h)))\n",
    "        return self.transform(h)\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, img_channels=1, time_emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.time_emb_dim = time_emb_dim\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(time_emb_dim),\n",
    "            nn.Linear(time_emb_dim, time_emb_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # Initial projection: (B, C, H, W) -> (B, 64, H, W)\n",
    "        self.initial_conv = nn.Conv2d(img_channels, 64, 3, padding=1)\n",
    "\n",
    "        # Downsampling path for 28x28 image\n",
    "        # (B, 64, 28, 28) -> (B, 128, 14, 14)\n",
    "        self.down1 = Block(64, 128, time_emb_dim)\n",
    "        # (B, 128, 14, 14) -> (B, 256, 7, 7)\n",
    "        self.down2 = Block(128, 256, time_emb_dim)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bot_conv1 = nn.Conv2d(256, 512, 3, padding=1) # Stays at 7x7\n",
    "        self.bot_relu = nn.ReLU()\n",
    "        self.bot_time_mlp = nn.Linear(time_emb_dim, 512) # Time embedding for bottleneck\n",
    "        self.bot_conv2 = nn.Conv2d(512, 256, 3, padding=1) # Back to 256 channels, 7x7\n",
    "\n",
    "        # Upsampling path\n",
    "        # Skip from down2 (256 ch) + bottleneck output (256 ch) = 512 ch input\n",
    "        # (B, 512, 7, 7) -> (B, 128, 14, 14)\n",
    "        self.up1 = Block(256 + 256, 128, time_emb_dim, up=True)\n",
    "        # Skip from down1 (128 ch) + up1 output (128 ch) = 256 ch input\n",
    "        # (B, 256, 14, 14) -> (B, 64, 28, 28)\n",
    "        self.up2 = Block(128 + 128, 64, time_emb_dim, up=True)\n",
    "\n",
    "        # Final output layer\n",
    "        # Input from up2 (64 ch) + initial_conv output for skip (64 ch) = 128 ch input\n",
    "        # (B, 128, 28, 28) -> (B, img_channels, 28, 28)\n",
    "        # A common U-Net pattern is to have a final conv that takes the last upsample layer's output\n",
    "        # and potentially a skip from the very first layer.\n",
    "        # Let's adjust self.output to take the output of up2 and the initial projection.\n",
    "        self.output_conv = nn.Conv2d(64 + 64, img_channels, kernel_size=1)\n",
    "        # Simpler: just use the output of up2 if that's the intended design\n",
    "        # self.output_conv = nn.Conv2d(64, img_channels, kernel_size=1) # If no final skip\n",
    "\n",
    "    def forward(self, x, timestep):\n",
    "        t_emb = self.time_mlp(timestep)\n",
    "\n",
    "        x0 = self.initial_conv(x) # (B, 64, 28, 28) - This is the first skip connection source\n",
    "\n",
    "        # Downsample\n",
    "        d1 = self.down1(x0, t_emb) # (B, 128, 14, 14)\n",
    "        d2 = self.down2(d1, t_emb) # (B, 256, 7, 7)\n",
    "\n",
    "        # Bottleneck\n",
    "        b = self.bot_conv1(d2)    # (B, 512, 7, 7)\n",
    "        time_emb_bot = self.bot_relu(self.bot_time_mlp(t_emb))\n",
    "        time_emb_bot = time_emb_bot[(...,) + (None,) * 2]\n",
    "        b = b + time_emb_bot\n",
    "        b = self.bot_conv2(self.bot_relu(b)) # (B, 256, 7, 7)\n",
    "\n",
    "        # Upsample with skip connections\n",
    "        # up1 input: concat(b, d2) -> (B, 256+256=512, 7, 7)\n",
    "        u1 = self.up1(torch.cat((b, d2), dim=1), t_emb)    # (B, 128, 14, 14)\n",
    "        # up2 input: concat(u1, d1) -> (B, 128+128=256, 14, 14)\n",
    "        u2 = self.up2(torch.cat((u1, d1), dim=1), t_emb)    # (B, 64, 28, 28)\n",
    "\n",
    "        # Final output\n",
    "        # Option 1: Concatenate with initial projection x0\n",
    "        out = self.output_conv(torch.cat((u2, x0), dim=1)) # (B, C, 28, 28)\n",
    "        # Option 2: If output_conv takes 64 channels (no final skip with x0)\n",
    "        # out = self.output_conv(u2)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XFfpGgu9sL_S"
   },
   "source": [
    "## Diffusion Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1768864356781,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "xM_bT2Br8lSk"
   },
   "outputs": [],
   "source": [
    "def linear_beta_schedule(timesteps, start=0.0001, end=0.02):\n",
    "    return torch.linspace(start, end, timesteps)\n",
    "\n",
    "def get_index_from_list(vals, t, x_shape):\n",
    "    batch_size = t.shape[0]\n",
    "    out = vals.gather(-1, t)\n",
    "    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, timesteps=T, img_size=IMG_SIZE, device=DEVICE):\n",
    "        self.timesteps = timesteps\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "        self.betas = linear_beta_schedule(timesteps).to(device)\n",
    "        self.alphas = 1. - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, axis=0)\n",
    "        # Ensure F.pad is torch.nn.functional.pad\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - self.alphas_cumprod)\n",
    "        self.posterior_variance = self.betas * (1. - self.alphas_cumprod_prev) / (1. - self.alphas_cumprod)\n",
    "\n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        sqrt_alphas_cumprod_t = get_index_from_list(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x, t, t_index):\n",
    "        betas_t = get_index_from_list(self.betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = get_index_from_list(self.sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "        sqrt_recip_alphas_t = get_index_from_list(self.sqrt_recip_alphas, t, x.shape)\n",
    "        model_mean = sqrt_recip_alphas_t * (x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t)\n",
    "        if t_index == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_variance_t = get_index_from_list(self.posterior_variance, t, x.shape)\n",
    "            noise = torch.randn_like(x)\n",
    "            return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=16, channels=1):\n",
    "        shape = (batch_size, channels, image_size, image_size)\n",
    "        img = torch.randn(shape, device=self.device)\n",
    "        imgs = []\n",
    "        for i in tqdm(reversed(range(0, self.timesteps)), desc='Sampling loop', total=self.timesteps):\n",
    "            t = torch.full((batch_size,), i, device=self.device, dtype=torch.long)\n",
    "            img = self.p_sample(model, img, t, i)\n",
    "            if i % (self.timesteps // 10) == 0 or i < 10 :\n",
    "                 imgs.append(img.cpu())\n",
    "        imgs.append(img.cpu())\n",
    "        return imgs\n",
    "\n",
    "    def p_losses(self, denoise_model, x_start, t, noise=None, loss_type=\"l1\"):\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "        x_noisy = self.q_sample(x_start=x_start, t=t, noise=noise)\n",
    "        predicted_noise = denoise_model(x_noisy, t)\n",
    "        if loss_type == 'l1':\n",
    "            loss = F.l1_loss(noise, predicted_noise)\n",
    "        elif loss_type == 'l2':\n",
    "            loss = F.mse_loss(noise, predicted_noise)\n",
    "        elif loss_type == \"huber\":\n",
    "            loss = F.smooth_l1_loss(noise, predicted_noise)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GNz4lhr8sIKF"
   },
   "source": [
    "## MNIST Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1768864356789,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "MFSfeE8ZDRAq"
   },
   "outputs": [],
   "source": [
    "class MNISTClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MNISTClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(self.relu1(self.conv1(x)))\n",
    "        x = self.pool2(self.relu2(self.conv2(x)))\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu3(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SCTjvYYqsDF2"
   },
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2568,
     "status": "ok",
     "timestamp": 1768864359357,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "JY-0w-CxDUqN",
    "outputId": "9a35185b-d5ec-494d-9e21-2b77f57b7b2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets and DataLoaders ready.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "transform_ddpm = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "transform_classifier = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# For DDPM training\n",
    "train_dataset_ddpm = datasets.MNIST('.', train=True, download=True, transform=transform_ddpm)\n",
    "train_loader_ddpm = DataLoader(train_dataset_ddpm, batch_size=BATCH_SIZE_DDPM, shuffle=True, drop_last=True)\n",
    "\n",
    "# For Classifier training and evaluation\n",
    "train_dataset_classifier = datasets.MNIST('.', train=True, download=True, transform=transform_classifier)\n",
    "test_dataset_classifier = datasets.MNIST('.', train=False, download=True, transform=transform_classifier)\n",
    "train_loader_classifier = DataLoader(train_dataset_classifier, batch_size=BATCH_SIZE_CLASSIFIER, shuffle=True)\n",
    "test_loader_classifier = DataLoader(test_dataset_classifier, batch_size=BATCH_SIZE_CLASSIFIER, shuffle=False)\n",
    "\n",
    "print(\"Datasets and DataLoaders ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PYpJG28fwYpc"
   },
   "source": [
    "## Visualizing the Forward Diffusion Process\n",
    "\n",
    "This section visualizes the **forward diffusion process**, which is the foundation of Denoising Diffusion Probabilistic Models (DDPMs). The forward process describes how a clean, original image ($x_0$) is gradually corrupted by adding small amounts of [Gaussian noise](https://en.wikipedia.org/wiki/Gaussian_noise) over a series of discrete timesteps ($t$).\n",
    "\n",
    "**What we are visualizing:**\n",
    "\n",
    "We start with a single, clean sample image from the MNIST dataset ($x_0$). Then, for a sequence of increasing timesteps $t$ (from $t=0$ up to $t=T-1$, where $T$ is the total number of diffusion steps defined in our model), we apply the forward diffusion equation:\n",
    "\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1 - \\bar{\\alpha}_t} \\epsilon$$\n",
    "\n",
    "where:\n",
    "*   $x_t$ is the noisy image at timestep $t$.\n",
    "*   $x_0$ is the original clean image.\n",
    "*   $\\bar{\\alpha}_t$ (alpha-bar-t) is a pre-computed noise schedule parameter that decreases as $t$ increases. This means more noise is effectively added at later timesteps.\n",
    "*   $\\epsilon$ is a sample from a standard Gaussian (normal) distribution.\n",
    "\n",
    "**The Plot:**\n",
    "\n",
    "The plot will display a row of images:\n",
    "*   Each image in the row represents the state of the original $x_0$ after noise has been added up to the indicated timestep $t$.\n",
    "*   The leftmost images (small $t$) will show the original image with very little noise.\n",
    "*   As we move to the right (increasing $t$), the images will become progressively noisier.\n",
    "*   The rightmost image (large $t$, close to $T-1$) will be almost entirely noise, with little to no discernible structure from the original image.\n",
    "\n",
    "**Why is this important?**\n",
    "\n",
    "Understanding the forward process is important because:\n",
    "1.  It defines how the training data for the U-Net (the noise prediction model) is generated. The U-Net is trained to predict the noise $\\epsilon$ that was added to $x_0$ to get $x_t$, given $x_t$ and $t$.\n",
    "2.  The reverse process (which the U-Net learns to approximate) aims to undo this noise addition, step-by-step, starting from pure noise ($x_T$) and gradually denoising it back to a clean image ($x_0$). This reverse process is how new images are generated.\n",
    "\n",
    "This visualization helps build intuition about the nature of the noisy inputs the U-Net will encounter during training and the progressive nature of the denoising task it must learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "executionInfo": {
     "elapsed": 1212,
     "status": "ok",
     "timestamp": 1768864360571,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "zDYrjUUBwjso",
    "outputId": "f67914fa-a095-4a7c-a607-c1a9ac4c751c"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x350 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "if 'train_dataset_ddpm' not in locals() or 'Diffusion' not in locals():\n",
    "    print(\"Please ensure 'Data Loading' (Cell 8) and 'Diffusion Process' (Cell 6) have been run.\")\n",
    "else:\n",
    "    # Create a diffusion instance specifically for this visualization\n",
    "    # Note: 'T' from config is the total number of diffusion timesteps.\n",
    "    # Timestep indices in the code usually go from 0 to T-1.\n",
    "    diffusion_visualizer = Diffusion(timesteps=T, img_size=IMG_SIZE, device=DEVICE)\n",
    "\n",
    "    # Get a sample image from the DDPM training dataset\n",
    "    # The transform_ddpm has already scaled images to [-1, 1]\n",
    "    sample_idx = 0 # You can change this index to see different digits\n",
    "    x0_tensor, _ = train_dataset_ddpm[sample_idx]\n",
    "    x0_tensor = x0_tensor.to(DEVICE) # Shape: (C, H, W)\n",
    "\n",
    "    # --- Timesteps to visualize ---\n",
    "    # We want to show how the image gets corrupted over time.\n",
    "    # The `q_sample` function with t=0 (index) applies the first scheduled noise amount.\n",
    "    num_images_to_plot = 8 # Total images to display in the row\n",
    "\n",
    "    if T >= num_images_to_plot:\n",
    "        # Select evenly spaced timesteps from 0 to T-1\n",
    "        plot_t_indices = torch.linspace(0, T - 1, num_images_to_plot, dtype=torch.long).tolist()\n",
    "    else:\n",
    "        # If T is small, show all available steps up to num_images_to_plot\n",
    "        plot_t_indices = torch.arange(0, T, dtype=torch.long).tolist()\n",
    "        # Pad with the last step if num_images_to_plot is larger than T\n",
    "        while len(plot_t_indices) < num_images_to_plot:\n",
    "            plot_t_indices.append(T - 1 if T > 0 else 0)\n",
    "\n",
    "\n",
    "    # --- Plotting ---\n",
    "    fig, axes = plt.subplots(1, num_images_to_plot, figsize=(2.5 * num_images_to_plot, 3.5))\n",
    "    fig.suptitle(f\"Forward Diffusion: x_0 Corrupted to x_t (MNIST Sample {sample_idx})\", fontsize=16)\n",
    "\n",
    "    for i, t_idx_val in enumerate(plot_t_indices):\n",
    "        ax = axes[i]\n",
    "\n",
    "        # Prepare input for q_sample: batch of x0 and batch of timesteps\n",
    "        x_start_batch = x0_tensor.unsqueeze(0)  # Shape: (1, C, H, W)\n",
    "        t_batch = torch.tensor([t_idx_val], device=DEVICE, dtype=torch.long) # Shape: (1)\n",
    "\n",
    "        # Apply forward diffusion q(x_t | x_0)\n",
    "        # xt will be on DEVICE, shape (1, C, H, W)\n",
    "        xt = diffusion_visualizer.q_sample(x_start_batch, t_batch)\n",
    "\n",
    "        # Denormalize from [-1, 1] to [0, 1] for plotting\n",
    "        # Squeeze batch dim, move to CPU, denormalize\n",
    "        img_to_plot = (xt.squeeze(0).cpu() + 1) / 2.0\n",
    "\n",
    "        # Plot\n",
    "        # permute for (H, W, C) if multi-channel, then squeeze if C=1 for grayscale\n",
    "        ax.imshow(img_to_plot.permute(1, 2, 0).squeeze().numpy(), cmap=\"gray\")\n",
    "        ax.set_title(f\"t={t_idx_val}\")\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.93]) # Adjust layout to make space for suptitle\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emGdMj3Fr-hU"
   },
   "source": [
    "## Helper function to load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1768864360573,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "mYXre4RZ8Jo4"
   },
   "outputs": [],
   "source": [
    "def load_model_weights(model, model_name, paths_to_try, device):\n",
    "    \"\"\"\n",
    "    Tries to load model weights from a list of paths.\n",
    "    Paths should be ordered by priority (e.g., best on Drive, best local, final on Drive, final local).\n",
    "    Returns True if loaded, False otherwise.\n",
    "    \"\"\"\n",
    "    for path in paths_to_try:\n",
    "        if path and os.path.exists(path):\n",
    "            print(f\"Attempting to load {model_name} from {path}...\")\n",
    "            try:\n",
    "                model.load_state_dict(torch.load(path, map_location=device))\n",
    "                model.to(device) # Ensure model is on the correct device after loading\n",
    "                print(f\"{model_name} loaded successfully from {path}.\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {model_name} from {path}: {e}. Trying next path.\")\n",
    "        elif path:\n",
    "            # print(f\"Path {path} for {model_name} does not exist.\") # Optional debug\n",
    "            pass\n",
    "    print(f\"{model_name} not found in any specified paths.\")\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPVhagVJr7DK"
   },
   "source": [
    "## Training functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1768864360589,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "ZdrdKfSj1-ds"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test_classifier(model, test_loader, criterion, name=\"Test\"):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    avg_test_loss = test_loss / len(test_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"{name} Results: Avg Loss: {avg_test_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n",
    "    # model.train() # Set back to train mode by the caller if needed\n",
    "    return accuracy\n",
    "\n",
    "def train_classifier(model, train_loader, test_loader, optimizer, criterion, epochs,\n",
    "                       initial_best_accuracy=0.0):\n",
    "    model.train() # Ensure model is in training mode\n",
    "    print(\"Starting Classifier Training...\")\n",
    "    best_accuracy = initial_best_accuracy\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Ensure model is in training mode for each epoch\n",
    "        epoch_loss_train = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for images, labels in tqdm(train_loader, desc=f\"Classifier Epoch {epoch+1}/{epochs} (Train)\"):\n",
    "            images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss_train += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total_train += labels.size(0)\n",
    "            correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "        avg_epoch_loss_train = epoch_loss_train / len(train_loader)\n",
    "        accuracy_train = 100 * correct_train / total_train\n",
    "        print(f\"Classifier Epoch {epoch+1}/{epochs} (Train): Loss: {avg_epoch_loss_train:.4f}, Accuracy: {accuracy_train:.2f}%\")\n",
    "\n",
    "        current_test_accuracy = test_classifier(model, test_loader, criterion, f\"Classifier Test Epoch {epoch+1}\")\n",
    "\n",
    "        if current_test_accuracy > best_accuracy:\n",
    "            best_accuracy = current_test_accuracy\n",
    "            print(f\"New best classifier accuracy: {best_accuracy:.2f}%. Saving best model...\")\n",
    "            torch.save(model.state_dict(), BEST_CLASSIFIER_MODEL_PATH_LOCAL)\n",
    "            print(f\"  Best classifier model saved locally: {BEST_CLASSIFIER_MODEL_PATH_LOCAL}\")\n",
    "            if BEST_CLASSIFIER_MODEL_PATH_DRIVE:\n",
    "                try:\n",
    "                    shutil.copyfile(BEST_CLASSIFIER_MODEL_PATH_LOCAL, BEST_CLASSIFIER_MODEL_PATH_DRIVE)\n",
    "                    print(f\"  Best classifier model copied to Drive: {BEST_CLASSIFIER_MODEL_PATH_DRIVE}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error saving best classifier model to Drive: {e}\")\n",
    "\n",
    "    print(f\"\\nFinished classifier training. Saving final model...\")\n",
    "    torch.save(model.state_dict(), CLASSIFIER_MODEL_PATH_LOCAL)\n",
    "    print(f\"  Final classifier model saved locally: {CLASSIFIER_MODEL_PATH_LOCAL}\")\n",
    "    if CLASSIFIER_MODEL_PATH_DRIVE:\n",
    "        try:\n",
    "            shutil.copyfile(CLASSIFIER_MODEL_PATH_LOCAL, CLASSIFIER_MODEL_PATH_DRIVE)\n",
    "            print(f\"  Final classifier model copied to Drive: {CLASSIFIER_MODEL_PATH_DRIVE}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving final classifier model to Drive: {e}\")\n",
    "    return best_accuracy\n",
    "\n",
    "\n",
    "def train_ddpm(model, diffusion_process, dataloader, optimizer, epochs,\n",
    "                 initial_best_loss=float('inf')):\n",
    "    model.train() # Ensure model is in training mode\n",
    "    print(\"Starting DDPM Training...\")\n",
    "    best_loss = initial_best_loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train() # Ensure model is in training mode for each epoch\n",
    "        epoch_loss = 0\n",
    "        for step, (images, _) in enumerate(tqdm(dataloader, desc=f\"DDPM Epoch {epoch+1}/{epochs}\")):\n",
    "            optimizer.zero_grad()\n",
    "            images = images.to(DEVICE)\n",
    "            batch_size = images.shape[0]\n",
    "            t = torch.randint(0, diffusion_process.timesteps, (batch_size,), device=DEVICE).long()\n",
    "            loss = diffusion_process.p_losses(model, images, t, loss_type=\"huber\")\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
    "        print(f\"DDPM Epoch {epoch+1}/{epochs}, Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            print(f\"New best DDPM loss: {best_loss:.4f}. Saving best model...\")\n",
    "            torch.save(model.state_dict(), BEST_DDPM_MODEL_PATH_LOCAL)\n",
    "            print(f\"  Best DDPM model saved locally: {BEST_DDPM_MODEL_PATH_LOCAL}\")\n",
    "            if BEST_DDPM_MODEL_PATH_DRIVE:\n",
    "                try:\n",
    "                    shutil.copyfile(BEST_DDPM_MODEL_PATH_LOCAL, BEST_DDPM_MODEL_PATH_DRIVE)\n",
    "                    print(f\"  Best DDPM model copied to Drive: {BEST_DDPM_MODEL_PATH_DRIVE}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"  Error saving best DDPM model to Drive: {e}\")\n",
    "\n",
    "        if (epoch + 1) % SAVE_INTERVAL_DDPM == 0 or epoch == epochs - 1:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                sampled_images_steps = diffusion_process.sample(model, IMG_SIZE, batch_size=16, channels=1)\n",
    "                final_sampled_images = sampled_images_steps[-1]\n",
    "                final_sampled_images = (final_sampled_images + 1) / 2.0 # Denormalize\n",
    "                grid = make_grid(final_sampled_images, nrow=4)\n",
    "                img_save_path = os.path.join(LOCAL_OUTPUT_DIR, f\"ddpm_sample_epoch_{epoch+1}.png\")\n",
    "                save_image(grid, img_save_path)\n",
    "                print(f\"Saved sample images at epoch {epoch+1} to {img_save_path}\")\n",
    "            # model.train() # Re-set by loop start\n",
    "\n",
    "    print(f\"\\nFinished DDPM training. Saving final model...\")\n",
    "    torch.save(model.state_dict(), DDPM_MODEL_PATH_LOCAL)\n",
    "    print(f\"  Final DDPM model saved locally: {DDPM_MODEL_PATH_LOCAL}\")\n",
    "    if DDPM_MODEL_PATH_DRIVE:\n",
    "        try:\n",
    "            shutil.copyfile(DDPM_MODEL_PATH_LOCAL, DDPM_MODEL_PATH_DRIVE)\n",
    "            print(f\"  Final DDPM model copied to Drive: {DDPM_MODEL_PATH_DRIVE}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error saving final DDPM model to Drive: {e}\")\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oJROkh-Yr0Uv"
   },
   "source": [
    "## Load, Train, Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 60159,
     "status": "ok",
     "timestamp": 1768864420749,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "egb6S2BS5VB9",
    "outputId": "3b76c9c1-587a-48e8-ca00-ace75eda5733"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initializing MNIST Classifier ---\n",
      "Attempting to load MNIST Classifier from /content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03/best_mnist_classifier.pth...\n",
      "MNIST Classifier loaded successfully from /content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03/best_mnist_classifier.pth.\n",
      "Evaluating loaded classifier model...\n",
      "Loaded Classifier Performance Results: Avg Loss: 0.0207, Accuracy: 99.32%\n",
      "Proceeding to train classifier for 3 epochs.\n",
      "Continuing training. Initial best accuracy for this session: 99.32%\n",
      "Starting Classifier Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch 1/3 (Train): 100%|██████████| 938/938 [00:15<00:00, 58.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch 1/3 (Train): Loss: 0.0141, Accuracy: 99.55%\n",
      "Classifier Test Epoch 1 Results: Avg Loss: 0.0299, Accuracy: 99.07%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch 2/3 (Train): 100%|██████████| 938/938 [00:17<00:00, 53.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch 2/3 (Train): Loss: 0.0102, Accuracy: 99.67%\n",
      "Classifier Test Epoch 2 Results: Avg Loss: 0.0331, Accuracy: 99.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Classifier Epoch 3/3 (Train): 100%|██████████| 938/938 [00:15<00:00, 60.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifier Epoch 3/3 (Train): Loss: 0.0087, Accuracy: 99.72%\n",
      "Classifier Test Epoch 3 Results: Avg Loss: 0.0303, Accuracy: 99.07%\n",
      "\n",
      "Finished classifier training. Saving final model...\n",
      "  Final classifier model saved locally: ddpm_mnist_output/mnist_classifier.pth\n",
      "  Final classifier model copied to Drive: /content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03/mnist_classifier.pth\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Initializing MNIST Classifier ---\")\n",
    "mnist_classifier = MNISTClassifier().to(DEVICE) # Initialized on DEVICE\n",
    "optimizer_classifier = optim.Adam(mnist_classifier.parameters(), lr=LR_CLASSIFIER)\n",
    "criterion_classifier = nn.CrossEntropyLoss()\n",
    "\n",
    "classifier_ready = False\n",
    "current_best_classifier_accuracy = 0.0\n",
    "\n",
    "# Define paths to try for loading classifier, in order of preference\n",
    "classifier_load_paths_ordered = [\n",
    "    BEST_CLASSIFIER_MODEL_PATH_DRIVE, BEST_CLASSIFIER_MODEL_PATH_LOCAL,\n",
    "    CLASSIFIER_MODEL_PATH_DRIVE, CLASSIFIER_MODEL_PATH_LOCAL\n",
    "]\n",
    "if load_model_weights(mnist_classifier, \"MNIST Classifier\", classifier_load_paths_ordered, DEVICE):\n",
    "    classifier_ready = True\n",
    "    print(\"Evaluating loaded classifier model...\")\n",
    "    current_best_classifier_accuracy = test_classifier(mnist_classifier, test_loader_classifier, criterion_classifier, \"Loaded Classifier Performance\")\n",
    "    mnist_classifier.train() # Set back to train mode if test_classifier set it to eval\n",
    "else:\n",
    "    print(\"No pre-trained classifier found or failed to load.\")\n",
    "\n",
    "if EPOCHS_CLASSIFIER > 0:\n",
    "    print(f\"Proceeding to train classifier for {EPOCHS_CLASSIFIER} epochs.\")\n",
    "    if classifier_ready:\n",
    "        print(f\"Continuing training. Initial best accuracy for this session: {current_best_classifier_accuracy:.2f}%\")\n",
    "    else: # No model loaded, starting fresh\n",
    "        print(\"Training classifier from scratch.\")\n",
    "        current_best_classifier_accuracy = 0.0 # Ensure it starts from 0 if no load\n",
    "    train_classifier(mnist_classifier, train_loader_classifier, test_loader_classifier,\n",
    "                     optimizer_classifier, criterion_classifier, EPOCHS_CLASSIFIER,\n",
    "                     initial_best_accuracy=current_best_classifier_accuracy)\n",
    "    classifier_ready = True # Mark as ready after training\n",
    "elif not classifier_ready:\n",
    "    print(\"WARNING: EPOCHS_CLASSIFIER is 0, and no classifier model was loaded. Evaluation of DDPM samples might fail or be inaccurate.\")\n",
    "else: # EPOCHS_CLASSIFIER == 0 and classifier_loaded == True\n",
    "    print(\"Classifier loaded, and EPOCHS_CLASSIFIER is 0. Skipping classifier training.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PLJVMGY2rkte"
   },
   "source": [
    "## DDPM U-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 602
    },
    "executionInfo": {
     "elapsed": 19818,
     "status": "error",
     "timestamp": 1768864440569,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "28PTISrprjgx",
    "outputId": "82bb90e1-4054-4def-cb26-64d21e4c4297"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Initializing DDPM U-Net ---\n",
      "Attempting to load DDPM U-Net from /content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03/best_ddpm_unet.pth...\n",
      "Error loading DDPM U-Net from /content/drive/MyDrive/Applied-Computer-Vision-Projects/Diffusion_Model_03/best_ddpm_unet.pth: Error(s) in loading state_dict for UNet:\n",
      "\tMissing key(s) in state_dict: \"time_mlp.1.weight\", \"time_mlp.1.bias\", \"initial_conv.weight\", \"initial_conv.bias\", \"down1.time_mlp.weight\", \"down1.time_mlp.bias\", \"down1.conv1.weight\", \"down1.conv1.bias\", \"down1.transform.weight\", \"down1.transform.bias\", \"down1.conv2.weight\", \"down1.conv2.bias\", \"down1.bnorm1.weight\", \"down1.bnorm1.bias\", \"down1.bnorm1.running_mean\", \"down1.bnorm1.running_var\", \"down1.bnorm2.weight\", \"down1.bnorm2.bias\", \"down1.bnorm2.running_mean\", \"down1.bnorm2.running_var\", \"down2.time_mlp.weight\", \"down2.time_mlp.bias\", \"down2.conv1.weight\", \"down2.conv1.bias\", \"down2.transform.weight\", \"down2.transform.bias\", \"down2.conv2.weight\", \"down2.conv2.bias\", \"down2.bnorm1.weight\", \"down2.bnorm1.bias\", \"down2.bnorm1.running_mean\", \"down2.bnorm1.running_var\", \"down2.bnorm2.weight\", \"down2.bnorm2.bias\", \"down2.bnorm2.running_mean\", \"down2.bnorm2.running_var\", \"bot_conv1.weight\", \"bot_conv1.bias\", \"bot_time_mlp.weight\", \"bot_time_mlp.bias\", \"bot_conv2.weight\", \"bot_conv2.bias\", \"up1.time_mlp.weight\", \"up1.time_mlp.bias\", \"up1.conv1.weight\", \"up1.conv1.bias\", \"up1.transform.weight\", \"up1.transform.bias\", \"up1.conv2.weight\", \"up1.conv2.bias\", \"up1.bnorm1.weight\", \"up1.bnorm1.bias\", \"up1.bnorm1.running_mean\", \"up1.bnorm1.running_var\", \"up1.bnorm2.weight\", \"up1.bnorm2.bias\", \"up1.bnorm2.running_mean\", \"up1.bnorm2.running_var\", \"up2.time_mlp.weight\", \"up2.time_mlp.bias\", \"up2.conv1.weight\", \"up2.conv1.bias\", \"up2.transform.weight\", \"up2.transform.bias\", \"up2.conv2.weight\", \"up2.conv2.bias\", \"up2.bnorm1.weight\", \"up2.bnorm1.bias\", \"up2.bnorm1.running_mean\", \"up2.bnorm1.running_var\", \"up2.bnorm2.weight\", \"up2.bnorm2.bias\", \"up2.bnorm2.running_mean\", \"up2.bnorm2.running_var\", \"output_conv.weight\", \"output_conv.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"down0.conv1.model.0.weight\", \"down0.conv1.model.0.bias\", \"down0.conv1.model.1.weight\", \"down0.conv1.model.1.bias\", \"down0.conv2.model.0.weight\", \"down0.conv2.model.0.bias\", \"down0.conv2.model.1.weight\", \"down0.conv2.model.1.bias\", \"dense_emb.0.weight\", \"dense_emb.0.bias\", \"dense_emb.2.weight\", \"dense_emb.2.bias\", \"dense_emb.4.weight\", \"dense_emb.4.bias\", \"t_emb1.model.0.weight\", \"t_emb1.model.0.bias\", \"t_emb1.model.2.weight\", \"t_emb1.model.2.bias\", \"t_emb2.model.0.weight\", \"t_emb2.model.0.bias\", \"t_emb2.model.2.weight\", \"t_emb2.model.2.bias\", \"c_embed1.model.0.weight\", \"c_embed1.model.0.bias\", \"c_embed1.model.2.weight\", \"c_embed1.model.2.bias\", \"c_embed2.model.0.weight\", \"c_embed2.model.0.bias\", \"c_embed2.model.2.weight\", \"c_embed2.model.2.bias\", \"up0.1.model.0.weight\", \"up0.1.model.0.bias\", \"up0.1.model.1.weight\", \"up0.1.model.1.bias\", \"out.0.weight\", \"out.0.bias\", \"out.1.weight\", \"out.1.bias\", \"out.3.weight\", \"out.3.bias\", \"down1.model.0.model.0.weight\", \"down1.model.0.model.0.bias\", \"down1.model.0.model.1.weight\", \"down1.model.0.model.1.bias\", \"down1.model.1.model.0.weight\", \"down1.model.1.model.0.bias\", \"down1.model.1.model.1.weight\", \"down1.model.1.model.1.bias\", \"down1.model.2.conv.model.0.weight\", \"down1.model.2.conv.model.0.bias\", \"down1.model.2.conv.model.1.weight\", \"down1.model.2.conv.model.1.bias\", \"down2.model.0.model.0.weight\", \"down2.model.0.model.0.bias\", \"down2.model.0.model.1.weight\", \"down2.model.0.model.1.bias\", \"down2.model.1.model.0.weight\", \"down2.model.1.model.0.bias\", \"down2.model.1.model.1.weight\", \"down2.model.1.model.1.bias\", \"down2.model.2.conv.model.0.weight\", \"down2.model.2.conv.model.0.bias\", \"down2.model.2.conv.model.1.weight\", \"down2.model.2.conv.model.1.bias\", \"up1.model.0.weight\", \"up1.model.0.bias\", \"up1.model.1.model.0.weight\", \"up1.model.1.model.0.bias\", \"up1.model.1.model.1.weight\", \"up1.model.1.model.1.bias\", \"up1.model.2.model.0.weight\", \"up1.model.2.model.0.bias\", \"up1.model.2.model.1.weight\", \"up1.model.2.model.1.bias\", \"up1.model.3.model.0.weight\", \"up1.model.3.model.0.bias\", \"up1.model.3.model.1.weight\", \"up1.model.3.model.1.bias\", \"up1.model.4.model.0.weight\", \"up1.model.4.model.0.bias\", \"up1.model.4.model.1.weight\", \"up1.model.4.model.1.bias\", \"up2.model.0.weight\", \"up2.model.0.bias\", \"up2.model.1.model.0.weight\", \"up2.model.1.model.0.bias\", \"up2.model.1.model.1.weight\", \"up2.model.1.model.1.bias\", \"up2.model.2.model.0.weight\", \"up2.model.2.model.0.bias\", \"up2.model.2.model.1.weight\", \"up2.model.2.model.1.bias\", \"up2.model.3.model.0.weight\", \"up2.model.3.model.0.bias\", \"up2.model.3.model.1.weight\", \"up2.model.3.model.1.bias\", \"up2.model.4.model.0.weight\", \"up2.model.4.model.0.bias\", \"up2.model.4.model.1.weight\", \"up2.model.4.model.1.bias\". . Trying next path.\n",
      "Attempting to load DDPM U-Net from ddpm_mnist_output/best_ddpm_unet.pth...\n",
      "DDPM U-Net loaded successfully from ddpm_mnist_output/best_ddpm_unet.pth.\n",
      "DDPM U-Net loaded. Training will continue/start from this state.\n",
      "Proceeding to train DDPM U-Net for 3 epochs.\n",
      "Continuing DDPM training from loaded model.\n",
      "Starting DDPM Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDPM Epoch 1/3:  17%|█▋        | 81/468 [00:11<00:53,  7.30it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2795409399.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training DDPM U-Net from scratch.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mcurrent_best_ddpm_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'inf'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Ensure it starts from inf if no load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     train_ddpm(unet_model, diffusion, train_loader_ddpm, optimizer_ddpm, EPOCHS_DDPM,\n\u001b[0m\u001b[1;32m     29\u001b[0m                  initial_best_loss=current_best_ddpm_loss)\n\u001b[1;32m     30\u001b[0m     \u001b[0mddpm_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;31m# Mark as ready after training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-3720802097.py\u001b[0m in \u001b[0;36mtrain_ddpm\u001b[0;34m(model, diffusion_process, dataloader, optimizer, epochs, initial_best_loss)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiffusion_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiffusion_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp_losses\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"huber\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-652472080.py\u001b[0m in \u001b[0;36mp_losses\u001b[0;34m(self, denoise_model, x_start, t, noise, loss_type)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_start\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mx_noisy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_sample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_start\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mpredicted_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdenoise_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_noisy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mloss_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'l1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_noise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-4245303499.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, timestep)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;31m# Downsample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, 128, 14, 14)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0md2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt_emb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, 256, 7, 7)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-4245303499.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, t)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtime_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Expand to match spatial dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtime_emb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Initializing DDPM U-Net ---\")\n",
    "unet_model = UNet(img_channels=1, time_emb_dim=256).to(DEVICE) # Initialized on DEVICE\n",
    "diffusion = Diffusion(timesteps=T, img_size=IMG_SIZE, device=DEVICE)\n",
    "optimizer_ddpm = optim.Adam(unet_model.parameters(), lr=LR_DDPM)\n",
    "\n",
    "ddpm_ready = False\n",
    "current_best_ddpm_loss = float('inf')\n",
    "\n",
    "ddpm_load_paths_ordered = [\n",
    "    BEST_DDPM_MODEL_PATH_DRIVE, BEST_DDPM_MODEL_PATH_LOCAL,\n",
    "    DDPM_MODEL_PATH_DRIVE, DDPM_MODEL_PATH_LOCAL\n",
    "]\n",
    "if load_model_weights(unet_model, \"DDPM U-Net\", ddpm_load_paths_ordered, DEVICE):\n",
    "    ddpm_ready = True\n",
    "    # For DDPM, we don't typically \"evaluate\" a loaded model's loss before training in the same way.\n",
    "    # The 'initial_best_loss' in train_ddpm will handle new bests.\n",
    "    print(\"DDPM U-Net loaded. Training will continue/start from this state.\")\n",
    "else:\n",
    "    print(\"No pre-trained DDPM U-Net found or failed to load.\")\n",
    "\n",
    "if EPOCHS_DDPM > 0:\n",
    "    print(f\"Proceeding to train DDPM U-Net for {EPOCHS_DDPM} epochs.\")\n",
    "    if ddpm_ready:\n",
    "        print(\"Continuing DDPM training from loaded model.\")\n",
    "    else: # No model loaded, starting fresh\n",
    "        print(\"Training DDPM U-Net from scratch.\")\n",
    "        current_best_ddpm_loss = float('inf') # Ensure it starts from inf if no load\n",
    "    train_ddpm(unet_model, diffusion, train_loader_ddpm, optimizer_ddpm, EPOCHS_DDPM,\n",
    "                 initial_best_loss=current_best_ddpm_loss)\n",
    "    ddpm_ready = True # Mark as ready after training\n",
    "elif not ddpm_ready:\n",
    "    print(\"WARNING: EPOCHS_DDPM is 0, and no DDPM model was loaded. Sample generation will fail.\")\n",
    "else: # EPOCHS_DDPM == 0 and ddpm_loaded == True\n",
    "    print(\"DDPM U-Net loaded, and EPOCHS_DDPM is 0. Skipping DDPM training.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3IzyFrEbvlz9"
   },
   "source": [
    "## Generate Digits with DDPM and Evaluate with Classifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136763,
     "status": "aborted",
     "timestamp": 1768864440563,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "kYYAFzkSvjq4"
   },
   "outputs": [],
   "source": [
    "if not ddpm_ready:\n",
    "    print(\"DDPM model not ready (not trained or loaded). Cannot generate samples.\")\n",
    "if not classifier_ready:\n",
    "    print(\"Classifier model not ready (not trained or loaded). Cannot fully evaluate generated samples.\")\n",
    "else:\n",
    "    unet_model.eval()\n",
    "    mnist_classifier.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        num_generated_samples = 1024 # Increase for more comprehensive FiftyOne dataset\n",
    "        print(f\"Generating {num_generated_samples} samples with DDPM...\")\n",
    "        generated_image_steps = diffusion.sample(unet_model, IMG_SIZE, batch_size=num_generated_samples, channels=1)\n",
    "        generated_images_final = generated_image_steps[-1].to(DEVICE) # Already on DEVICE if sample puts it there\n",
    "\n",
    "        # Denormalize DDPM output from [-1, 1] to [0, 1]\n",
    "        generated_images_0_1 = (generated_images_final + 1) / 2.0\n",
    "\n",
    "        # Normalize for classifier input (from [0,1] to classifier's expected norm)\n",
    "        classifier_normalize_transform = transforms.Normalize((0.1307,), (0.3081,))\n",
    "        generated_images_for_classifier = classifier_normalize_transform(generated_images_0_1.clone()) # Use .clone()\n",
    "\n",
    "        # Save a grid of generated images\n",
    "        grid_save_path = os.path.join(LOCAL_OUTPUT_DIR, \"ddpm_final_generated_grid.png\")\n",
    "        grid = make_grid(generated_images_0_1.cpu(), nrow=8 if num_generated_samples >=8 else int(num_generated_samples**0.5))\n",
    "        save_image(grid, grid_save_path)\n",
    "        print(f\"Saved final generated grid to {grid_save_path}\")\n",
    "\n",
    "        # Get classifier predictions\n",
    "        print(\"Classifying generated samples...\")\n",
    "        classifier_outputs_logits = mnist_classifier(generated_images_for_classifier)\n",
    "        classifier_outputs_probs = F.softmax(classifier_outputs_logits, dim=1)\n",
    "        predicted_confidences, predicted_classes_int = torch.max(classifier_outputs_probs.data, 1)\n",
    "\n",
    "        print(f\"\\nClassifier predictions for first {min(num_generated_samples, 64)} DDPM-generated digits (showing a subset if many):\")\n",
    "        print(predicted_classes_int.cpu().numpy()[:64].reshape(-1, 8))\n",
    "\n",
    "        correct_scores, incorrect_scores = other_utils.plot_confidence_distribution(unet_model, test_loader_classifier, config.DEVICE)\n",
    "\n",
    "        thresholds, acc, coverages = metrics.compute_idk_tradeoff(correct_scores, incorrect_scores)\n",
    "        final_acc, final_cov =  other_utils.plot_acc_coverage_curve(coverages, acc)\n",
    "        print(f\"The model is stable up to {final_cov:.1%} coverage with {final_acc:.2%} accuracy.\")\n",
    "\n",
    "        TARGET_ACC = 0.9985\n",
    "        IDK_THRESHOLD = metrics.select_threshold_by_target_accuracy(thresholds, acc, target_accuracy=TARGET_ACC)\n",
    "        IDK_THRESHOLD = float(np.clip(IDK_THRESHOLD, 0.0, 1.0))\n",
    "        print(f\"IDK_THRESHOLD={IDK_THRESHOLD:.4f} to achieve >= {TARGET_ACC*100:.2f}% accuracy on covered samples (test set).\")\n",
    "\n",
    "\n",
    "        raw_digits = predicted_classes_int.detach().cpu().tolist()\n",
    "        raw_conf = predicted_confidences.detach().cpu().tolist()\n",
    "\n",
    "        is_idk = [c < IDK_THRESHOLD for c in raw_conf]\n",
    "        labels_with_idk = [\"IDK\" if flag else str(d) for d, flag in zip(raw_digits, is_idk)]\n",
    "\n",
    "        print(f\"coverage={(1 - sum(is_idk)/len(is_idk)):.3f}  idk_rate={(sum(is_idk)/len(is_idk)):.3f}\")\n",
    "\n",
    "\n",
    "        # Plot some generated images with their predicted classes\n",
    "        fig, axes = plt.subplots(4, 4, figsize=(10, 10))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "            if i < num_generated_samples and i < 16: # Plot first 16\n",
    "                img_to_plot = generated_images_0_1[i].cpu().squeeze().numpy()\n",
    "                ax.imshow(img_to_plot, cmap='gray')\n",
    "                ax.set_title(f\"Pred: {predicted_classes_int[i].item()} ({predicted_confidences[i].item():.2f})\")\n",
    "                ax.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plot_save_path = os.path.join(LOCAL_OUTPUT_DIR, \"ddpm_generated_with_predictions.png\")\n",
    "        plt.savefig(plot_save_path)\n",
    "        print(f\"Saved plot of generated images with predictions to {plot_save_path}\")\n",
    "        # plt.show() # Uncomment to display plot if running interactively and not just saving\n",
    "\n",
    "        class_counts = torch.bincount(predicted_classes_int.cpu(), minlength=10)\n",
    "        print(\"\\nDistribution of predicted classes for generated samples:\")\n",
    "        for digit, count in enumerate(class_counts):\n",
    "            print(f\"Digit {digit}: {count.item()} samples ({100*count.item()/num_generated_samples:.2f}%)\")\n",
    "\n",
    "        # --- FiftyOne Integration ---\n",
    "        print(\"\\n--- Creating FiftyOne Dataset ---\")\n",
    "        fiftyone_image_dir = os.path.join(LOCAL_OUTPUT_DIR, \"fiftyone_generated_images\")\n",
    "        if os.path.exists(fiftyone_image_dir):\n",
    "            shutil.rmtree(fiftyone_image_dir)\n",
    "        os.makedirs(fiftyone_image_dir, exist_ok=True)\n",
    "\n",
    "        dataset_name = \"ddpm_mnist_generated_v3\" # Versioning dataset name\n",
    "        try:\n",
    "            if fo.dataset_exists(dataset_name):\n",
    "                dataset = fo.load_dataset(dataset_name)\n",
    "                dataset.delete()\n",
    "                print(f\"Deleted existing FiftyOne dataset: {dataset_name}\")\n",
    "        except Exception as e: # Broader exception for any fo issue\n",
    "            print(f\"Error checking/deleting FiftyOne dataset {dataset_name}: {e}. Proceeding to create new.\")\n",
    "            pass\n",
    "\n",
    "        dataset = fo.Dataset(name=dataset_name, persistent=True)\n",
    "        dataset.info[\"description\"] = (\n",
    "            \"MNIST digits generated by a DDPM U-Net model, \"\n",
    "            \"with predictions from a separately trained classifier.\"\n",
    "        )\n",
    "\n",
    "        fo_samples = []\n",
    "        for i in range(num_generated_samples):\n",
    "            img_tensor_0_1 = generated_images_0_1[i] # Already in [0,1] format, CHW\n",
    "            img_filename = f\"generated_img_{i:04d}.png\"\n",
    "            img_filepath = os.path.join(fiftyone_image_dir, img_filename)\n",
    "            save_image(img_tensor_0_1.cpu(), img_filepath)\n",
    "\n",
    "            sample_predicted_class_int = predicted_classes_int[i].item()\n",
    "            sample_predicted_class_label = str(sample_predicted_class_int)\n",
    "            sample_confidence = predicted_confidences[i].item()\n",
    "            sample_probabilities = classifier_outputs_probs[i].cpu().numpy().tolist()\n",
    "            sample_label_with_idk = labels_with_idk[i]     # \"IDK\" or digit string\n",
    "            sample_is_idk = bool(is_idk[i])                # True if rejected\n",
    "\n",
    "            fo_sample = fo.Sample(filepath=img_filepath)\n",
    "            fo_sample[\"prediction\"] = fo.Classification(\n",
    "                label=sample_predicted_class_label,\n",
    "                confidence=sample_confidence\n",
    "            )\n",
    "\n",
    "            fo_sample[\"predicted_digit\"] = sample_predicted_class_int # scalar field\n",
    "            fo_sample[\"probabilities\"] = sample_probabilities\n",
    "\n",
    "            # Add IDK-aware fields\n",
    "            fo_sample[\"prediction_with_idk\"] = fo.Classification(\n",
    "                label=sample_label_with_idk,\n",
    "                confidence=sample_confidence\n",
    "            )\n",
    "            fo_sample[\"predicted_label_with_idk\"] = sample_label_with_idk  # simple string field\n",
    "            fo_sample[\"is_idk\"] = sample_is_idk\n",
    "            fo_sample[\"idk_threshold\"] = float(IDK_THRESHOLD)\n",
    "            fo_sample[\"confidence_raw\"] = sample_confidence\n",
    "            fo_sample[\"predicted_digit_raw\"] = sample_predicted_class_int\n",
    "\n",
    "            # Tag abstentions for easy filtering in FiftyOne UI\n",
    "            if sample_is_idk:\n",
    "                fo_sample.tags.append(\"IDK\")\n",
    "\n",
    "            fo_samples.append(fo_sample)\n",
    "\n",
    "        dataset.add_samples(fo_samples)\n",
    "        print(f\"Added {len(fo_samples)} samples to FiftyOne dataset '{dataset_name}'.\")\n",
    "        print(f\"Images for FiftyOne are stored in: {fiftyone_image_dir}\")\n",
    "        print(f\"\\nTo view the dataset in FiftyOne App, (if in Colab) run the next cell,\")\n",
    "        print(f\"or (if local) run in your terminal: fiftyone app launch {dataset_name}\")\n",
    "\n",
    "# Global dataset variable for the next cell\n",
    "GLOBAL_FO_DATASET = dataset if 'dataset' in locals() and dataset is not None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJuQ-whlxHl2"
   },
   "source": [
    "## Visualizing the U-Net's Denoising Task\n",
    "\n",
    "Here visualize the core task of the U-Net within our Denoising Diffusion Probabilistic Model (DDPM). The U-Net is trained to predict the noise that was added to an image during the forward diffusion process. By observing its predictions, we can understand how well it has learned to \"denoise.\"\n",
    "\n",
    "We will take a clean image, manually add a known amount of noise to it for a specific timestep `t`, and then ask our trained U-Net to predict that noise. We'll compare the U-Net's prediction to the actual noise and also see how well we can reconstruct the original image using the U-Net's prediction.\n",
    "\n",
    "**What We Are Visualizing in Each Row (for a given timestep `t`):**\n",
    "\n",
    "1.  **Original x₀:**\n",
    "    *   This is a clean, original image sampled directly from the MNIST dataset. It represents the starting point before any noise is added.\n",
    "\n",
    "2.  **Noisy xₜ (t = selected timestep):**\n",
    "    *   This is the `Original x₀` after a specific amount of Gaussian noise (`Actual Noise ε`) has been added to it, corresponding to the forward diffusion process up to timestep `t`.\n",
    "    *   The formula is: `xₜ = sqrt(ᾱₜ) * x₀ + sqrt(1 - ᾱₜ) * ε`, where `ε` is the `Actual Noise`.\n",
    "    *   This `xₜ` is the input that our U-Net receives.\n",
    "\n",
    "3.  **Actual Noise ε:**\n",
    "    *   This is the *specific* Gaussian noise sample (scaled appropriately by the diffusion schedule constants implicitly within `q_sample`) that was intentionally added to `Original x₀` to produce `Noisy xₜ`.\n",
    "    *   This is the \"ground truth\" noise that we want our U-Net to predict.\n",
    "\n",
    "4.  **Predicted Noise ε\\_θ:**\n",
    "    *   This is the output of our trained U-Net when given `Noisy xₜ` and the timestep `t` as input.\n",
    "    *   The U-Net's goal is to make this `Predicted Noise ε\\_θ` as close as possible to the `Actual Noise ε`.\n",
    "\n",
    "5.  **Reconstructed x̂₀:**\n",
    "    *   This is an attempt to recover the `Original x₀` from the `Noisy xₜ` by using the `Predicted Noise ε\\_θ` from the U-Net.\n",
    "    *   The formula used for reconstruction is derived from the forward process:\n",
    "        `x̂₀ = (xₜ - sqrt(1 - ᾱₜ) * ε\\_θ) / sqrt(ᾱₜ)`\n",
    "    *   How closely `Reconstructed x̂₀` resembles `Original x₀` is a direct measure of the U-Net's denoising performance at that specific timestep `t`.\n",
    "\n",
    "By observing these five images across different timesteps `t` (representing different noise levels), we can gain insights into:\n",
    "*   How the noise characteristics change with `t`.\n",
    "*   How well the U-Net learns to predict noise under varying conditions.\n",
    "*   The U-Net's ability to facilitate the reverse diffusion process, which is key to generating new images.\n",
    "\n",
    "Ideally, the `Predicted Noise ε\\_θ` should look very similar to the `Actual Noise ε`, and the `Reconstructed x̂₀` should be a good approximation of the `Original x₀`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136763,
     "status": "aborted",
     "timestamp": 1768864440565,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "yN8zyizfxMa2"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if 'unet_model' not in locals() or not ddpm_ready: # ddpm_ready from Cell 11 indicates model is loaded/trained\n",
    "    print(\"U-Net model is not trained or loaded. Cannot visualize its output.\")\n",
    "elif 'diffusion' not in locals():\n",
    "    print(\"Diffusion object not found. Cannot perform q_sample.\")\n",
    "elif 'train_dataset_ddpm' not in locals():\n",
    "    print(\"Training dataset not found. Cannot get a sample image.\")\n",
    "else:\n",
    "    unet_model.eval() # Ensure U-Net is in evaluation mode\n",
    "\n",
    "    # Get a sample clean image (x_0)\n",
    "    sample_idx = 100 # You can change this\n",
    "    x0_original_tensor, _ = train_dataset_ddpm[sample_idx]\n",
    "    x0_original_tensor = x0_original_tensor.to(DEVICE).unsqueeze(0) # Add batch dim: (1, C, H, W)\n",
    "\n",
    "    # Choose a few timesteps to visualize\n",
    "    # T is the total number of timesteps (e.g., 300)\n",
    "    # Timestep indices go from 0 to T-1\n",
    "    # Let's pick some representative timesteps: early, mid, late in the noise schedule\n",
    "    timesteps_to_visualize = [int(T * 0.1), int(T * 0.5), int(T * 0.9)]\n",
    "    if T < 10: # Adjust if T is very small\n",
    "        timesteps_to_visualize = [0, T // 2, T - 1 if T > 0 else 0]\n",
    "    timesteps_to_visualize = [max(0, min(t, T - 1)) for t in timesteps_to_visualize] # Clamp to [0, T-1]\n",
    "\n",
    "\n",
    "    num_cols = 5 # x_0, x_t, actual_noise, predicted_noise, reconstructed_x_0\n",
    "    fig, axes = plt.subplots(len(timesteps_to_visualize), num_cols,\n",
    "                             figsize=(num_cols * 3, len(timesteps_to_visualize) * 3.5))\n",
    "    if len(timesteps_to_visualize) == 1: # Handle case for single row subplot\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    fig.suptitle(\"U-Net Noise Prediction and Reconstruction\", fontsize=16)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, t_val in enumerate(timesteps_to_visualize):\n",
    "            t_tensor = torch.tensor([t_val], device=DEVICE, dtype=torch.long)\n",
    "\n",
    "            # 1. Generate actual noise and the noisy image x_t\n",
    "            actual_noise = torch.randn_like(x0_original_tensor) # Standard Gaussian noise\n",
    "            xt_noisy_tensor = diffusion.q_sample(x_start=x0_original_tensor, t=t_tensor, noise=actual_noise)\n",
    "\n",
    "            # 2. U-Net predicts noise\n",
    "            predicted_noise_tensor = unet_model(xt_noisy_tensor, t_tensor)\n",
    "\n",
    "            # 3. Reconstruct x_0 using the predicted noise (simplified from DDPM paper, Eq. 4 rearranged)\n",
    "            # x_0_reconstructed = (x_t - sqrt(1-alpha_bar_t) * predicted_noise) / sqrt(alpha_bar_t)\n",
    "            sqrt_alphas_cumprod_t = get_index_from_list(diffusion.sqrt_alphas_cumprod, t_tensor, xt_noisy_tensor.shape)\n",
    "            sqrt_one_minus_alphas_cumprod_t = get_index_from_list(diffusion.sqrt_one_minus_alphas_cumprod, t_tensor, xt_noisy_tensor.shape)\n",
    "            x0_reconstructed_tensor = (xt_noisy_tensor - sqrt_one_minus_alphas_cumprod_t * predicted_noise_tensor) / sqrt_alphas_cumprod_t\n",
    "            x0_reconstructed_tensor = torch.clamp(x0_reconstructed_tensor, -1.0, 1.0) # Clamp to image range\n",
    "\n",
    "            # --- Plotting ---\n",
    "            # Denormalize all images/noise from [-1, 1] to [0, 1] for display\n",
    "            img_list = [\n",
    "                (x0_original_tensor.squeeze(0).cpu() + 1) / 2.0,\n",
    "                (xt_noisy_tensor.squeeze(0).cpu() + 1) / 2.0,\n",
    "                (actual_noise.squeeze(0).cpu() + 1) / 2.0,         # Visualizing noise this way might make it mostly gray\n",
    "                                                                    # but shows structure if present.\n",
    "                (predicted_noise_tensor.squeeze(0).cpu() + 1) / 2.0,\n",
    "                (x0_reconstructed_tensor.squeeze(0).cpu() + 1) / 2.0\n",
    "            ]\n",
    "            titles = [\"Original x_0\", f\"Noisy x_t (t={t_val})\", \"Actual Noise ε\", \"Predicted Noise ε_θ\", \"Reconstructed x̂_0\"]\n",
    "\n",
    "            for j, img_tensor_0_1 in enumerate(img_list):\n",
    "                ax = axes[i, j]\n",
    "                # Permute for (H, W, C) if multi-channel, then squeeze if C=1 for grayscale\n",
    "                ax.imshow(img_tensor_0_1.permute(1, 2, 0).squeeze().numpy(), cmap=\"gray\", vmin=0, vmax=1)\n",
    "                ax.set_title(titles[j])\n",
    "                ax.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95]) # Adjust layout for suptitle\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJgJGUhCvvse"
   },
   "source": [
    "## Launch FiftyOne App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136763,
     "status": "aborted",
     "timestamp": 1768864440565,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "XyTHF3R4L1jY"
   },
   "outputs": [],
   "source": [
    "# Cell 12: Launch FiftyOne (if dataset was created)\n",
    "if 'GLOBAL_FO_DATASET' in locals() and GLOBAL_FO_DATASET is not None:\n",
    "    print(f\"Launching FiftyOne App for dataset: {GLOBAL_FO_DATASET.name}\")\n",
    "    # In Colab, auto=False is often better, then print URL.\n",
    "    # Or use session.show() if it works for your Colab setup.\n",
    "    session = fo.launch_app(GLOBAL_FO_DATASET, auto=False)\n",
    "    print(f\"FiftyOne App URL: {session.url}\")\n",
    "    # To open in a new tab directly from Colab (might be blocked by popup blockers):\n",
    "    # session.show()\n",
    "else:\n",
    "    print(\"FiftyOne dataset was not created or is unavailable. Cannot launch app.\")\n",
    "\n",
    "print(\"\\nAll operations complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XXQELLw66soL"
   },
   "source": [
    "## Inspect How Good is the Model Producing Digits of the Different Classes\n",
    "\n",
    "`Labels` -> `Prediction` -> `Label` will allow you to check how good is the model at producing digits of different classes.  \n",
    "\n",
    "![](https://raw.githubusercontent.com/andandandand/practical-computer-vision/refs/heads/main/images/filtering_zeros.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2T0dxI6pOxQ4"
   },
   "source": [
    "### Filter the Output by the Confidence of the Classifier\n",
    "\n",
    "Go to `Labels` -> `Prediction` and filter samples by threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kYAyj4yuNXub"
   },
   "source": [
    "![](https://raw.githubusercontent.com/andandandand/practical-computer-vision/refs/heads/main/images/mnist_confidence.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hhqf2gBWz8N_"
   },
   "source": [
    "## Compute Embeddings for the Generated Images Using the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136767,
     "status": "aborted",
     "timestamp": 1768864440570,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "sVfiTdu4oN8o"
   },
   "outputs": [],
   "source": [
    "if 'mnist_classifier' not in locals() or not classifier_ready:\n",
    "    print(\"MNIST Classifier model not available or not ready. Cannot compute embeddings.\")\n",
    "elif 'generated_images_for_classifier' not in locals():\n",
    "    print(\"Generated images (for classifier) not found. Cannot compute embeddings.\")\n",
    "else:\n",
    "    mnist_classifier.eval() # Ensure classifier is in evaluation mode\n",
    "\n",
    "    # We want to get the output of the layer before the final classification layer (fc1)\n",
    "    # In our MNISTClassifier, fc1 is nn.Linear(64 * 7 * 7, 128)\n",
    "    # The output of fc1 (after relu3) is the 128-dimensional embedding.\n",
    "\n",
    "    # Store the embeddings\n",
    "    embeddings_list = []\n",
    "\n",
    "    # Define a hook function to capture the output of the desired layer\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            # `input` is a tuple, `output` is the tensor from the layer\n",
    "            # We are interested in the output of fc1, which is then passed to relu3.\n",
    "            # Let's get the output of relu3, which is the activated embedding.\n",
    "            embeddings_list.append(output.detach().cpu())\n",
    "        return hook\n",
    "\n",
    "    # Register the hook. We need to find the correct layer.\n",
    "    # The target layer is self.relu3 in MNISTClassifier, which processes the output of self.fc1\n",
    "    hook_handle = None\n",
    "    target_layer = None\n",
    "\n",
    "    # Find the target layer (relu3 after fc1)\n",
    "    for name, layer in mnist_classifier.named_modules():\n",
    "        if name == 'relu3': # This is nn.ReLU() after fc1\n",
    "            target_layer = layer\n",
    "            break\n",
    "\n",
    "    if target_layer:\n",
    "        print(f\"Registering hook to layer: {target_layer}\")\n",
    "        hook_handle = target_layer.register_forward_hook(get_activation(\"fc1_relu_output\"))\n",
    "\n",
    "        # Perform a forward pass to trigger the hook\n",
    "        # No need to store the final output of the classifier here,\n",
    "        # as we are interested in the intermediate embeddings captured by the hook.\n",
    "        with torch.no_grad():\n",
    "            _ = mnist_classifier(generated_images_for_classifier.to(DEVICE)) # Ensure images are on device\n",
    "\n",
    "        # Remove the hook now that we're done\n",
    "        if hook_handle:\n",
    "            hook_handle.remove()\n",
    "            print(\"Hook removed.\")\n",
    "\n",
    "        if embeddings_list:\n",
    "            # Concatenate all captured batch embeddings (if they were processed in batches by the hook)\n",
    "            # In this case, the forward pass is on all generated_images_for_classifier at once,\n",
    "            # so embeddings_list should contain one tensor.\n",
    "            all_embeddings = torch.cat(embeddings_list, dim=0)\n",
    "            print(f\"\\nSuccessfully computed embeddings for {all_embeddings.shape[0]} generated images.\")\n",
    "            print(f\"Shape of the embeddings tensor: {all_embeddings.shape}\")\n",
    "            print(f\"Each image is represented by a {all_embeddings.shape[1]}-dimensional vector.\")\n",
    "\n",
    "            # You can now use these `all_embeddings` for further analysis,\n",
    "            # e.g., dimensionality reduction (t-SNE, UMAP) and visualization,\n",
    "            # or for calculating diversity/similarity metrics.\n",
    "\n",
    "            # Example: Print the first 2 embeddings (first 10 features)\n",
    "            print(\"\\nFirst 2 embeddings (first 10 features):\")\n",
    "            for i in range(min(2, all_embeddings.shape[0])):\n",
    "                print(f\"Image {i}: {all_embeddings[i, :10].numpy()}\")\n",
    "\n",
    "            # Store these embeddings in the FiftyOne dataset if desired\n",
    "            if 'GLOBAL_FO_DATASET' in locals() and GLOBAL_FO_DATASET is not None and fo.dataset_exists(GLOBAL_FO_DATASET.name):\n",
    "                try:\n",
    "                    dataset_to_update = fo.load_dataset(GLOBAL_FO_DATASET.name)\n",
    "                    print(f\"\\nAdding embeddings to FiftyOne dataset '{dataset_to_update.name}'...\")\n",
    "                    # Ensure number of embeddings matches number of samples in FO dataset slice\n",
    "                    if len(dataset_to_update) == all_embeddings.shape[0]:\n",
    "                        # Add embeddings as a vector field to each sample\n",
    "                        # Make sure samples are iterated in the same order as generated_images_for_classifier\n",
    "                        # This assumes the current `dataset_to_update` view has the same samples in the same order.\n",
    "                        # If num_generated_samples in cell 11 was different, this might mismatch.\n",
    "                        # For safety, let's assume they match the initial `num_generated_samples`.\n",
    "\n",
    "                        embedding_vectors = np.array([emb.tolist() for emb in all_embeddings])\n",
    "                        dataset_to_update.add_sample_field(\"classifier_embedding\", fo.VectorField)\n",
    "                        dataset_to_update.set_values(\"classifier_embedding\", embedding_vectors)\n",
    "\n",
    "                        # Or if you want to be very careful matching filepaths:\n",
    "                        # sample_filepaths_ordered = [s.filepath for s in dataset_to_update]\n",
    "                        # # This requires knowing the order of generated_images_for_classifier\n",
    "                        # # and how they map to filepaths. For simplicity, direct assignment is used above.\n",
    "\n",
    "                        dataset_to_update.save() # Persist changes\n",
    "                        print(f\"Embeddings added to FiftyOne samples as 'classifier_embedding'.\")\n",
    "                        print(f\"You might need to refresh the FiftyOne App to see the new field.\")\n",
    "                    else:\n",
    "                        print(f\"Mismatch in number of embeddings ({all_embeddings.shape[0]}) and FO samples ({len(dataset_to_update)}). Skipping FO update.\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error updating FiftyOne dataset with embeddings: {e}\")\n",
    "            else:\n",
    "                print(\"FiftyOne dataset not available for updating with embeddings.\")\n",
    "\n",
    "        else:\n",
    "            print(\"No embeddings were captured. Check hook registration and forward pass.\")\n",
    "    else:\n",
    "        print(\"Could not find the target layer 'relu3' in the MNIST classifier for hook registration.\")\n",
    "\n",
    "print(\"\\nEmbedding computation process finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136766,
     "status": "aborted",
     "timestamp": 1768864440570,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "LD0qWuDn0DoD"
   },
   "outputs": [],
   "source": [
    "type(embedding_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136766,
     "status": "aborted",
     "timestamp": 1768864440571,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "PEjJJRd20VsW"
   },
   "outputs": [],
   "source": [
    "embedding_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136779,
     "status": "aborted",
     "timestamp": 1768864440584,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "EpdCdaWK0gPd"
   },
   "outputs": [],
   "source": [
    "fo.list_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136779,
     "status": "aborted",
     "timestamp": 1768864440585,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "OkK4wlTB2l64"
   },
   "outputs": [],
   "source": [
    "embedding_vectors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136779,
     "status": "aborted",
     "timestamp": 1768864440585,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "liyAdDnq0lRc"
   },
   "outputs": [],
   "source": [
    "print(\"Adding 'embedding_vectors' field to samples...\")\n",
    "with fo.ProgressBar() as pb:\n",
    "    for i, sample in enumerate(pb(dataset)):\n",
    "          # Create a FiftyOne Embeddings object (or just store as a list/ndarray)\n",
    "          # FiftyOne can store numpy arrays directly\n",
    "          sample[\"embedding\"] = embedding_vectors[i]\n",
    "          sample.save() # Save the changes to the sample\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYgb-gAT9B0n"
   },
   "source": [
    "## Project in Two Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136779,
     "status": "aborted",
     "timestamp": 1768864440586,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "ITIZkpXx03Su"
   },
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embeddings = dataset.values('embedding')\n",
    "\n",
    "pca_brain_key = f\"{dataset_name}_pca\"\n",
    "\n",
    "# Check if brain key exists\n",
    "if dataset.has_brain_run(pca_brain_key):\n",
    "    # Delete the brain key\n",
    "    dataset.clear_brain_runs(pca_brain_key)\n",
    "    dataset.save()\n",
    "\n",
    "# Compute 2D representation\n",
    "results = fob.compute_visualization(\n",
    "    dataset,\n",
    "    embeddings=embeddings,\n",
    "    num_dims=2,\n",
    "    method=\"pca\",\n",
    "    brain_key=pca_brain_key,\n",
    "    verbose=True,\n",
    "    seed=51,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136778,
     "status": "aborted",
     "timestamp": 1768864440586,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "a8oIaRau3Z2k"
   },
   "outputs": [],
   "source": [
    "# Compute uniqueness of embeddings\n",
    "fob.compute_uniqueness(dataset, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vz7vtir9QQw"
   },
   "source": [
    "## Cluster Images in 10 Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136777,
     "status": "aborted",
     "timestamp": 1768864440586,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "eO3y_QTL21TR"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Choose the number of clusters\n",
    "num_clusters = 10\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Assign each sample its cluster label\n",
    "for sample, label in zip(dataset, labels):\n",
    "    sample[\"k_means_cluster\"] = int(label)\n",
    "    sample.save()\n",
    "\n",
    "print(\"Clusters computed and stored in each sample's 'cluster' field.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136777,
     "status": "aborted",
     "timestamp": 1768864440586,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "Nv_OiKkQ26Oc"
   },
   "outputs": [],
   "source": [
    "# Save the dataset to keep views in sync\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136777,
     "status": "aborted",
     "timestamp": 1768864440587,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "Xe8hRnOc06t3"
   },
   "outputs": [],
   "source": [
    "session = fo.launch_app(GLOBAL_FO_DATASET, auto=False)\n",
    "print(f\"FiftyOne App URL: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0gM5R4G9Xiu"
   },
   "source": [
    "## Sort by Uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136777,
     "status": "aborted",
     "timestamp": 1768864440587,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "2gcUreQq1uZ1"
   },
   "outputs": [],
   "source": [
    "# Sort in increasing order of uniqueness (least unique first)\n",
    "dups_view = dataset.sort_by(\"uniqueness\", reverse=False)\n",
    "\n",
    "# Open view in the App\n",
    "session.view = dups_view\n",
    "print(f\"Less unique images at: {session.url}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvcub5eRUA1-"
   },
   "source": [
    "# IDK Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQOvQcoOUKBe"
   },
   "source": [
    "## Create the prediction function\n",
    "\n",
    "The prediction function applies a softmax to model outputs and compares the maximum confidence to a threshold. Predictions below the threshold are labeled as “IDK”, enabling controlled abstention when the model is uncertain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MAXHhG_AUNXD"
   },
   "source": [
    "## Create Fiftyone Dataset\n",
    "\n",
    "Inference results are stored in a FiftyOne dataset, including ground-truth labels, predicted labels (with IDK), and confidence scores. This enables interactive inspection of uncertain predictions and systematic analysis of coverage versus accuracy.\n",
    "\n",
    "### Choosing the confidence threshold\n",
    "Because the trained MNIST classifier achieves 99.24% accuracy, it typically assigns very high confidence to real MNIST images. Consequently, only very high thresholds (e.g. 0.99) produce noticeable IDK behavior on in-distribution data. For diffusion-generated images, confidence values are often lower due to artifacts and ambiguity, so the same threshold leads to substantially more abstentions.\n",
    "\n",
    "This illustrates how the confidence threshold directly controls the trade-off between prediction coverage and reliability, and why threshold selection must be adapted to the expected data quality and distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136776,
     "status": "aborted",
     "timestamp": 1768864440588,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "nvleC4WNUS1i"
   },
   "outputs": [],
   "source": [
    "# Create a View of only IDK samples\n",
    "idk_view = dataset.match_tags(\"IDK\")\n",
    "print(f\"Total IDK cases found: {len(idk_view)}\")\n",
    "\n",
    "# Launch App\n",
    "session.view = idk_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136776,
     "status": "aborted",
     "timestamp": 1768864440588,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "EvLRxiWQUUmm"
   },
   "outputs": [],
   "source": [
    "other_utils.plot_samples_from_view(idk_view, n=10, threshold=IDK_THRESHOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136776,
     "status": "aborted",
     "timestamp": 1768864440589,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "MvlFYF1bUV5j"
   },
   "outputs": [],
   "source": [
    "# Confidence stats for accepted samples (no ground truth available)\n",
    "covered_conf = covered_view.values(\"confidence_raw\")  # we stored this field\n",
    "\n",
    "if len(covered_conf) > 0:\n",
    "    covered_conf = np.array(covered_conf, dtype=float)\n",
    "    mean_conf = covered_conf.mean()\n",
    "    p10 = np.quantile(covered_conf, 0.10)\n",
    "    p50 = np.quantile(covered_conf, 0.50)\n",
    "    p90 = np.quantile(covered_conf, 0.90)\n",
    "else:\n",
    "    mean_conf = p10 = p50 = p90 = float(\"nan\")\n",
    "\n",
    "print(f\"Total Generated Images: {total_samples}\")\n",
    "print(f\"IDK Responses:          {idk_count}\")\n",
    "print(f\"Covered Responses:      {covered_count}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"COVERAGE:               {coverage:.2%}\")\n",
    "print(f\"CONF (Covered) mean:    {mean_conf:.3f}\")\n",
    "print(f\"CONF (Covered) P10/P50/P90: {p10:.3f} / {p50:.3f} / {p90:.3f}\")\n",
    "print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwW63BxbUYMj"
   },
   "source": [
    "## Publish Dataset on Hugging Face\n",
    "\n",
    "Finally, the FiftyOne dataset is exported and published to Hugging Face. This makes the results reproducible and shareable, allowing others to explore the dataset and the behavior of the IDK mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136777,
     "status": "aborted",
     "timestamp": 1768864440590,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "_c_-xYMUUYAs"
   },
   "outputs": [],
   "source": [
    "# Save FiftyOne dataset (images + metadata) to disk\n",
    "print(f\"Exporting dataset to {config.EXPORT_MNIST_DIR}...\")\n",
    "\n",
    "dataset.export(\n",
    "    export_dir=str(config.EXPORT_MNIST_DIR),\n",
    "    dataset_type=fo.types.FiftyOneDataset,\n",
    "    export_media=True, # This ensures the actual .png images are included\n",
    ")\n",
    "\n",
    "print(\"Export complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 136776,
     "status": "aborted",
     "timestamp": 1768864440590,
     "user": {
      "displayName": "Michele Marschner",
      "userId": "00906875642059722595"
     },
     "user_tz": -60
    },
    "id": "ambpjRlsUbVJ"
   },
   "outputs": [],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"HF_TOKEN\"\n",
    "\n",
    "# Make sure your token is loaded\n",
    "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
    "print(HF_TOKEN)\n",
    "assert HF_TOKEN is not None, \"HF_TOKEN env var is not set!\"\n",
    "\n",
    "api = HfApi(token=HF_TOKEN)\n",
    "\n",
    "api.upload_large_folder(\n",
    "    folder_path=f\"{config.EXPORT_MNIST_DIR}\",\n",
    "    repo_id=\"mmarschn/mnist_idk\",                   # ! must already exist on HF\n",
    "    repo_type=\"dataset\",\n",
    "    ignore_patterns=[\"*.ipynb_checkpoints\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
